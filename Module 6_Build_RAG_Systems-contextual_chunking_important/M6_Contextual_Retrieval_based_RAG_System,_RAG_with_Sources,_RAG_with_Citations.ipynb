{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Build a Contextual Retrieval based RAG System"],"metadata":{"id":"jbw4wHV4zlKj"}},{"cell_type":"markdown","source":["## Install OpenAI, and LangChain dependencies"],"metadata":{"id":"4vtFl39Ofu_8"}},{"cell_type":"code","source":["!pip install langchain==0.3.10\n","!pip install langchain-openai==0.2.12\n","!pip install langchain-community==0.3.11\n","!pip install jq==1.8.0\n","!pip install pymupdf==1.25.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa37d3dc-eda1-42d7-87c3-46bd3a670017","id":"LVX6450Lfu_9","executionInfo":{"status":"ok","timestamp":1734093570014,"user_tz":-330,"elapsed":47847,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain==0.3.10 in /usr/local/lib/python3.10/dist-packages (0.3.10)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (3.11.10)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.22 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.22)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.10.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.18.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.10) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.2.2)\n","Collecting langchain-openai==0.2.12\n","  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.12) (0.3.22)\n","Collecting openai<2.0.0,>=1.55.3 (from langchain-openai==0.2.12)\n","  Downloading openai-1.57.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.12)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.1.147)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.10.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.0.0)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.12.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.66.6)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.32.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.2.3)\n","Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.57.3-py3-none-any.whl (390 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.54.5\n","    Uninstalling openai-1.54.5:\n","      Successfully uninstalled openai-1.54.5\n","Successfully installed langchain-openai-0.2.12 openai-1.57.3 tiktoken-0.8.0\n","Collecting langchain-community==0.3.11\n","  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (3.11.10)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.11)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.11)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n","  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community==0.3.11)\n","  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (1.26.4)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.11)\n","  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.18.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.10.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.11) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.27.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.2.2)\n","Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n","Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.22\n","    Uninstalling langchain-core-0.3.22:\n","      Successfully uninstalled langchain-core-0.3.22\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.10\n","    Uninstalling langchain-0.3.10:\n","      Successfully uninstalled langchain-0.3.10\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n","Collecting jq==1.8.0\n","  Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jq\n","Successfully installed jq-1.8.0\n","Collecting pymupdf==1.25.1\n","  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n","Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pymupdf\n","Successfully installed pymupdf-1.25.1\n"]}]},{"cell_type":"markdown","source":["## Install Chroma Vector DB and LangChain wrapper"],"metadata":{"id":"bwUBYHjPfu_-"}},{"cell_type":"code","source":["!pip install langchain-chroma==0.1.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6606e56e-c58e-4f22-ed48-859c4ba66cbb","id":"p30SmCgTfu__","executionInfo":{"status":"ok","timestamp":1734093603723,"user_tz":-330,"elapsed":33712,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-chroma==0.1.4\n","  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n","Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma==0.1.4)\n","  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n","Collecting fastapi<1,>=0.95.2 (from langchain-chroma==0.1.4)\n","  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma==0.1.4) (0.3.24)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma==0.1.4) (1.26.4)\n","Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.10.3)\n","Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n","Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n","Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.12.2)\n","Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.28.2)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.28.2)\n","Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.20.3)\n","Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.66.6)\n","Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.68.1)\n","Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.15.1)\n","Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (9.0.0)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.0.2)\n","Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10.12)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (13.9.4)\n","Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma==0.1.4)\n","  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.33)\n","Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (0.1.147)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (24.2)\n","Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (3.0.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.32.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.3)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.0.0)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.25.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.15)\n","Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.5.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.66.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n","Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.26.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.1)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.10.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.4.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.6.1)\n","Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n","Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n","Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n","Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n","Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n","Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n","Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n","Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n","Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=c0ac2d95055e950fe7b015783aad910ae7cd83bae259e07d421d40ecbbaf38ec\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built pypika\n","Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.25.5\n","    Uninstalling protobuf-4.25.5:\n","      Successfully uninstalled protobuf-4.25.5\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.28.2\n","    Uninstalling opentelemetry-api-1.28.2:\n","      Successfully uninstalled opentelemetry-api-1.28.2\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n","    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.28.2\n","    Uninstalling opentelemetry-sdk-1.28.2:\n","      Successfully uninstalled opentelemetry-sdk-1.28.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 langchain-chroma-0.1.4 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.41.3 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.3 websockets-14.1\n"]}]},{"cell_type":"markdown","source":["## Enter Open AI API Key"],"metadata":{"id":"EITC17hwfu__"}},{"cell_type":"code","source":["from getpass import getpass\n","\n","OPENAI_KEY = getpass('Enter Open AI API Key: ')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f70e61f9-5d9e-4231-a206-a188edd11861","id":"yEh2olNvfvAA","executionInfo":{"status":"ok","timestamp":1734093609780,"user_tz":-330,"elapsed":6063,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Open AI API Key: ··········\n"]}]},{"cell_type":"markdown","source":["## Setup Environment Variables"],"metadata":{"id":"pm_mx0v-fvAA"}},{"cell_type":"code","source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = OPENAI_KEY"],"metadata":{"id":"Jhfb4gMUfvAC","executionInfo":{"status":"ok","timestamp":1734093612694,"user_tz":-330,"elapsed":541,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Open AI Embedding Models\n","\n","LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."],"metadata":{"id":"jiokYxD8fvAC"}},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","\n","# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n","openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"],"metadata":{"id":"-On4AS0HfvAD","executionInfo":{"status":"ok","timestamp":1734093622010,"user_tz":-330,"elapsed":7059,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Loading and Processing the Data"],"metadata":{"id":"afzeN_WkHIz2"}},{"cell_type":"markdown","source":["### Get the dataset"],"metadata":{"id":"RA_-hzHbFeSP"}},{"cell_type":"code","source":["# if you can't download using the following code\n","# go to https://drive.google.com/file/d/1aZxZejfteVuofISodUrY2CDoyuPLYDGZ download it\n","# manually upload it on colab\n","!gdown 1aZxZejfteVuofISodUrY2CDoyuPLYDGZ"],"metadata":{"id":"RZFMYH-yFhWn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9ed1bd5f-a2d7-463e-db22-06214ccad8a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1aZxZejfteVuofISodUrY2CDoyuPLYDGZ\n","To: /content/rag_docs.zip\n","100% 5.92M/5.92M [00:00<00:00, 35.1MB/s]\n"]}]},{"cell_type":"code","source":["!unzip rag_docs.zip"],"metadata":{"id":"WwLEBC4nF9ly","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2369aa8c-de53-462b-ebd5-5b9404841bd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  rag_docs.zip\n","   creating: rag_docs/\n","  inflating: rag_docs/attention_paper.pdf  \n","  inflating: rag_docs/cnn_paper.pdf  \n","  inflating: rag_docs/resnet_paper.pdf  \n","  inflating: rag_docs/vision_transformer.pdf  \n","  inflating: rag_docs/wikidata_rag_demo.jsonl  \n"]}]},{"cell_type":"markdown","source":["### Load and Process JSON Documents"],"metadata":{"id":"wMlxKZ_5jIdE"}},{"cell_type":"code","source":["from langchain.document_loaders import JSONLoader\n","\n","loader = JSONLoader(file_path='./rag_docs/wikidata_rag_demo.jsonl',\n","                    jq_schema='.',\n","                    text_content=False,\n","                    json_lines=True)\n","wiki_docs = loader.load()"],"metadata":{"id":"RZ5y0NfzHPhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(wiki_docs)"],"metadata":{"id":"G4E1zYFSG7J-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"32080ed1-8b19-405a-ffed-ec759522c284"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1801"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["wiki_docs[3]"],"metadata":{"id":"aSbhERAyGw0v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c03689eb-8ad3-44c9-8553-71c7439f961c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 4}, page_content='{\"id\": \"71548\", \"title\": \"Chi-square distribution\", \"paragraphs\": [\"In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\\\u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution.\", \"Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\"]}')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import json\n","from langchain.docstore.document import Document\n","wiki_docs_processed = []\n","\n","for doc in wiki_docs:\n","    doc = json.loads(doc.page_content)\n","    metadata = {\n","        \"title\": doc['title'],\n","        \"id\": doc['id'],\n","        \"source\": \"Wikipedia\",\n","        \"page\": 1\n","    }\n","    data = ' '.join(doc['paragraphs'])\n","    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"],"metadata":{"id":"yICyAF85h2DO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wiki_docs_processed[3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IATrHWKh7II","outputId":"08d4cc81-b8f4-4a33-ac6c-6ff33febd933"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'title': 'Chi-square distribution', 'id': '71548', 'source': 'Wikipedia', 'page': 1}, page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\xa0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution. Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.')"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### Load and Process PDF documents"],"metadata":{"id":"F_GzvHP1jSBo"}},{"cell_type":"markdown","source":["#### Create Chunk Contexts for Contextual Retrieval\n","\n","![](https://i.imgur.com/LRhKHzk.png)"],"metadata":{"id":"4vH6xGFOnv7m"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"],"metadata":{"id":"jxHHyhlbl_9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create chunk context generation chain\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema import StrOutputParser\n","\n","\n","def generate_chunk_context(document, chunk):\n","\n","    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n","                            Your task is to provide brief, relevant context for a chunk of text\n","                            based on the following research paper.\n","\n","                            Here is the research paper:\n","                            <paper>\n","                            {paper}\n","                            </paper>\n","\n","                            Here is the chunk we want to situate within the whole document:\n","                            <chunk>\n","                            {chunk}\n","                            </chunk>\n","\n","                            Provide a concise context (3-4 sentences max) for this chunk,\n","                            considering the following guidelines:\n","\n","                            - Give a short succinct context to situate this chunk within the overall document\n","                            for the purposes of improving search retrieval of the chunk.\n","                            - Answer only with the succinct context and nothing else.\n","                            - Context should be mentioned like 'Focuses on ....'\n","                            do not mention 'this chunk or section focuses on...'\n","\n","                            Context:\n","                        \"\"\"\n","\n","    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n","\n","    agentic_chunk_chain = (prompt_template\n","                                |\n","                            chatgpt\n","                                |\n","                            StrOutputParser())\n","\n","    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n","\n","    return context"],"metadata":{"id":"MHSh0Vg-mIUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import PyMuPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","import uuid\n","\n","def create_contextual_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n","\n","    print('Loading pages:', file_path)\n","    loader = PyMuPDFLoader(file_path)\n","    doc_pages = loader.load()\n","\n","    print('Chunking pages:', file_path)\n","    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n","                                              chunk_overlap=chunk_overlap)\n","    doc_chunks = splitter.split_documents(doc_pages)\n","\n","    print('Generating contextual chunks:', file_path)\n","    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n","    contextual_chunks = []\n","    for chunk in doc_chunks:\n","        chunk_content = chunk.page_content\n","        chunk_metadata = chunk.metadata\n","        chunk_metadata_upd = {\n","            'id': str(uuid.uuid4()),\n","            'page': chunk_metadata['page'],\n","            'source': chunk_metadata['source'],\n","            'title': chunk_metadata['source'].split('/')[-1]\n","        }\n","        context = generate_chunk_context(original_doc, chunk_content)\n","        contextual_chunks.append(Document(page_content=context+'\\n'+chunk_content,\n","                                          metadata=chunk_metadata_upd))\n","    print('Finished processing:', file_path)\n","    print()\n","    return contextual_chunks"],"metadata":{"id":"-uxOSfcsxqHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from glob import glob\n","\n","pdf_files = glob('./rag_docs/*.pdf')\n","pdf_files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-VUpdmczt0C","outputId":"efe8db0c-6e47-464e-926f-2c993314d190"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./rag_docs/resnet_paper.pdf',\n"," './rag_docs/vision_transformer.pdf',\n"," './rag_docs/cnn_paper.pdf',\n"," './rag_docs/attention_paper.pdf']"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["paper_docs = []\n","for fp in pdf_files:\n","    paper_docs.extend(create_contextual_chunks(file_path=fp, chunk_size=3500))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsicBMPazzlg","outputId":"8b567e07-d403-4ae6-d13b-98d02b0a24b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading pages: ./rag_docs/resnet_paper.pdf\n","Chunking pages: ./rag_docs/resnet_paper.pdf\n","Generating contextual chunks: ./rag_docs/resnet_paper.pdf\n","Finished processing: ./rag_docs/resnet_paper.pdf\n","\n","Loading pages: ./rag_docs/vision_transformer.pdf\n","Chunking pages: ./rag_docs/vision_transformer.pdf\n","Generating contextual chunks: ./rag_docs/vision_transformer.pdf\n","Finished processing: ./rag_docs/vision_transformer.pdf\n","\n","Loading pages: ./rag_docs/cnn_paper.pdf\n","Chunking pages: ./rag_docs/cnn_paper.pdf\n","Generating contextual chunks: ./rag_docs/cnn_paper.pdf\n","Finished processing: ./rag_docs/cnn_paper.pdf\n","\n","Loading pages: ./rag_docs/attention_paper.pdf\n","Chunking pages: ./rag_docs/attention_paper.pdf\n","Generating contextual chunks: ./rag_docs/attention_paper.pdf\n","Finished processing: ./rag_docs/attention_paper.pdf\n","\n"]}]},{"cell_type":"code","source":["len(paper_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOkwPMxp0gFh","outputId":"6913c904-ac84-4281-d05f-5479f44f3c64"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["79"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["paper_docs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVK2i0lR0ieV","outputId":"da507bde-09e6-4f76-9e49-c62d545072c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'id': 'ac10b59a-79a9-4e24-9ae7-34ff0403dc3a', 'page': 0, 'source': './rag_docs/resnet_paper.pdf', 'title': 'resnet_paper.pdf'}, page_content='Focuses on the introduction of deep residual learning as a framework to facilitate the training of significantly deeper neural networks, addressing challenges such as vanishing gradients and degradation of accuracy. It highlights the empirical success of residual networks on the ImageNet dataset and their application in various visual recognition tasks, including object detection and segmentation, leading to top performances in major competitions.\\nDeep Residual Learning for Image Recognition\\nKaiming He\\nXiangyu Zhang\\nShaoqing Ren\\nJian Sun\\nMicrosoft Research\\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\\nAbstract\\nDeeper neural networks are more difﬁcult to train. We\\npresent a residual learning framework to ease the training\\nof networks that are substantially deeper than those used\\npreviously. We explicitly reformulate the layers as learn-\\ning residual functions with reference to the layer inputs, in-\\nstead of learning unreferenced functions. We provide com-\\nprehensive empirical evidence showing that these residual\\nnetworks are easier to optimize, and can gain accuracy from\\nconsiderably increased depth. On the ImageNet dataset we\\nevaluate residual nets with a depth of up to 152 layers—8×\\ndeeper than VGG nets [41] but still having lower complex-\\nity. An ensemble of these residual nets achieves 3.57% error\\non the ImageNet test set. This result won the 1st place on the\\nILSVRC 2015 classiﬁcation task. We also present analysis\\non CIFAR-10 with 100 and 1000 layers.\\nThe depth of representations is of central importance\\nfor many visual recognition tasks. Solely due to our ex-\\ntremely deep representations, we obtain a 28% relative im-\\nprovement on the COCO object detection dataset. Deep\\nresidual nets are foundations of our submissions to ILSVRC\\n& COCO 2015 competitions1, where we also won the 1st\\nplaces on the tasks of ImageNet detection, ImageNet local-\\nization, COCO detection, and COCO segmentation.\\n1. Introduction\\nDeep convolutional neural networks [22, 21] have led\\nto a series of breakthroughs for image classiﬁcation [21,\\n50, 40]. Deep networks naturally integrate low/mid/high-\\nlevel features [50] and classiﬁers in an end-to-end multi-\\nlayer fashion, and the “levels” of features can be enriched\\nby the number of stacked layers (depth). Recent evidence\\n[41, 44] reveals that network depth is of crucial importance,\\nand the leading results [41, 44, 13, 16] on the challenging\\nImageNet dataset [36] all exploit “very deep” [41] models,\\nwith a depth of sixteen [41] to thirty [16]. Many other non-\\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\\n1http://image-net.org/challenges/LSVRC/2015/\\nand\\nhttp://mscoco.org/dataset/#detections-challenge2015.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0 \\n10\\n20\\niter. (1e4)\\ntraining error (%)\\n \\n \\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n10\\n20\\niter. (1e4)\\ntest error (%)\\n \\n \\n56-layer\\n20-layer\\n56-layer\\n20-layer\\nFigure 1. Training error (left) and test error (right) on CIFAR-10\\nwith 20-layer and 56-layer “plain” networks. The deeper network\\nhas higher training error, and thus test error. Similar phenomena\\non ImageNet is presented in Fig. 4.\\ngreatly beneﬁted from very deep models.\\nDriven by the signiﬁcance of depth, a question arises: Is\\nlearning better networks as easy as stacking more layers?\\nAn obstacle to answering this question was the notorious\\nproblem of vanishing/exploding gradients [1, 9], which\\nhamper convergence from the beginning.\\nThis problem,\\nhowever, has been largely addressed by normalized initial-\\nization [23, 9, 37, 13] and intermediate normalization layers\\n[16], which enable networks with tens of layers to start con-\\nverging for stochastic gradient descent (SGD) with back-\\npropagation [22].\\nWhen deeper networks are able to start converging, a\\ndegradation problem has been exposed: with the network\\ndepth increasing, accuracy gets saturated (which might be\\nunsurprising) and then degrades rapidly.\\nUnexpectedly,\\nsuch degradation is not caused by overﬁtting, and adding')"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["### Combine all document chunks in one list"],"metadata":{"id":"UyPdlZo2xEly"}},{"cell_type":"code","source":["len(wiki_docs_processed)"],"metadata":{"id":"UbtpR-r50mEn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7dd4d35d-fafd-4fff-ddf0-8351819b501e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1801"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["total_docs = wiki_docs_processed + paper_docs\n","len(total_docs)"],"metadata":{"id":"lNQWgq9t0pMH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"58ca4c0c-18b4-4fbc-a77c-800e835a3610"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1880"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## Index Document Chunks and Embeddings in Vector DB\n","\n","Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."],"metadata":{"id":"Daqn6Hglw9Nk"}},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","# create vector DB of docs and embeddings - takes < 30s on Colab\n","chroma_db = Chroma.from_documents(documents=total_docs,\n","                                  collection_name='my_context_db',\n","                                  embedding=openai_embed_model,\n","                                  # need to set the distance function to cosine else it uses euclidean by default\n","                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n","                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n","                                  persist_directory=\"./my_context_db\")"],"metadata":{"id":"ZhAQyrFBfvAN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ju_zBIj1Zsb"},"source":["### Load Vector DB from disk\n","\n","This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNvj0dDH1WDg"},"outputs":[],"source":["# load from disk\n","chroma_db = Chroma(persist_directory=\"./my_context_db\",\n","                   collection_name='my_context_db',\n","                   embedding_function=openai_embed_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFC3uPqYop0a","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac8bd549-18a9-46af-f804-2a4ca5e0d192"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain_chroma.vectorstores.Chroma at 0x7f911083f3d0>"]},"metadata":{},"execution_count":24}],"source":["chroma_db"]},{"cell_type":"markdown","source":["### Semantic Similarity based Retrieval\n","\n","We use simple cosine similarity here and retrieve the top 5 similar documents based on the user input query"],"metadata":{"id":"njfZOOVZxj1a"}},{"cell_type":"code","source":["similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n","                                              search_kwargs={\"k\": 5})"],"metadata":{"id":"tV1l6HYdxj1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def display_docs(docs):\n","    for doc in docs:\n","        print('Metadata:', doc.metadata)\n","        print('Content Brief:')\n","        display(Markdown(doc.page_content[:1000]))\n","        print()"],"metadata":{"id":"nUIJG_bDxj1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"what is machine learning?\"\n","top_docs = similarity_retriever.invoke(query)\n","display_docs(top_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":899},"id":"PIh4xGv2xj1c","outputId":"f8b431ff-094d-49ab-9bcd-e2c9aae2a632"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata: {'id': '564928', 'page': 1, 'source': 'Wikipedia', 'title': 'Machine learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '359370', 'page': 1, 'source': 'Wikipedia', 'title': 'Supervised learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '663523', 'page': 1, 'source': 'Wikipedia', 'title': 'Deep learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '6360', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '44742', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"what is the difference between transformers and vision transformers?\"\n","top_docs = similarity_retriever.invoke(query)\n","display_docs(top_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"S_PXFMcJxuyO","outputId":"5133bece-a361-4543-9214-1c80fe673353"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata: {'id': '0a4aa65e-ba24-40c0-890f-7143ff77e6f4', 'page': 0, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the introduction of the Vision Transformer (ViT) model, which applies a pure Transformer architecture to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional networks in computer vision and presents the advantages of using Transformers, particularly when pre-trained on large datasets. The section sets the stage for the subsequent exploration of ViT's performance compared to state-of-the-art convolutional networks.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has becom"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '38169299-0ef0-4af7-a2cb-506caedabf6d', 'page': 7, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability, while also discussing the implications for future model scaling efforts.\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '9efab417-6594-4327-af0e-a8229e183a97', 'page': 2, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁ"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': 'c23af995-486e-4fc9-8762-5fb229441a61', 'page': 7, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the behavior of attention mechanisms in the Vision Transformer (ViT), highlighting how attention distances vary across layers and the implications for feature extraction. It compares the attention patterns in ViT to those in hybrid models that incorporate convolutional networks, suggesting that early layers in CNNs may perform a similar role to localized attention in ViT. Additionally, it notes the relevance of attention distance to the model's ability to focus on semantically important regions for classification tasks.\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁcation (Figure 6).\n4.6"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '9e785ba7-2575-4e74-a3d6-6edefb9053e8', 'page': 1, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the performance of the Vision Transformer (ViT) in image recognition tasks, highlighting its ability to achieve state-of-the-art results when pre-trained on large datasets like ImageNet-21k and JFT-300M. It contrasts the inductive biases of convolutional neural networks (CNNs) with the advantages of large-scale training for ViT, demonstrating its competitive accuracy across various benchmarks.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats st"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"markdown","source":["## Build the RAG Pipeline"],"metadata":{"id":"gQFWv7YUyVII"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                {question}\n","\n","                Context:\n","                {context}\n","\n","                Answer:\n","            \"\"\"\n","\n","rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"],"metadata":{"id":"PHOrfGXKyVIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","qa_rag_chain = (\n","    {\n","        \"context\": (similarity_retriever\n","                      |\n","                    format_docs),\n","        \"question\": RunnablePassthrough()\n","    }\n","      |\n","    rag_prompt_template\n","      |\n","    chatgpt\n",")"],"metadata":{"id":"KmWeCB4yyVIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","query = \"What is machine learning?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"outputId":"1320207b-c57a-4e08-f755-0f050045884b","id":"xvj_eGIWyVIJ"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept originated from work in artificial intelligence and involves the study and construction of algorithms that can learn from and make predictions based on data. These algorithms follow programmed instructions but can also make decisions or predictions based on the data they process. \n\nIn machine learning, a model is built from sample inputs, and it is particularly useful in scenarios where designing and programming explicit algorithms is not feasible. Common applications of machine learning include spam filtering, detection of network intruders, optical character recognition (OCR), search engines, and computer vision.\n\nThere are different types of machine learning, including supervised learning, where the system infers a function from labeled training data. In this case, the results of the training are known beforehand, and the system learns to achieve these results correctly, often using inductive reasoning to generalize from the training data.\n\nAdditionally, deep learning is a specific kind of machine learning that primarily utilizes neural networks. It involves multiple layers of processing, allowing the model to learn increasingly abstract representations of the data. Deep learning is particularly effective for complex tasks such as speech recognition, image understanding, and handwriting recognition.\n\nOverall, machine learning enables computers to adapt and improve their performance on tasks through experience, making it a crucial component of modern artificial intelligence systems."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is a CNN?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":750},"id":"pXtezDlZzadt","outputId":"747fbcca-5fcc-40f4-d903-2d70b334444d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A Convolutional Neural Network (CNN) is an advanced architecture of Artificial Neural Networks (ANNs) specifically designed for image-driven pattern recognition tasks. CNNs are particularly effective in solving complex image analysis problems due to their unique structure and functionality.\n\n### Key Features of CNNs:\n\n1. **Three-Dimensional Neuron Organization**:\n   - Unlike traditional ANNs, which typically have a two-dimensional structure, CNNs organize neurons in three dimensions: height, width, and depth. The depth dimension represents the activation volume rather than the total number of layers.\n\n2. **Layer Types**:\n   - CNNs are composed of three main types of layers:\n     - **Convolutional Layers**: These layers apply a convolution operation to the input, allowing the network to learn spatial hierarchies of features. Each neuron in a convolutional layer connects to a small region of the input, which helps in detecting local patterns.\n     - **Pooling Layers**: These layers reduce the spatial dimensionality of the input, helping to decrease the number of parameters and computations in the network, while also controlling overfitting.\n     - **Fully-Connected Layers**: These layers connect every neuron in one layer to every neuron in the next layer, typically used at the end of the network to produce the final output.\n\n3. **Architecture Design**:\n   - A common practice in CNN architecture is to stack multiple convolutional layers before pooling layers. This stacking allows for the extraction of more complex features from the input data. Smaller convolutional layers are often preferred to manage computational complexity and memory usage.\n\n4. **Input Handling**:\n   - CNNs are designed to process image data, which requires careful consideration of input dimensionality. Techniques such as zero-padding are used to maintain the spatial dimensions of the input during convolution operations.\n\n5. **Learning Paradigms**:\n   - CNNs typically utilize supervised learning, where the model is trained on labeled data to minimize classification error. This is crucial for tasks like image classification, where the goal is to accurately identify objects within images.\n\n### Applications:\nCNNs are widely used in various applications, including but not limited to:\n- Image classification\n- Object detection\n- Image segmentation\n- Facial recognition\n\nIn summary, CNNs represent a significant advancement in the field of machine learning, particularly for tasks involving image data, by leveraging their specialized architecture to efficiently learn and recognize patterns within images."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How is a resnet better than a CNN?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":609},"id":"Fo92-ZmIELPF","outputId":"1c1cc31c-8ea4-4310-9975-8cee615f3f0e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A ResNet (Residual Network) is considered better than a traditional CNN (Convolutional Neural Network) for several reasons, primarily due to its architectural innovations that address specific challenges faced by deeper networks. Here are the key advantages of ResNets over standard CNNs:\n\n1. **Residual Learning Framework**: \n   - ResNets introduce the concept of residual learning, where the network learns to fit a residual mapping instead of the original unreferenced mapping. This is mathematically expressed as \\( F(x) + x \\), where \\( F(x) \\) is the residual function. This approach simplifies the optimization process, making it easier for the network to learn identity mappings when necessary.\n\n2. **Shortcut Connections**:\n   - The architecture of ResNets includes shortcut connections that skip one or more layers. These connections allow gradients to flow more easily during backpropagation, mitigating issues like vanishing gradients that can occur in very deep networks. This results in more effective training of deeper models.\n\n3. **Degradation Problem**:\n   - Traditional deep networks often suffer from the degradation problem, where adding more layers leads to higher training error. In contrast, ResNets can maintain or even improve performance as depth increases. For instance, the 34-layer ResNet outperformed the 34-layer plain network by achieving lower training error and better generalization.\n\n4. **Higher Accuracy with Increased Depth**:\n   - ResNets can achieve significantly better accuracy with increased depth without the degradation problem. For example, the 152-layer ResNet has been shown to outperform shallower networks and even other architectures like VGG, achieving a top-5 validation error of 4.49% in the ILSVRC 2015 competition.\n\n5. **Lower Computational Complexity**:\n   - Despite being deeper, ResNets can have lower computational complexity (measured in FLOPs) compared to other architectures like VGG. For instance, a 152-layer ResNet has 11.3 billion FLOPs, which is lower than the 15.3 billion FLOPs of VGG-16, while still achieving superior accuracy.\n\n6. **Generalization Performance**:\n   - ResNets have demonstrated excellent generalization capabilities across various recognition tasks, not just in image classification but also in object detection and segmentation tasks, as evidenced by their success in competitions like ILSVRC and COCO.\n\nIn summary, the architectural innovations of ResNets, particularly the use of residual learning and shortcut connections, enable them to train deeper networks more effectively, avoid degradation issues, and achieve higher accuracy compared to traditional CNNs."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is NLP and its relation to linguistics?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"J5IQoBc0zlAr","outputId":"4ce7096b-f5b8-442b-b9c5-00bbe5e7a360"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Natural Language Processing (NLP) is a field within Artificial Intelligence that focuses on enabling computers to automatically understand and generate human languages. The term \"Natural Language\" specifically refers to human languages, distinguishing them from programming languages. The overarching goal of NLP is to facilitate seamless interaction between humans and computers through language.\n\nNLP is closely related to linguistics, which is the scientific study of language and its structure. Linguistics provides the foundational theories and frameworks that inform the development of NLP technologies. By leveraging insights from linguistics, NLP aims to decode the complexities of human language, including syntax, semantics, and pragmatics, to improve the accuracy and effectiveness of language processing tasks.\n\nIn summary, NLP is a crucial intersection of Artificial Intelligence and linguistics, aiming to enhance computer understanding and generation of human language."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is the difference between AI, ML and DL?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":493},"id":"AzeZuG1hzvGy","outputId":"ca55a840-f295-413f-d01a-54bc383e6b4b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between AI, ML, and DL can be summarized as follows:\n\n### Artificial Intelligence (AI)\n- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications that enable machines to perform tasks that typically require human intelligence.\n- **Scope**: AI is a field of study aimed at creating systems that can interpret external data, learn from it, and adapt to achieve specific goals. It includes various subfields, one of which is machine learning.\n- **Examples**: AI applications can range from simple rule-based systems to complex algorithms that can learn and adapt over time.\n\n### Machine Learning (ML)\n- **Definition**: ML is a subfield of AI that focuses on the study and construction of algorithms that allow computers to learn from and make predictions based on data without being explicitly programmed.\n- **Functionality**: ML algorithms build models from sample inputs and can make decisions or predictions based on new data. It is particularly useful in scenarios where traditional programming is impractical.\n- **Examples**: Applications of ML include spam filtering, network intrusion detection, optical character recognition (OCR), and computer vision.\n\n### Deep Learning (DL)\n- **Definition**: DL is a specialized subset of machine learning that primarily uses neural networks with multiple layers (deep neural networks) to process data.\n- **Characteristics**: In deep learning, the information processed becomes increasingly abstract with each added layer, allowing for complex tasks such as speech and image recognition. It is inspired by the structure and function of biological neural networks.\n- **Examples**: DL is often used in applications that require high levels of abstraction, such as image and speech recognition, where traditional ML methods may struggle.\n\nIn summary, AI is the overarching field that includes both machine learning and deep learning, with machine learning being a method of achieving AI and deep learning being a more advanced technique within machine learning."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is the difference between transformers and vision transformers?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":550},"id":"cWihDiL3zPzY","outputId":"b8b3a24f-b11c-424d-efcd-6d5530148d15"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between transformers and vision transformers (ViTs) primarily lies in their application and the way they process input data.\n\n1. **Architecture and Input Handling**:\n   - **Transformers**: Originally designed for natural language processing (NLP), transformers operate on sequences of tokens (words) and utilize self-attention mechanisms to capture relationships between these tokens. The input is typically a 1D sequence of embeddings.\n   - **Vision Transformers (ViTs)**: ViTs adapt the transformer architecture for image classification tasks by treating image patches as tokens. An image is divided into fixed-size patches, which are then flattened and linearly embedded into a sequence. This sequence of patch embeddings is fed into a standard transformer encoder, allowing the model to process 2D image data effectively.\n\n2. **Inductive Biases**:\n   - **Transformers**: In NLP, transformers benefit from the sequential nature of text, where the order of tokens is crucial, and they leverage the inherent structure of language.\n   - **Vision Transformers**: ViTs lack some of the inductive biases present in convolutional neural networks (CNNs), such as translation equivariance and locality. This means that while CNNs are designed to recognize patterns in spatial hierarchies, ViTs rely on large-scale training to learn these patterns from the data itself.\n\n3. **Performance and Training**:\n   - **Transformers**: In NLP, transformers have become the standard due to their ability to scale and perform well on large datasets.\n   - **Vision Transformers**: ViTs have shown that when pre-trained on large datasets, they can achieve competitive or superior performance compared to state-of-the-art CNNs, particularly in image classification tasks. They require fewer computational resources to train while still achieving high accuracy on various benchmarks.\n\n4. **Attention Mechanism**:\n   - **Transformers**: The attention mechanism in transformers allows for global context understanding across the entire sequence of tokens.\n   - **Vision Transformers**: ViTs utilize self-attention to integrate information across the entire image, enabling the model to focus on semantically important regions for classification. This capability is particularly beneficial in understanding complex visual patterns.\n\nIn summary, while both transformers and vision transformers share a foundational architecture, their differences lie in their input types, the biases they leverage, their performance characteristics, and how they apply attention mechanisms to their respective tasks."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How is self-attention important in transformers?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"k1lqzejlEvsj","outputId":"a21b4676-1ccb-477a-b2c6-4b6bda910171"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Self-attention is crucial in transformers for several reasons, particularly in the context of the Vision Transformer (ViT) and its application in feature extraction and classification tasks. Here are the key points regarding the importance of self-attention in transformers:\n\n1. **Attention Mechanism**: Self-attention allows the model to weigh the importance of different parts of the input data when making predictions. This is particularly beneficial in understanding contextual relationships within the data, whether it be in natural language processing or computer vision.\n\n2. **Localized Attention**: In the Vision Transformer, early layers exhibit highly localized attention, which is essential for focusing on specific regions of an image that are semantically relevant for classification tasks. This behavior is somewhat analogous to the function of early convolutional layers in convolutional neural networks (CNNs), which also focus on localized features.\n\n3. **Attention Distance**: The attention distance, which refers to how far apart the attended elements are in the input, varies across layers in the transformer. In lower layers, attention distances are consistently small, indicating a focus on local features. As the network depth increases, the attention distance grows, allowing the model to capture more global relationships and dependencies within the data.\n\n4. **Feature Extraction**: The ability of self-attention to adaptively focus on semantically important regions enhances the model's feature extraction capabilities. This is critical for tasks such as image classification, where understanding the context and relationships between different parts of an image can significantly impact performance.\n\n5. **Comparison with Hybrid Models**: The attention patterns in ViT are compared to those in hybrid models that incorporate CNNs. The findings suggest that while CNNs may perform localized attention through their early layers, transformers leverage self-attention to dynamically adjust their focus based on the input data, leading to potentially superior performance in certain tasks.\n\nIn summary, self-attention in transformers is vital for enabling the model to effectively capture both local and global dependencies in the data, enhancing its ability to perform complex tasks such as classification and feature extraction."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How does a resnet work?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":682},"id":"7Zf_BjmlFBcb","outputId":"97aa2758-d215-4fe3-8f5a-a4fa2a1b56c2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A ResNet, or Residual Network, operates on the principle of residual learning to address the degradation problem that occurs in deep neural networks. Here’s a detailed explanation of how it works:\n\n### Key Concepts of ResNet\n\n1. **Residual Mapping**:\n   - Instead of learning the desired underlying mapping \\( H(x) \\) directly, ResNets learn a residual mapping \\( F(x) = H(x) - x \\). This means that the network is designed to learn the difference between the desired output and the input, which is often easier than learning the output directly.\n\n2. **Shortcut Connections**:\n   - ResNets utilize shortcut connections that skip one or more layers. These connections perform identity mapping, meaning they pass the input \\( x \\) directly to the output of the stacked layers. The output of the residual block is then given by \\( F(x) + x \\).\n   - This approach allows gradients to flow through the network more effectively during backpropagation, mitigating issues like vanishing gradients that can occur in very deep networks.\n\n3. **Identity Mapping**:\n   - When the input and output dimensions are the same, the shortcut connection simply adds the input to the output of the stacked layers. If the dimensions differ, the shortcut can either pad the input with zeros or use a projection shortcut (e.g., 1x1 convolutions) to match the dimensions.\n\n### Advantages of ResNet\n\n- **Easier Optimization**: By reformulating the learning task to focus on residuals, ResNets make it easier to optimize deeper networks. This is evidenced by the fact that deeper ResNets (e.g., 34-layer, 110-layer) can achieve lower training errors compared to their plain counterparts of the same depth.\n  \n- **Higher Accuracy with Depth**: ResNets can effectively utilize increased depth to improve accuracy. For instance, a 34-layer ResNet outperforms an 18-layer ResNet, demonstrating that deeper networks can generalize better and achieve higher accuracy.\n\n- **Lower Complexity**: Despite having more layers, ResNets can maintain lower computational complexity compared to traditional architectures like VGG, allowing for efficient training and inference.\n\n### Empirical Evidence\n\n- Experiments on datasets like ImageNet and CIFAR-10 show that ResNets not only overcome the degradation problem but also achieve significant accuracy improvements as depth increases. For example, a 152-layer ResNet has been shown to outperform previous models while having lower complexity.\n\nIn summary, ResNets leverage residual learning and shortcut connections to facilitate the training of very deep networks, leading to improved performance and easier optimization compared to traditional deep learning architectures."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is LangGraph?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"UbaUpVXKz8IK","outputId":"7a4dc86a-91e4-458b-97bc-91b0595d7009"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I don't know."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is an Agentic AI System?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"FmxNo6B50AdP","outputId":"25797c69-e76e-4a30-cab3-334e0fd18178"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is LangChain?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"Ttz53mEy0J_D","outputId":"da8bcee5-d16e-4638-9ed8-06987aba9ccf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I don't know."},"metadata":{}}]},{"cell_type":"markdown","source":["# Build a RAG System with Sources"],"metadata":{"id":"-HkUAnWFH8LA"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                {question}\n","\n","                Context:\n","                {context}\n","\n","                Answer:\n","            \"\"\"\n","\n","rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"],"metadata":{"id":"cd9CWH6hH8LB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.runnables import RunnableLambda\n","from operator import itemgetter\n","\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","src_rag_response_chain = (\n","    {\n","        \"context\": (itemgetter('context')\n","                        |\n","                    RunnableLambda(format_docs)),\n","        \"question\": itemgetter(\"question\")\n","    }\n","        |\n","    rag_prompt_template\n","        |\n","    chatgpt\n","        |\n","    StrOutputParser()\n",")\n","\n","rag_chain_w_sources = (\n","    {\n","        \"context\": similarity_retriever,\n","        \"question\": RunnablePassthrough()\n","    }\n","        |\n","    RunnablePassthrough.assign(response=src_rag_response_chain)\n",")"],"metadata":{"id":"m6wFtKQqXNHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"What is machine learning?\"\n","result = rag_chain_w_sources.invoke(query)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYfU-VdjYf1u","outputId":"ad1b6c91-065b-41e3-d6d2-92085cf786d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': [Document(metadata={'id': '564928', 'page': 1, 'source': 'Wikipedia', 'title': 'Machine learning'}, page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'),\n","  Document(metadata={'id': '359370', 'page': 1, 'source': 'Wikipedia', 'title': 'Supervised learning'}, page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.'),\n","  Document(metadata={'id': '663523', 'page': 1, 'source': 'Wikipedia', 'title': 'Deep learning'}, page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.'),\n","  Document(metadata={'id': '6360', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial intelligence'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n","  Document(metadata={'id': '44742', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial neural network'}, page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.')],\n"," 'question': 'What is machine learning?',\n"," 'response': 'Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept originated from work in artificial intelligence and involves the study and construction of algorithms that can learn from and make predictions based on data. These algorithms follow programmed instructions but can also make decisions or predictions based on the data they process. \\n\\nIn machine learning, a model is built from sample inputs, and it is particularly useful in scenarios where designing and programming explicit algorithms is not feasible. Common applications of machine learning include spam filtering, detection of network intruders, optical character recognition (OCR), search engines, and computer vision.\\n\\nThere are different types of machine learning, including supervised learning, where the system infers a function from labeled training data. In this case, the results of the training are known beforehand, and the system learns to achieve these results correctly, often using inductive reasoning to generalize from the training data.\\n\\nAdditionally, deep learning is a specific kind of machine learning that primarily utilizes neural networks. It involves multiple layers of processing, allowing the model to learn increasingly abstract representations of the data. Deep learning is particularly effective for complex tasks such as speech recognition, image understanding, and handwriting recognition.\\n\\nOverall, machine learning enables computers to adapt and improve their performance on tasks through experience, making it a crucial component of modern artificial intelligence systems.'}"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def display_results(result_obj):\n","    print('Query:')\n","    display(Markdown(result_obj['question']))\n","    print()\n","    print('Response:')\n","    display(Markdown(result_obj['response']))\n","    print('='*50)\n","    print('Sources:')\n","    for source in result_obj['context']:\n","        print('Metadata:', source.metadata)\n","        print('Content Brief:')\n","        display(Markdown(source.page_content))\n","        print()\n"],"metadata":{"id":"2wWdkU4oa1BD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"What is machine learning?\"\n","result = rag_chain_w_sources.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8NTfGcChbFuj","outputId":"7687d1b3-efc7-442d-c9dd-819e4f0a907f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is machine learning?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept originated from work in artificial intelligence and involves the study and construction of algorithms that can learn from and make predictions based on data. These algorithms follow programmed instructions but can also make decisions or predictions based on the data they process. \n\nIn machine learning, a model is built from sample inputs, and it is particularly useful in scenarios where designing and programming explicit algorithms is not feasible. Common applications of machine learning include spam filtering, detection of network intruders, optical character recognition (OCR), search engines, and computer vision.\n\nThere are different types of machine learning, including supervised learning, where the system infers a function from labeled training data. In this case, the results of the training are known beforehand, and the system learns to achieve these results correctly, often using inductive reasoning to generalize from the training data.\n\nAdditionally, deep learning is a specific kind of machine learning that primarily utilizes neural networks. It involves multiple layers of processing, allowing the model to learn increasingly abstract representations of the data. Deep learning is particularly effective for complex tasks such as speech recognition, image understanding, and handwriting recognition.\n\nOverall, machine learning enables computers to adapt and improve their performance on tasks through experience, making it a powerful tool in the realm of artificial intelligence."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Metadata: {'id': '564928', 'page': 1, 'source': 'Wikipedia', 'title': 'Machine learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '359370', 'page': 1, 'source': 'Wikipedia', 'title': 'Supervised learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '663523', 'page': 1, 'source': 'Wikipedia', 'title': 'Deep learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '6360', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '44742', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is the difference between AI, ML and DL?\"\n","result = rag_chain_w_sources.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jhWbcmodbYfy","outputId":"83c9b9bc-2e9c-4f99-e90d-7cdafbc665a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is the difference between AI, ML and DL?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between AI, ML, and DL can be summarized as follows:\n\n### Artificial Intelligence (AI)\n- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications that enable machines to perform tasks that typically require human intelligence.\n- **Scope**: AI is a field of study aimed at creating systems that can interpret external data, learn from it, and adapt to achieve specific goals. It includes various subfields, one of which is machine learning.\n- **Examples**: AI applications can range from simple rule-based systems to complex algorithms that can learn and make decisions.\n\n### Machine Learning (ML)\n- **Definition**: ML is a subfield of AI that focuses on the development of algorithms that allow computers to learn from and make predictions based on data without being explicitly programmed for each task.\n- **Functionality**: ML algorithms build models from sample inputs and can improve their performance as they are exposed to more data. They are particularly useful in scenarios where traditional programming is impractical.\n- **Examples**: Applications of ML include spam filtering, network intrusion detection, optical character recognition (OCR), and computer vision.\n\n### Deep Learning (DL)\n- **Definition**: DL is a specialized subset of machine learning that utilizes multi-layered neural networks to process data. It is often referred to as deep structured learning or hierarchical learning.\n- **Structure**: Deep learning models consist of multiple layers (including hidden layers) that allow for the abstraction of information, making them particularly effective for complex tasks such as speech and image recognition.\n- **Inspiration**: These models are inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains.\n- **Examples**: DL is commonly used in applications that require high levels of abstraction, such as natural language processing and advanced image recognition.\n\nIn summary, AI is the overarching field that includes both ML and DL, with ML being a method of achieving AI through data-driven learning, and DL being a more advanced technique within ML that employs deep neural networks for complex problem-solving."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Metadata: {'id': '663523', 'page': 1, 'source': 'Wikipedia', 'title': 'Deep learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '564928', 'page': 1, 'source': 'Wikipedia', 'title': 'Machine learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '6360', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '669662', 'page': 1, 'source': 'Wikipedia', 'title': 'Loop AI Labs'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Loop AI Labs is an AI and cognitive computing company that focuses on language understanding technology. The company was founded in San Francisco in 2012 by Italian entrepreneur Gianmauro Calafiore, who sold his company Gsmbox to in 2004 and then relocated from Italy to San Francisco. Wanting to start an artificial intelligence company, he recruited two veterans of the project, the largest government-funded AI project in history, who had worked on the project at and Stanford University's . The original company name, \"Soshoma\", was changed to Loop AI Labs in 2015 after the company decided to change its focus from consumer-oriented to enterprise. Loop AI Labs is headquartered in San Francisco, California, with offices in New York, Milan, and Singapore. The company is privately funded. On May 4, 2017, Loop AI Labs entered into a deal with , a leading European provider of mobile messaging and solutions, to bring their cognitive computing technology to LINK's business clients, which cover 234 million people across Europe."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '44742', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is the difference between transformers and vision transformers?\"\n","result = rag_chain_w_sources.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"iKggz7qfYkh1","outputId":"6e2076f9-cf61-4466-fa18-fd2a8de09da7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is the difference between transformers and vision transformers?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between transformers and vision transformers (ViTs) primarily lies in their application and the way they process input data.\n\n1. **Architecture and Input Handling**:\n   - **Transformers**: Originally designed for natural language processing (NLP), transformers operate on sequences of tokens (words) and utilize self-attention mechanisms to capture relationships between these tokens. The input is typically a 1D sequence of embeddings.\n   - **Vision Transformers (ViTs)**: ViTs adapt the transformer architecture for image classification tasks by treating image patches as tokens. An image is divided into fixed-size patches, which are then flattened and linearly embedded into a sequence. This sequence of patch embeddings is fed into a standard transformer encoder, allowing the model to process 2D image data effectively.\n\n2. **Inductive Biases**:\n   - **Transformers**: In NLP, transformers benefit from the sequential nature of text, where the order of tokens is crucial, and they leverage the inherent structure of language.\n   - **Vision Transformers**: ViTs lack some of the inductive biases present in convolutional neural networks (CNNs), such as translation equivariance and locality. This means that while CNNs are designed to recognize patterns in spatial hierarchies, ViTs rely on large-scale training to learn these patterns from data.\n\n3. **Performance and Training**:\n   - **Transformers**: In NLP, transformers have become the standard due to their ability to scale and perform well on large datasets.\n   - **Vision Transformers**: ViTs have shown that when pre-trained on large datasets, they can achieve competitive or superior performance compared to state-of-the-art CNNs, particularly in terms of efficiency and scalability. ViTs can outperform CNNs while requiring significantly less computational resources for training.\n\n4. **Attention Mechanism**:\n   - **Transformers**: The self-attention mechanism in transformers allows them to consider the entire sequence of tokens, which is effective for capturing long-range dependencies in text.\n   - **Vision Transformers**: ViTs utilize self-attention to integrate information across the entire image, enabling them to focus on semantically important regions for classification tasks. This global attention capability is leveraged even in the early layers of the model.\n\nIn summary, while both transformers and vision transformers share a foundational architecture, they differ significantly in their input processing, reliance on inductive biases, performance characteristics, and the application of attention mechanisms tailored to their respective domains."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Metadata: {'id': '0a4aa65e-ba24-40c0-890f-7143ff77e6f4', 'page': 0, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the introduction of the Vision Transformer (ViT) model, which applies a pure Transformer architecture to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional networks in computer vision and presents the advantages of using Transformers, particularly when pre-trained on large datasets. The section sets the stage for the subsequent exploration of ViT's performance compared to state-of-the-art convolutional networks.\nPublished as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train.1\n1\nINTRODUCTION\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance.\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020).\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classiﬁcation in supervised fashion.\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\n1Fine-tuning\ncode\nand\npre-trained\nmodels\nare\navailable\nat\nhttps://github.com/\ngoogle-research/vision_transformer\n1\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '38169299-0ef0-4af7-a2cb-506caedabf6d', 'page': 7, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability, while also discussing the implications for future model scaling efforts.\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone).\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\n4.5\nINSPECTING VISION TRANSFORMER\nInput\nAttention\nFigure 6: Representative ex-\namples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails.\nTo begin to understand how the Vision Transformer processes im-\nage data, we analyze its internal representations. The ﬁrst layer of\nthe Vision Transformer linearly projects the ﬂattened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding ﬁlters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the ﬁne structure within each patch.\nAfter the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position em-\nbeddings, i.e. closer patches tend to have more similar position em-\nbeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology ex-\nplains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4).\nSelf-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speciﬁcally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '9efab417-6594-4327-af0e-a8229e183a97', 'page': 2, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3\nMETHOD\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\ntheir efﬁcient implementations – can be used almost out of the box.\n3.1\nVISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder (z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\ntached to z0\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at ﬁne-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n3"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': 'c23af995-486e-4fc9-8762-5fb229441a61', 'page': 7, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the behavior of attention mechanisms in the Vision Transformer (ViT), highlighting how attention distances vary across layers and the implications for feature extraction. It compares the attention patterns in ViT to those in hybrid models that incorporate convolutional networks, suggesting that early layers in CNNs may perform a similar role to localized attention in ViT. Additionally, it notes the relevance of attention distance to the model's ability to focus on semantically important regions for classification tasks.\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\nregions that are semantically relevant for classiﬁcation (Figure 6).\n4.6\nSELF-SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\n8"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '9e785ba7-2575-4e74-a3d6-6edefb9053e8', 'page': 1, 'source': './rag_docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the performance of the Vision Transformer (ViT) in image recognition tasks, highlighting its ability to achieve state-of-the-art results when pre-trained on large datasets like ImageNet-21k and JFT-300M. It contrasts the inductive biases of convolutional neural networks (CNNs) with the advantages of large-scale training for ViT, demonstrating its competitive accuracy across various benchmarks.\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks.\n2\nRELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\nNaive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nefﬁciently on hardware accelerators.\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well.\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is an Agentic AI System?\"\n","result = rag_chain_w_sources.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":873},"id":"fCCvcwzEbqQq","outputId":"62128376-aa6e-4290-c7bd-6e198ff8e362"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is an Agentic AI System?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Metadata: {'id': '6360', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '674015', 'page': 1, 'source': 'Wikipedia', 'title': 'A.I. Artificial Intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A.I. Artificial Intelligence, or A.I., is a 2001 American science fiction drama movie directed by Steven Spielberg. The screenplay was by Spielberg based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. The movie was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt. It is set in a futuristic post-climate change society. \"A.I.\" tells the story of David (Osment), a childlike android uniquely programmed with the ability to love."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '112634', 'page': 1, 'source': 'Wikipedia', 'title': 'Swarm intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Swarm Intelligence is a field of Computer science. It is a form of Artificial intelligence. Some animals, mostly insects like ants, or bees form large colonies. These colonies are made of many animals that communicate with each other. Each animal is relatively simple, but by working together with other animals it is able to solve complex tasks. Swarm intelligence wants to obtain similar behaviour than that observed with these animals. Instead of the animals, so called \"agents\" are used."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '692745', 'page': 1, 'source': 'Wikipedia', 'title': 'Shakey the robot'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Shakey the Robot was the first general purpose mobile AI robot. The project combined research in robotics, computer vision, and natural language processing. Because of this, it was the first project that melded logical reasoning and physical action. Shakey was developed at the Artificial Intelligence Center of Stanford Research Institute (now called SRI International) by Nils John Nilsson from 1966 to 1972."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '564218', 'page': 1, 'source': 'Wikipedia', 'title': 'Robot lawyer'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A robot lawyer is an artificial intelligence (AI) computer program. It is designed to ask the same questions as a real lawyer about certain legal issues. Robot lawyers are being used in many countries around the world including the United States, the United Kingdom, and Holland."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"markdown","source":["# Build a RAG System with Source Citations Agentic Pipeline"],"metadata":{"id":"25Si_mSAc9HY"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                {question}\n","\n","                Context:\n","                {context}\n","\n","                Answer:\n","            \"\"\"\n","\n","rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)\n","rag_prompt_template.pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3cKwTUIc9HZ","outputId":"2ca7b70c-cae1-4e73-c20a-653354a1c315"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n","\n","                Context:\n","                \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n","\n","                Answer:\n","            \n"]}]},{"cell_type":"code","source":["citations_prompt = \"\"\"You are an assistant who is an expert in analyzing answers to questions\n","                      and finding out referenced citations from context articles.\n","\n","                      Given the following question, context and generated answer,\n","                      analyze the generated answer and quote citations from context articles\n","                      that can be used to justify the generated answer.\n","\n","                      Question:\n","                      {question}\n","\n","                      Context Articles:\n","                      {context}\n","\n","                      Answer:\n","                      {answer}\n","                  \"\"\"\n","\n","cite_prompt_template = ChatPromptTemplate.from_template(citations_prompt)\n","cite_prompt_template.pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fbhOllRprRx","outputId":"26e90a30-253c-4220-adf7-20269e9dc4e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","You are an assistant who is an expert in analyzing answers to questions\n","                      and finding out referenced citations from context articles.\n","\n","                      Given the following question, context and generated answer,\n","                      analyze the generated answer and quote citations from context articles\n","                      that can be used to justify the generated answer.\n","\n","                      Question:\n","                      \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n","\n","                      Context Articles:\n","                      \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n","\n","                      Answer:\n","                      \u001b[33;1m\u001b[1;3m{answer}\u001b[0m\n","                  \n"]}]},{"cell_type":"code","source":["from pydantic import BaseModel, Field\n","from typing import List\n","\n","class Citation(BaseModel):\n","    id: str = Field(description=\"\"\"The string ID of a SPECIFIC context article\n","                                   which justifies the answer.\"\"\")\n","    source: str = Field(description=\"\"\"The source of the SPECIFIC context article\n","                                       which justifies the answer.\"\"\")\n","    title: str = Field(description=\"\"\"The title of the SPECIFIC context article\n","                                      which justifies the answer.\"\"\")\n","    page: int = Field(description=\"\"\"The page number of the SPECIFIC context article\n","                                     which justifies the answer.\"\"\")\n","    quotes: str = Field(description=\"\"\"The VERBATIM sentences from the SPECIFIC context article\n","                                      that are used to generate the answer.\n","                                      Should be exact sentences from context article without missing words.\"\"\")\n","\n","\n","class QuotedCitations(BaseModel):\n","    \"\"\"Quote citations from given context articles\n","       that can be used to justify the generated answer. Can be multiple articles.\"\"\"\n","    citations: List[Citation] = Field(description=\"\"\"Citations (can be multiple) from the given\n","                                                     context articles that justify the answer.\"\"\")"],"metadata":{"id":"n-ehwnM4dyGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.runnables import RunnableLambda\n","from operator import itemgetter\n","\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","structured_chatgpt = chatgpt.with_structured_output(QuotedCitations)\n","\n","\n","def format_docs_with_metadata(docs: List[Document]) -> str:\n","    formatted_docs = [\n","        f\"\"\"Context Article ID: {doc.metadata['id']}\n","            Context Article Source: {doc.metadata['source']}\n","            Context Article Title: {doc.metadata['title']}\n","            Context Article Page: {doc.metadata['page']}\n","            Context Article Details: {doc.page_content}\n","         \"\"\"\n","            for i, doc in enumerate(docs)\n","    ]\n","    return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs)\n","\n","rag_response_chain = (\n","    {\n","        \"context\": (itemgetter('context')\n","                        |\n","                    RunnableLambda(format_docs_with_metadata)),\n","        \"question\": itemgetter(\"question\")\n","    }\n","        |\n","    rag_prompt_template\n","        |\n","    chatgpt\n","        |\n","    StrOutputParser()\n",")\n","\n","cite_response_chain = (\n","    {\n","        \"context\": itemgetter('context'),\n","        \"question\": itemgetter(\"question\"),\n","        \"answer\": itemgetter(\"answer\")\n","    }\n","        |\n","    cite_prompt_template\n","        |\n","    structured_chatgpt\n",")\n","\n","rag_chain_w_citations = (\n","    {\n","        \"context\": similarity_retriever,\n","        \"question\": RunnablePassthrough()\n","    }\n","        |\n","    RunnablePassthrough.assign(answer=rag_response_chain)\n","        |\n","    RunnablePassthrough.assign(citations=cite_response_chain)\n",")"],"metadata":{"id":"haxp_9LmjVp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"What is machine learning\"\n","result = rag_chain_w_citations.invoke(query)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ff2a1d85-1310-4ded-f8a7-afb16e6ee5c0","id":"Qt12nMvTjVp2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': [Document(metadata={'id': '564928', 'page': 1, 'source': 'Wikipedia', 'title': 'Machine learning'}, page_content='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'),\n","  Document(metadata={'id': '359370', 'page': 1, 'source': 'Wikipedia', 'title': 'Supervised learning'}, page_content='In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data.'),\n","  Document(metadata={'id': '663523', 'page': 1, 'source': 'Wikipedia', 'title': 'Deep learning'}, page_content='Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.'),\n","  Document(metadata={'id': '6360', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial intelligence'}, page_content='Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'),\n","  Document(metadata={'id': '44742', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial neural network'}, page_content='A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation.')],\n"," 'question': 'What is machine learning',\n"," 'answer': 'Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in the broader field of artificial intelligence (AI). Machine learning focuses on the study and construction of algorithms that can learn from and make predictions based on data. \\n\\nThese algorithms operate by following programmed instructions while also being capable of making predictions or decisions based on the data they process. They build a model from sample inputs, which allows them to perform tasks where traditional programming methods are insufficient. \\n\\nExamples of machine learning applications include:\\n- Spam filtering\\n- Detection of network intruders or malicious insiders\\n- Optical character recognition (OCR)\\n- Search engines\\n- Computer vision\\n\\nOverall, machine learning enables systems to improve their performance on tasks over time as they are exposed to more data, making it a powerful tool in various domains.',\n"," 'citations': QuotedCitations(citations=[Citation(id='564928', source='Wikipedia', title='Machine learning', page=1, quotes='Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data.'), Citation(id='564928', source='Wikipedia', title='Machine learning', page=1, quotes='Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.'), Citation(id='564928', source='Wikipedia', title='Machine learning', page=1, quotes='Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'), Citation(id='6360', source='Wikipedia', title='Artificial intelligence', page=1, quotes=\"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers 'smart'.\")])}"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["result['citations'].dict()['citations']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3fufsoAHiuD","outputId":"c3f9a01f-e1b7-4d72-9eca-3efae2b85bd4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'id': '564928',\n","  'source': 'Wikipedia',\n","  'title': 'Machine learning',\n","  'page': 1,\n","  'quotes': 'Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data.'},\n"," {'id': '564928',\n","  'source': 'Wikipedia',\n","  'title': 'Machine learning',\n","  'page': 1,\n","  'quotes': 'Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.'},\n"," {'id': '564928',\n","  'source': 'Wikipedia',\n","  'title': 'Machine learning',\n","  'page': 1,\n","  'quotes': 'Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'},\n"," {'id': '6360',\n","  'source': 'Wikipedia',\n","  'title': 'Artificial intelligence',\n","  'page': 1,\n","  'quotes': \"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers 'smart'.\"}]"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["import re\n","# used mostly for nice display formatting, ignore if not needed\n","def get_cited_context(result_obj):\n","    # Dictionary to hold separate citation information for each unique source and title combination\n","    source_with_citations = {}\n","\n","    def highlight_text(context, quote):\n","        # Normalize whitespace and remove unnecessary punctuation\n","        quote = re.sub(r'\\s+', ' ', quote).strip()\n","        context = re.sub(r'\\s+', ' ', context).strip()\n","\n","        # Split quote into phrases, being careful with punctuation\n","        phrases = [phrase.strip() for phrase in re.split(r'[.!?]', quote) if phrase.strip()]\n","\n","        highlighted_context = context\n","\n","        for phrase in phrases: # for each quoted phrase\n","\n","            # Create regex pattern to match cited phrases\n","            # Escape special regex characters, but preserve word boundaries\n","            escaped_phrase = re.escape(phrase)\n","            # Create regex pattern that allows for slight variations\n","            pattern = re.compile(r'\\b' + escaped_phrase + r'\\b', re.IGNORECASE)\n","\n","            # Replace all matched phrases with bolded version\n","            highlighted_context = pattern.sub(lambda m: f\"**{m.group(0)}**\", highlighted_context)\n","\n","        return highlighted_context\n","\n","    # Process the citation data\n","    for cite in result_obj['citations'].dict()['citations']:\n","        cite_id = cite['id']\n","        title = cite['title']\n","        source = cite['source']\n","        page = cite['page']\n","        quote = cite['quotes']\n","\n","        # Check if the (source, title) key exists, and initialize if it doesn't\n","        if (source, title) not in source_with_citations:\n","            source_with_citations[(source, title)] = {\n","                'title': title,\n","                'source': source,\n","                'citations': []\n","            }\n","\n","        # Find or create the citation entry for this unique (id, page) combination\n","        citation_entry = next(\n","            (c for c in source_with_citations[(source, title)]['citations'] if c['id'] == cite_id and c['page'] == page),\n","            None\n","        )\n","        if citation_entry is None:\n","            citation_entry = {'id': cite_id, 'page': page, 'quote': [quote], 'context': None}\n","            source_with_citations[(source, title)]['citations'].append(citation_entry)\n","        else:\n","            citation_entry['quote'].append(quote)\n","\n","    # Process context data\n","    for context in result_obj['context']:\n","        context_id = context.metadata['id']\n","        context_page = context.metadata['page']\n","        source = context.metadata['source']\n","        title = context.metadata['title']\n","        page_content = context.page_content\n","\n","        # Match the context to the correct citation entry by source, title, id, and page\n","        if (source, title) in source_with_citations:\n","            for citation in source_with_citations[(source, title)]['citations']:\n","                if citation['id'] == context_id and citation['page'] == context_page:\n","                    # Apply highlighting for each quote in the citation's quote list\n","                    highlighted_content = page_content\n","                    for quote in citation['quote']:\n","                        highlighted_content = highlight_text(highlighted_content, quote)\n","                    citation['context'] = highlighted_content\n","\n","    # Convert the dictionary to a list of dictionaries for separate entries\n","    final_result_list = [\n","        {\n","            'title': details['title'],\n","            'source': details['source'],\n","            'citations': details['citations']\n","        }\n","        for details in source_with_citations.values()\n","    ]\n","\n","    return final_result_list\n"],"metadata":{"id":"docbPBPDxSVa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_cited_context(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kq8XyKnlJb4i","outputId":"29f2fb4c-a6a1-499a-ca06-9f39bc0dabd7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'title': 'Machine learning',\n","  'source': 'Wikipedia',\n","  'citations': [{'id': '564928',\n","    'page': 1,\n","    'quote': ['Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data.',\n","     'Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.',\n","     'Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.'],\n","    'context': 'Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. **They build a model from sample inputs**. Machine learning is done where designing and programming explicit algorithms cannot be done. **Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision**.'}]},\n"," {'title': 'Artificial intelligence',\n","  'source': 'Wikipedia',\n","  'citations': [{'id': '6360',\n","    'page': 1,\n","    'quote': [\"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers 'smart'.\"],\n","    'context': '**Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn**. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology.'}]}]"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def display_results(result_obj):\n","    print('Query:')\n","    display(Markdown(result_obj['question']))\n","    print()\n","    print('Response:')\n","    display(Markdown(result_obj['answer']))\n","    print('='*50)\n","    print('Sources:')\n","    cited_context = get_cited_context(result_obj)\n","    for source in cited_context:\n","        print('Title:', source['title'], ' ', 'Source:', source['source'])\n","        print('Citations:')\n","        for citation in source['citations']:\n","            print('ID:', citation['id'], ' ', 'Page:', citation['page'])\n","            print('Cited Quotes:')\n","            display(Markdown('*'+' '.join(citation['quote'])+'*'))\n","            print('Cited Context:')\n","            display(Markdown(citation['context']))\n","            print()\n"],"metadata":{"id":"aChUXboG903B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pG6fcxAE3I3G","outputId":"a37ea781-e6e3-49bb-fead-0a73881391ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is machine learning"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in the broader field of artificial intelligence (AI). Machine learning focuses on the study and construction of algorithms that can learn from and make predictions based on data. \n\nThese algorithms operate by following programmed instructions while also being capable of making predictions or decisions based on the data they process. They build a model from sample inputs, which allows them to perform tasks where traditional programming methods are insufficient. \n\nExamples of machine learning applications include:\n- Spam filtering\n- Detection of network intruders or malicious insiders\n- Optical character recognition (OCR)\n- Search engines\n- Computer vision\n\nOverall, machine learning enables systems to improve their performance on tasks over time as they are exposed to more data, making it a powerful tool in various domains."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Title: Machine learning   Source: Wikipedia\n","Citations:\n","ID: 564928   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. **They build a model from sample inputs**. Machine learning is done where designing and programming explicit algorithms cannot be done. **Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision**."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Artificial intelligence   Source: Wikipedia\n","Citations:\n","ID: 6360   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers 'smart'.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn**. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is AI, ML and DL?\"\n","result = rag_chain_w_citations.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rgTlW5hg_d0x","outputId":"2f15c49b-9e98-409b-bff8-0f0bcbd156ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is AI, ML and DL?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Artificial Intelligence (AI)**: AI is defined as the ability of a computer program or machine to think and learn. It is also a field of study aimed at making computers \"smart,\" allowing them to operate independently without being explicitly programmed with commands. The term was coined by John McCarthy in 1955. AI encompasses systems that can interpret external data, learn from it, and adapt to achieve specific goals. As technology advances, tasks once considered to require intelligence, such as optical character recognition, are no longer classified as AI but rather as routine technologies.\n\n**Machine Learning (ML)**: ML is a subfield of computer science that provides computers the ability to learn without being explicitly programmed. This concept emerged from the broader field of artificial intelligence. Machine learning focuses on the development of algorithms that can learn from and make predictions based on data. These algorithms can follow programmed instructions while also making decisions based on the data they process. ML is particularly useful in scenarios where traditional programming is impractical, such as spam filtering, network intrusion detection, and computer vision.\n\n**Deep Learning (DL)**: DL is a specialized subset of machine learning that primarily utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). It involves learning sessions that can be unsupervised, semi-supervised, or supervised. Deep learning excels in tasks that are challenging for computers, such as speech recognition, image understanding, and handwriting recognition. The architecture of deep learning models is inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Title: Artificial intelligence   Source: Wikipedia\n","Citations:\n","ID: 6360   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn**. It is also a field of study which tries to make computers \"smart\". **They work on their own without being encoded with commands**. **John McCarthy came up with the name \"Artificial Intelligence\" in 1955**. **In general use, the term \"artificial intelligence\" means a programme which mimics human cognition**. **At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do**. **Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation**. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental faculties once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an example of \"artificial intelligence\": it is just a routine technology."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Machine learning   Source: Wikipedia\n","Citations:\n","ID: 564928   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. **Such algorithms follow programmed instructions, but can also make predictions or decisions based on data**. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Deep learning   Source: Wikipedia\n","Citations:\n","ID: 663523   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks**. **As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised**. **In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer**. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"How is Machine learning related to supervised learning and clustering?\"\n","result = rag_chain_w_citations.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Zho3RfTQ-_WU","outputId":"16957e6c-e635-4482-cddc-ab4ba4529fff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"How is Machine learning related to supervised learning and clustering?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a broad field that encompasses various techniques and methodologies for enabling computers to learn from data. Within this field, supervised learning and clustering are two important approaches.\n\n### Supervised Learning\n- **Definition**: Supervised learning is a specific type of machine learning where the model is trained on labeled data. This means that the training dataset includes both the input data and the corresponding correct outputs (labels).\n- **Process**: The system infers a function from this labeled training data, learning how to map inputs to the correct outputs. The results of the training are known beforehand, allowing the system to learn to produce a \"classifier\" that can make predictions on new, unseen data.\n- **Inductive Reasoning**: Supervised learning typically employs inductive reasoning to generalize from the training data to make predictions on new data.\n\n### Clustering\n- **Definition**: Clustering, or cluster analysis, is a type of data analysis that involves grouping a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups.\n- **Relation to Machine Learning**: Clustering is often used in data mining and is considered an unsupervised learning technique because it does not rely on labeled data. Instead, it identifies patterns and structures in the data based solely on the inherent characteristics of the data points.\n\n### Relationship Between Machine Learning, Supervised Learning, and Clustering\n- **Machine Learning as an Umbrella**: Machine learning serves as the overarching field that includes various learning paradigms, including supervised learning and clustering.\n- **Supervised vs. Unsupervised**: While supervised learning requires labeled data and focuses on predicting outcomes based on that data, clustering operates without labels and focuses on discovering inherent groupings within the data.\n- **Applications**: Both supervised learning and clustering have practical applications in various domains, such as spam filtering (supervised) and customer segmentation (clustering).\n\nIn summary, machine learning encompasses both supervised learning, which relies on labeled data to train models, and clustering, which seeks to find natural groupings in data without predefined labels."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Title: Supervised learning   Source: Wikipedia\n","Citations:\n","ID: 359370   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**In machine learning, supervised learning is the task of inferring a function from labelled training data**. **The results of the training are known beforehand, the system simply learns how to get to these results correctly**. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Machine learning   Source: Wikipedia\n","Citations:\n","ID: 564928   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Machine learning gives computers the ability to learn without being explicitly programmed. It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Machine learning gives computers the ability to learn without being explicitly programmed** (Arthur Samuel, 1959). **It is a subfield of computer science**. **The idea came from work in artificial intelligence**. **Machine learning explores the study and construction of algorithms which can learn and make predictions on data**. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Cluster analysis   Source: Wikipedia\n","Citations:\n","ID: 593732   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Clustering or cluster analysis is a type of data analysis**. **The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way**. This is a common task in data mining."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Title: Reinforcement learning   Source: Wikipedia\n","Citations:\n","ID: 610032   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Reinforcement learning is different from supervised learning because the correct inputs and outputs are never shown.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Reinforcement learning (RL) is teaching a \"software agent\" how to behave in an environment by telling it how good it's doing. It is an area of machine learning inspired by behaviorist psychology. **Reinforcement learning is different from supervised learning because the correct inputs and outputs are never shown**. Also, reinforcement learning usually learns as it goes (online learning) unlike supervised learning. This means an agent has to choose between exploring and sticking with what it knows best. A reinforcement learning system is made of a \"policy\" (formula_1), a \"reward function\" (formula_2), a \"value function\" (formula_3), and an optional \"model\" of the environment."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"What is the difference between transformers and vision transformers?\"\n","result = rag_chain_w_citations.invoke(query)\n","display_results(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vy2zTEfAFxBo","outputId":"5b6b577f-9c3b-4ae0-f51b-4634b2584ddf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"What is the difference between transformers and vision transformers?"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Response:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between traditional Transformers and Vision Transformers (ViT) primarily lies in their application and the way they process input data.\n\n### Traditional Transformers\n- **Application**: Transformers were originally designed for natural language processing (NLP) tasks. They excel in handling sequential data, where the input is typically a sequence of tokens (words).\n- **Input Processing**: In NLP, the input is a 1D sequence of token embeddings. The architecture relies on self-attention mechanisms to capture relationships between tokens, allowing the model to understand context and dependencies in the text.\n\n### Vision Transformers (ViT)\n- **Application**: Vision Transformers adapt the Transformer architecture for image classification tasks. They aim to leverage the strengths of Transformers in a domain where convolutional neural networks (CNNs) have traditionally dominated.\n- **Input Processing**: Instead of processing a sequence of tokens, ViT processes images by splitting them into fixed-size patches. Each patch is treated as a token, and the sequence of these patch embeddings is fed into a standard Transformer encoder. This approach allows ViT to integrate information across the entire image, similar to how Transformers handle sequences in NLP.\n- **Architecture Modifications**: ViT includes additional components such as position embeddings to retain spatial information about the patches, and a classification token that helps in the final classification task.\n\n### Key Advantages of ViT\n- **Performance**: When pre-trained on large datasets, ViT can outperform traditional CNNs in terms of efficiency and scalability. It requires significantly less computational resources to achieve comparable or superior performance on image classification tasks.\n- **Global Context**: The self-attention mechanism in ViT allows it to capture global context across the entire image, which is a departure from the localized feature extraction typically seen in CNNs.\n\nIn summary, while traditional Transformers are designed for sequential data in NLP, Vision Transformers adapt this architecture for image data by treating image patches as tokens, allowing for effective image classification through global attention mechanisms."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==================================================\n","Sources:\n","Title: vision_transformer.pdf   Source: ./rag_docs/vision_transformer.pdf\n","Citations:\n","ID: 0a4aa65e-ba24-40c0-890f-7143ff77e6f4   Page: 0\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the introduction of the Vision Transformer (ViT) model, which applies a pure Transformer architecture to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional networks in computer vision and presents the advantages of using Transformers, particularly when pre-trained on large datasets. The section sets the stage for the subsequent exploration of ViT's performance compared to state-of-the-art convolutional networks. Published as a conference paper at ICLR 2021 AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗, Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,† ∗equal technical contribution, †equal advising Google Research, Brain Team {adosovitskiy, neilhoulsby}@google.com ABSTRACT **While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited**. **In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place**. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classiﬁcation tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring sub- stantially fewer computational resources to train.1 1 INTRODUCTION Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks to Transformers’ computational efﬁciency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the models and datasets growing, there is still no sign of saturating performance. In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet- like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Trans- former. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classiﬁcation in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these mod- els yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases 1Fine-tuning code and pre-trained models are available at https://github.com/ google-research/vision_transformer 1 arXiv:2010.11929v2 [cs.CV] 3 Jun 2021"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ID: 9e785ba7-2575-4e74-a3d6-6edefb9053e8   Page: 1\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the performance of the Vision Transformer (ViT) in image recognition tasks, highlighting its ability to achieve state-of-the-art results when pre-trained on large datasets like ImageNet-21k and JFT-300M. It contrasts the inductive biases of convolutional neural networks (CNNs) with the advantages of large-scale training for ViT, demonstrating its competitive accuracy across various benchmarks. Published as a conference paper at ICLR 2021 inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufﬁcient amounts of data. However, the picture changes if the models are trained on larger datasets (14M-300M images). We ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks. 2 RELATED WORK Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since be- come the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language mod- eling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020). Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self- attention in order to be applicable to images. An alternative way to scale attention is to apply it in blocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho et al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efﬁciently on hardware accelerators. Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well. There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu et al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ID: 9efab417-6594-4327-af0e-a8229e183a97   Page: 2\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*To handle 2D images, we reshape the image x ∈RH×W ×C into a sequence of flattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture. Published as a conference paper at ICLR 2021 Transformer Encoder MLP Head Vision Transformer (ViT) * Linear Projection of Flattened Patches * Extra learnable [ cl ass] embedding 1 2 3 4 5 6 7 8 9 0 Patch + Position Embedding Class Bird Ball Car ... Embedded Patches Multi-Head Attention Norm MLP Norm + L x + Transformer Encoder Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable “classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by Vaswani et al. (2017). 3 METHOD In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efﬁcient implementations – can be used almost out of the box. 3.1 VISION TRANSFORMER (VIT) An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a sequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2 is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size D through all of its layers, so we ﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings. Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed- ded patches (z0 0 = xclass), whose state at the output of the Transformer encoder (z0 L) serves as the image representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at- tached to z0 L. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at ﬁne-tuning time. Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed signiﬁcant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder. The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self- attention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019). 3"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ID: c23af995-486e-4fc9-8762-5fb229441a61   Page: 7\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer, suggesting that it may serve a similar function as early convolutional layers in CNNs.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on the behavior of attention mechanisms in the Vision Transformer (ViT), highlighting how attention distances vary across layers and the implications for feature extraction. It compares the attention patterns in ViT to those in hybrid models that incorporate convolutional networks, suggesting that early layers in CNNs may perform a similar role to localized attention in ViT. Additionally, it notes the relevance of attention distance to the model's ability to focus on semantically important regions for classification tasks. have consistently small attention distances in the low layers. This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right), suggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the attention distance increases with network depth. Globally, we ﬁnd that the model attends to image regions that are semantically relevant for classiﬁcation (Figure 6). 4.6 SELF-SUPERVISION Transformers show impressive performance on NLP tasks. However, much of their success stems not only from their excellent scalability but also from large scale self-supervised pre-training (Devlin 8"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ID: 38169299-0ef0-4af7-a2cb-506caedabf6d   Page: 7\n","Cited Quotes:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"*Vision Transformers dominate ResNets on the performance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same performance.*"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cited Context:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, evaluating their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability, while also discussing the implications for future model scaling efforts. Published as a conference paper at ICLR 2021 4.4 SCALING STUDY We perform a controlled scaling study of different models by evaluating transfer performance from JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1, R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre- trained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the end of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet backbone). Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5 for details on computational costs). Detailed results per model are provided in Table 6 in the Ap- pendix. A few patterns can be observed. First, **Vision Transformers dominate ResNets on the performance/compute trade-off**. **ViT uses approximately 2 −4× less compute to attain the same performance** (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu- tational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts. 4.5 INSPECTING VISION TRANSFORMER Input Attention Figure 6: Representative ex- amples of attention from the output token to the input space. See Appendix D.7 for details. To begin to understand how the Vision Transformer processes im- age data, we analyze its internal representations. The ﬁrst layer of the Vision Transformer linearly projects the ﬂattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin- cipal components of the the learned embedding ﬁlters. The com- ponents resemble plausible basis functions for a low-dimensional representation of the ﬁne structure within each patch. After the projection, a learned position embedding is added to the patch representations. Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position em- beddings, i.e. closer patches tend to have more similar position em- beddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology ex- plains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4). Self-attention allows ViT to integrate information across the entire image even in the lowest layers. We investigate to what degree the network makes use of this capability. Speciﬁcally, we compute the average distance in image space across which information is integrated, based on the attention weights (Figure 7, right). This “attention distance” is analogous to receptive ﬁeld size in CNNs. We ﬁnd that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]}]}