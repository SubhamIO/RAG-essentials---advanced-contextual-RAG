{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Build a Simple RAG System"],"metadata":{"id":"jbw4wHV4zlKj"}},{"cell_type":"markdown","source":["## Install OpenAI, and LangChain dependencies"],"metadata":{"id":"4vtFl39Ofu_8"}},{"cell_type":"code","source":["!pip install langchain==0.3.10\n","!pip install langchain-openai==0.2.12\n","!pip install langchain-community==0.3.11\n","!pip install jq==1.8.0\n","!pip install pymupdf==1.25.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d97580c-ad52-4a15-b9bf-39510105f657","id":"LVX6450Lfu_9","executionInfo":{"status":"ok","timestamp":1734093415950,"user_tz":-330,"elapsed":43125,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain==0.3.10 in /usr/local/lib/python3.10/dist-packages (0.3.10)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (3.11.10)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.22 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.22)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.10.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.18.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.10) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.2.2)\n","Collecting langchain-openai==0.2.12\n","  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.12) (0.3.22)\n","Collecting openai<2.0.0,>=1.55.3 (from langchain-openai==0.2.12)\n","  Downloading openai-1.57.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.12)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.1.147)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.10.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.0.0)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.12.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.66.6)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.32.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.2.3)\n","Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m823.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.57.3-py3-none-any.whl (390 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.54.5\n","    Uninstalling openai-1.54.5:\n","      Successfully uninstalled openai-1.54.5\n","Successfully installed langchain-openai-0.2.12 openai-1.57.3 tiktoken-0.8.0\n","Collecting langchain-community==0.3.11\n","  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (3.11.10)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.11)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.11)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n","  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community==0.3.11)\n","  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (1.26.4)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.11)\n","  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.18.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.10.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.11) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.27.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.2.2)\n","Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n","Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.22\n","    Uninstalling langchain-core-0.3.22:\n","      Successfully uninstalled langchain-core-0.3.22\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.10\n","    Uninstalling langchain-0.3.10:\n","      Successfully uninstalled langchain-0.3.10\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n","Collecting jq==1.8.0\n","  Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jq\n","Successfully installed jq-1.8.0\n","Collecting pymupdf==1.25.1\n","  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n","Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pymupdf\n","Successfully installed pymupdf-1.25.1\n"]}]},{"cell_type":"markdown","source":["## Install Chroma Vector DB and LangChain wrapper"],"metadata":{"id":"bwUBYHjPfu_-"}},{"cell_type":"code","source":["!pip install langchain-chroma==0.1.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6d6d5a2-8b70-4ffd-ef27-be2efa604bf4","id":"p30SmCgTfu__","executionInfo":{"status":"ok","timestamp":1734093448791,"user_tz":-330,"elapsed":32844,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-chroma==0.1.4\n","  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n","Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma==0.1.4)\n","  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n","Collecting fastapi<1,>=0.95.2 (from langchain-chroma==0.1.4)\n","  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma==0.1.4) (0.3.24)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma==0.1.4) (1.26.4)\n","Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.10.3)\n","Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n","Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n","Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.12.2)\n","Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.28.2)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.28.2)\n","Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.20.3)\n","Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.66.6)\n","Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.68.1)\n","Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.15.1)\n","Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (9.0.0)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.0.2)\n","Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10.12)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (13.9.4)\n","Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma==0.1.4)\n","  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.33)\n","Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (0.1.147)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (24.2)\n","Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (3.0.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.32.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.3)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.0.0)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.25.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.15)\n","Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.5.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.66.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n","Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.26.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.1)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.10.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.4.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.6.1)\n","Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n","Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n","Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n","Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n","Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n","Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n","Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n","Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n","Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=15e8642a4398d458bb6e4b769657a6643fca28d9efd8738eaf95d972ccc38d78\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built pypika\n","Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.25.5\n","    Uninstalling protobuf-4.25.5:\n","      Successfully uninstalled protobuf-4.25.5\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.28.2\n","    Uninstalling opentelemetry-api-1.28.2:\n","      Successfully uninstalled opentelemetry-api-1.28.2\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n","    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.28.2\n","    Uninstalling opentelemetry-sdk-1.28.2:\n","      Successfully uninstalled opentelemetry-sdk-1.28.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 langchain-chroma-0.1.4 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.41.3 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.3 websockets-14.1\n"]}]},{"cell_type":"markdown","source":["## Enter Open AI API Key"],"metadata":{"id":"EITC17hwfu__"}},{"cell_type":"code","source":["from getpass import getpass\n","\n","OPENAI_KEY = getpass('Enter Open AI API Key: ')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2dd74f07-c336-475b-fc07-02e468d9a94e","id":"yEh2olNvfvAA","executionInfo":{"status":"ok","timestamp":1734093457003,"user_tz":-330,"elapsed":3774,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Open AI API Key: ··········\n"]}]},{"cell_type":"markdown","source":["## Setup Environment Variables"],"metadata":{"id":"pm_mx0v-fvAA"}},{"cell_type":"code","source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = OPENAI_KEY"],"metadata":{"id":"Jhfb4gMUfvAC","executionInfo":{"status":"ok","timestamp":1734093460301,"user_tz":-330,"elapsed":356,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Open AI Embedding Models\n","\n","LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."],"metadata":{"id":"jiokYxD8fvAC"}},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","\n","# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n","openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"],"metadata":{"id":"-On4AS0HfvAD","executionInfo":{"status":"ok","timestamp":1734093468899,"user_tz":-330,"elapsed":2768,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Loading and Processing the Data"],"metadata":{"id":"afzeN_WkHIz2"}},{"cell_type":"markdown","source":["### Get the dataset"],"metadata":{"id":"RA_-hzHbFeSP"}},{"cell_type":"code","source":["# if you can't download using the following code\n","# go to https://drive.google.com/file/d/1aZxZejfteVuofISodUrY2CDoyuPLYDGZ download it\n","# manually upload it on colab\n","!gdown 1aZxZejfteVuofISodUrY2CDoyuPLYDGZ"],"metadata":{"id":"RZFMYH-yFhWn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ced58ba9-788f-4c1c-9701-75d610a4a430"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1aZxZejfteVuofISodUrY2CDoyuPLYDGZ\n","To: /content/rag_docs.zip\n","100% 5.92M/5.92M [00:00<00:00, 24.4MB/s]\n"]}]},{"cell_type":"code","source":["!unzip rag_docs.zip"],"metadata":{"id":"WwLEBC4nF9ly","colab":{"base_uri":"https://localhost:8080/"},"outputId":"984818fb-6dc8-4c57-8475-e71c169419dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  rag_docs.zip\n","   creating: rag_docs/\n","  inflating: rag_docs/attention_paper.pdf  \n","  inflating: rag_docs/cnn_paper.pdf  \n","  inflating: rag_docs/resnet_paper.pdf  \n","  inflating: rag_docs/vision_transformer.pdf  \n","  inflating: rag_docs/wikidata_rag_demo.jsonl  \n"]}]},{"cell_type":"markdown","source":["### Load and Process JSON Documents"],"metadata":{"id":"wMlxKZ_5jIdE"}},{"cell_type":"code","source":["from langchain.document_loaders import JSONLoader\n","\n","loader = JSONLoader(file_path='./rag_docs/wikidata_rag_demo.jsonl',\n","                    jq_schema='.',\n","                    text_content=False,\n","                    json_lines=True)\n","wiki_docs = loader.load()"],"metadata":{"id":"RZ5y0NfzHPhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(wiki_docs)"],"metadata":{"id":"G4E1zYFSG7J-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2bc928a-c812-4102-ddf5-83faa5c6faea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1801"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["wiki_docs[3]"],"metadata":{"id":"aSbhERAyGw0v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d80d2dd4-0452-4257-b64a-bfb48ec8cac7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': '/content/rag_docs/wikidata_rag_demo.jsonl', 'seq_num': 4}, page_content='{\"id\": \"71548\", \"title\": \"Chi-square distribution\", \"paragraphs\": [\"In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\\\u00a0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution.\", \"Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.\"]}')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import json\n","from langchain.docstore.document import Document\n","wiki_docs_processed = []\n","\n","for doc in wiki_docs:\n","    doc = json.loads(doc.page_content)\n","    metadata = {\n","        \"title\": doc['title'],\n","        \"id\": doc['id'],\n","        \"source\": \"Wikipedia\"\n","    }\n","    data = ' '.join(doc['paragraphs'])\n","    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"],"metadata":{"id":"yICyAF85h2DO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wiki_docs_processed[3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IATrHWKh7II","outputId":"c54ac927-dcc2-4401-d3ad-763f51cb12e7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'title': 'Chi-square distribution', 'id': '71548', 'source': 'Wikipedia'}, page_content='In probability theory and statistics, the chi-square distribution (also chi-squared or formula_1\\xa0 distribution) is one of the most widely used theoretical probability distributions. Chi-square distribution with formula_2 degrees of freedom is written as formula_3. It is a special case of gamma distribution. Chi-square distribution is primarily used in statistical significance tests and confidence intervals. It is useful, because it is relatively easy to show that certain probability distributions come close to it, under certain conditions. One of these conditions is that the null hypothesis must be true. Another one is that the different random variables (or observations) must be independent of each other.')"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### Load and Process PDF documents"],"metadata":{"id":"F_GzvHP1jSBo"}},{"cell_type":"markdown","source":["#### Create Simple Document Chunks for Standard Retrieval\n","\n","Here we just use simple chunking where each chunk is a fixed size of <= 3500 characters and overlap of 200 characters for any small isolated chunks (you can and should experiment with various chunk sizes and overlaps)"],"metadata":{"id":"RKNyuUpxq0xg"}},{"cell_type":"code","source":["from langchain.document_loaders import PyMuPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","def create_simple_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n","\n","    print('Loading pages:', file_path)\n","    loader = PyMuPDFLoader(file_path)\n","    doc_pages = loader.load()\n","\n","    print('Chunking pages:', file_path)\n","    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n","                                              chunk_overlap=chunk_overlap)\n","    doc_chunks = splitter.split_documents(doc_pages)\n","    print('Finished processing:', file_path)\n","    print()\n","    return doc_chunks"],"metadata":{"id":"bvBDsEQ4rFWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from glob import glob\n","\n","pdf_files = glob('./rag_docs/*.pdf')\n","pdf_files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b36d55c5-5d92-4936-98d3-0b20eb214e36","id":"j_8a-LAowhW7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./rag_docs/resnet_paper.pdf',\n"," './rag_docs/vision_transformer.pdf',\n"," './rag_docs/cnn_paper.pdf',\n"," './rag_docs/attention_paper.pdf']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["paper_docs = []\n","for fp in pdf_files:\n","    paper_docs.extend(create_simple_chunks(file_path=fp,\n","                                           chunk_size=3500,\n","                                           chunk_overlap=200))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"94f0418f-ada3-4deb-8e97-e9746288b2a4","id":"RDJMXBZQwhW8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading pages: ./rag_docs/resnet_paper.pdf\n","Chunking pages: ./rag_docs/resnet_paper.pdf\n","Finished processing: ./rag_docs/resnet_paper.pdf\n","\n","Loading pages: ./rag_docs/vision_transformer.pdf\n","Chunking pages: ./rag_docs/vision_transformer.pdf\n","Finished processing: ./rag_docs/vision_transformer.pdf\n","\n","Loading pages: ./rag_docs/cnn_paper.pdf\n","Chunking pages: ./rag_docs/cnn_paper.pdf\n","Finished processing: ./rag_docs/cnn_paper.pdf\n","\n","Loading pages: ./rag_docs/attention_paper.pdf\n","Chunking pages: ./rag_docs/attention_paper.pdf\n","Finished processing: ./rag_docs/attention_paper.pdf\n","\n"]}]},{"cell_type":"code","source":["len(paper_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HgbqBdu6wski","outputId":"d1263ca0-43fd-41de-8a4c-6965bd1210e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["80"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["paper_docs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHBBJdXXwtv1","outputId":"8acf0b24-0089-4df2-a89a-a6b20c757953"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': './rag_docs/resnet_paper.pdf', 'file_path': './rag_docs/resnet_paper.pdf', 'page': 0, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='Deep Residual Learning for Image Recognition\\nKaiming He\\nXiangyu Zhang\\nShaoqing Ren\\nJian Sun\\nMicrosoft Research\\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\\nAbstract\\nDeeper neural networks are more difﬁcult to train. We\\npresent a residual learning framework to ease the training\\nof networks that are substantially deeper than those used\\npreviously. We explicitly reformulate the layers as learn-\\ning residual functions with reference to the layer inputs, in-\\nstead of learning unreferenced functions. We provide com-\\nprehensive empirical evidence showing that these residual\\nnetworks are easier to optimize, and can gain accuracy from\\nconsiderably increased depth. On the ImageNet dataset we\\nevaluate residual nets with a depth of up to 152 layers—8×\\ndeeper than VGG nets [41] but still having lower complex-\\nity. An ensemble of these residual nets achieves 3.57% error\\non the ImageNet test set. This result won the 1st place on the\\nILSVRC 2015 classiﬁcation task. We also present analysis\\non CIFAR-10 with 100 and 1000 layers.\\nThe depth of representations is of central importance\\nfor many visual recognition tasks. Solely due to our ex-\\ntremely deep representations, we obtain a 28% relative im-\\nprovement on the COCO object detection dataset. Deep\\nresidual nets are foundations of our submissions to ILSVRC\\n& COCO 2015 competitions1, where we also won the 1st\\nplaces on the tasks of ImageNet detection, ImageNet local-\\nization, COCO detection, and COCO segmentation.\\n1. Introduction\\nDeep convolutional neural networks [22, 21] have led\\nto a series of breakthroughs for image classiﬁcation [21,\\n50, 40]. Deep networks naturally integrate low/mid/high-\\nlevel features [50] and classiﬁers in an end-to-end multi-\\nlayer fashion, and the “levels” of features can be enriched\\nby the number of stacked layers (depth). Recent evidence\\n[41, 44] reveals that network depth is of crucial importance,\\nand the leading results [41, 44, 13, 16] on the challenging\\nImageNet dataset [36] all exploit “very deep” [41] models,\\nwith a depth of sixteen [41] to thirty [16]. Many other non-\\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\\n1http://image-net.org/challenges/LSVRC/2015/\\nand\\nhttp://mscoco.org/dataset/#detections-challenge2015.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0 \\n10\\n20\\niter. (1e4)\\ntraining error (%)\\n \\n \\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n10\\n20\\niter. (1e4)\\ntest error (%)\\n \\n \\n56-layer\\n20-layer\\n56-layer\\n20-layer\\nFigure 1. Training error (left) and test error (right) on CIFAR-10\\nwith 20-layer and 56-layer “plain” networks. The deeper network\\nhas higher training error, and thus test error. Similar phenomena\\non ImageNet is presented in Fig. 4.\\ngreatly beneﬁted from very deep models.\\nDriven by the signiﬁcance of depth, a question arises: Is\\nlearning better networks as easy as stacking more layers?\\nAn obstacle to answering this question was the notorious\\nproblem of vanishing/exploding gradients [1, 9], which\\nhamper convergence from the beginning.\\nThis problem,\\nhowever, has been largely addressed by normalized initial-\\nization [23, 9, 37, 13] and intermediate normalization layers\\n[16], which enable networks with tens of layers to start con-\\nverging for stochastic gradient descent (SGD) with back-\\npropagation [22].\\nWhen deeper networks are able to start converging, a\\ndegradation problem has been exposed: with the network\\ndepth increasing, accuracy gets saturated (which might be\\nunsurprising) and then degrades rapidly.\\nUnexpectedly,\\nsuch degradation is not caused by overﬁtting, and adding')"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["### Combine all document chunks in one list"],"metadata":{"id":"UyPdlZo2xEly"}},{"cell_type":"code","source":["len(wiki_docs_processed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3nWkHfkw1_X","outputId":"b55f1341-3038-4ede-b69a-97e7c8fde38e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1801"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["total_docs = wiki_docs_processed + paper_docs\n","len(total_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jO0ShHaDw1_X","outputId":"5d10dac5-cded-4206-d9ee-eabb880811bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1881"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## Index Document Chunks and Embeddings in Vector DB\n","\n","Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."],"metadata":{"id":"Daqn6Hglw9Nk"}},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","# create vector DB of docs and embeddings - takes < 30s on Colab\n","chroma_db = Chroma.from_documents(documents=total_docs,\n","                                  collection_name='my_db',\n","                                  embedding=openai_embed_model,\n","                                  # need to set the distance function to cosine else it uses euclidean by default\n","                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n","                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n","                                  persist_directory=\"./my_db\")"],"metadata":{"id":"EYjyZdCyw9Nl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-0qbbpjw9Nl"},"source":["### Load Vector DB from disk\n","\n","This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PI3ITuGZw9Nl"},"outputs":[],"source":["# load from disk\n","chroma_db = Chroma(persist_directory=\"./my_db\",\n","                   collection_name='my_db',\n","                   embedding_function=openai_embed_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Udsb8xyVw9Nl","outputId":"7fa531a0-56da-44db-ac66-fbe486e5b8af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain_chroma.vectorstores.Chroma at 0x78f688238100>"]},"metadata":{},"execution_count":22}],"source":["chroma_db"]},{"cell_type":"markdown","source":["### Semantic Similarity based Retrieval\n","\n","We use simple cosine similarity here and retrieve the top 5 similar documents based on the user input query"],"metadata":{"id":"njfZOOVZxj1a"}},{"cell_type":"code","source":["similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n","                                              search_kwargs={\"k\": 5})"],"metadata":{"id":"tV1l6HYdxj1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def display_docs(docs):\n","    for doc in docs:\n","        print('Metadata:', doc.metadata)\n","        print('Content Brief:')\n","        display(Markdown(doc.page_content[:1000]))\n","        print()"],"metadata":{"id":"nUIJG_bDxj1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"what is machine learning?\"\n","top_docs = similarity_retriever.invoke(query)\n","display_docs(top_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":899},"id":"PIh4xGv2xj1c","outputId":"6f53620c-0ff4-4d61-974f-1cb185394e3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata: {'id': '564928', 'source': 'Wikipedia', 'title': 'Machine learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '359370', 'source': 'Wikipedia', 'title': 'Supervised learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '663523', 'source': 'Wikipedia', 'title': 'Deep learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '6360', 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': '44742', 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"what is the difference between transformers and vision transformers?\"\n","top_docs = similarity_retriever.invoke(query)\n","display_docs(top_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":953},"id":"S_PXFMcJxuyO","outputId":"79903c45-824d-451e-e4f7-cdc7a60aea31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 7, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Published as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone).\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per mode"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 0, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Published as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n∗equal technical contribution, †equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classiﬁcation tasks. When pre-traine"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 0, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"1Fine-tuning\ncode\nand\npre-trained\nmodels\nare\navailable\nat\nhttps://github.com/\ngoogle-research/vision_transformer\n1\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 1, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Published as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insufﬁcient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks.\n2\nRELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based mo"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'author': '', 'creationDate': 'D:20210604001958Z', 'creator': 'LaTeX with hyperref', 'file_path': './rag_docs/vision_transformer.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210604001958Z', 'page': 4, 'producer': 'pdfTeX-1.40.21', 'source': './rag_docs/vision_transformer.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': ''}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Published as a conference paper at ICLR 2021\nModel\nLayers\nHidden size D\nMLP size\nHeads\nParams\nViT-Base\n12\n768\n3072\n12\n86M\nViT-Large\n24\n1024\n4096\n16\n307M\nViT-Huge\n32\n1280\n5120\n16\n632M\nTable 1: Details of Vision Transformer model variants.\nWe also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\nimagery, and Structured – tasks that require geometric understanding like localization.\nModel Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\nsummarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\nadd the larger “Huge” model. In what follows we use brief notation to indicate the model size and\nthe input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch siz"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"markdown","source":["## Build the RAG Pipeline"],"metadata":{"id":"gQFWv7YUyVII"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer detailed and well formatted based on the information from the context.\n","\n","                Question:\n","                {question}\n","\n","                Context:\n","                {context}\n","\n","                Answer:\n","            \"\"\"\n","\n","rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"],"metadata":{"id":"PHOrfGXKyVIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","qa_rag_chain = (\n","    {\n","        \"context\": (similarity_retriever\n","                      |\n","                    format_docs),\n","        \"question\": RunnablePassthrough()\n","    }\n","      |\n","    rag_prompt_template\n","      |\n","    chatgpt\n",")"],"metadata":{"id":"KmWeCB4yyVIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","query = \"What is machine learning?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"outputId":"9f8c87a6-077c-4b11-de39-a387b7b0e161","id":"xvj_eGIWyVIJ"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\n\nMachine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Common applications include spam filtering, detecting network intruders or malicious insiders, optical character recognition (OCR), search engines, and computer vision.\n\nWithin machine learning, there are different types of learning approaches, such as supervised learning, where a function is inferred from labeled training data. In this case, the system learns to produce correct results based on known outcomes, typically using vectors for training data and results to create a \"classifier.\" Inductive reasoning is often employed to generalize from the training data.\n\nAdditionally, deep learning is a specialized area of machine learning that utilizes neural networks, particularly those with multiple layers (known as multi-layer neural networks). Deep learning is effective for complex tasks like speech recognition, image understanding, and handwriting recognition, which are challenging for computers but relatively easy for humans. These models are inspired by the information processing patterns of biological nervous systems, although they differ significantly from the structural and functional properties of human brains.\n\nIn summary, machine learning enables computers to learn from data, adapt their behavior, and make predictions, playing a crucial role in the development of intelligent systems."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is a CNN?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":675},"id":"pXtezDlZzadt","outputId":"fdec5afe-f61c-436e-a5b4-c87319222292"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A CNN, or Convolutional Neural Network, is a type of artificial neural network primarily used for image-driven pattern recognition tasks. CNNs are designed to process data with a grid-like topology, such as images, and they consist of three main types of layers: convolutional layers, pooling layers, and fully-connected layers.\n\n### Key Features of CNNs:\n\n1. **Architecture**:\n   - CNNs are structured to handle the spatial dimensionality of input data, which includes height, width, and depth (for color channels in images).\n   - The architecture typically involves stacking multiple convolutional layers followed by pooling layers, which helps in reducing the dimensionality of the data while retaining important features.\n\n2. **Convolutional Layers**:\n   - These layers apply a convolution operation to the input, which involves sliding a filter (or kernel) over the input image to produce feature maps. Each neuron in a convolutional layer is connected to a small region of the input, allowing the network to learn spatial hierarchies of features.\n\n3. **Pooling Layers**:\n   - Pooling layers are used to down-sample the feature maps, reducing their dimensionality and helping to make the representation more manageable. This also aids in making the model invariant to small translations in the input.\n\n4. **Fully-Connected Layers**:\n   - After several convolutional and pooling layers, the high-level reasoning in the neural network is done through fully-connected layers, where every neuron is connected to every neuron in the previous layer.\n\n5. **Activation Functions**:\n   - CNNs commonly use activation functions like the Rectified Linear Unit (ReLU) to introduce non-linearity into the model, which helps in learning complex patterns.\n\n6. **Efficiency**:\n   - CNNs are particularly efficient for image processing tasks because they reduce the number of parameters compared to traditional fully connected networks, making them less prone to overfitting and more computationally efficient.\n\n7. **Applications**:\n   - CNNs are widely used in various applications, including image classification, object detection, and image segmentation, due to their ability to automatically learn and extract features from images.\n\nIn summary, CNNs are a powerful class of neural networks specifically designed for processing and analyzing visual data, leveraging their unique architecture to effectively learn from images."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How is a resnet better than a CNN?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"id":"27ZLcajnEGKt","outputId":"c150dc8f-4d12-4279-b34f-da1c60ce1876"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A ResNet (Residual Network) is considered better than a traditional CNN (Convolutional Neural Network) for several reasons, primarily related to its architecture and training efficiency:\n\n1. **Overcoming Optimization Difficulties**: Traditional deep CNNs often face optimization challenges as their depth increases, leading to higher training errors. In contrast, ResNets utilize shortcut connections that allow gradients to flow more easily during backpropagation. This helps mitigate issues like vanishing gradients, enabling deeper networks to be trained effectively.\n\n2. **Improved Accuracy with Depth**: ResNets can achieve better performance as they increase in depth. For instance, a 34-layer ResNet outperforms an 18-layer ResNet by 2.8% in top-1 error on the ImageNet validation set. This is significant because traditional networks often experience degradation in performance as they become deeper, but ResNets do not exhibit this degradation problem.\n\n3. **Lower Training Error**: ResNets demonstrate considerably lower training errors compared to their plain counterparts. For example, the 34-layer ResNet shows a marked improvement in training error over a 34-layer plain network, indicating that ResNets are more effective at learning from the training data.\n\n4. **Parameter Efficiency**: Despite being deeper, ResNets do not require additional parameters compared to traditional networks. This efficiency allows them to maintain lower complexity while achieving higher accuracy. For instance, a 152-layer ResNet has fewer parameters than VGG-16/19 networks, yet it achieves better performance.\n\n5. **Generalization**: ResNets have shown to generalize better across various tasks, including object detection and localization. The architecture's ability to learn robust features contributes to its success in competitions, such as winning first place in multiple tracks of the ILSVRC & COCO 2015 competitions.\n\nIn summary, ResNets improve upon traditional CNNs by effectively addressing optimization challenges, allowing for deeper architectures without performance degradation, achieving lower training errors, maintaining parameter efficiency, and demonstrating superior generalization capabilities."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is NLP and its relation to linguistics?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"J5IQoBc0zlAr","outputId":"4a3af9b5-f867-43c3-c1a6-f1cba4e66fbe"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Natural Language Processing (NLP) is a field within Artificial Intelligence that focuses on enabling computers to automatically understand and generate human languages. The term \"Natural Language\" specifically refers to human languages, distinguishing them from programming languages. The overarching goal of NLP is to facilitate seamless interaction between humans and computers through language.\n\nNLP is closely related to linguistics, which is the scientific study of language and its structure. Linguistics provides the foundational theories and frameworks that inform the development of NLP technologies. By leveraging insights from linguistics, NLP aims to decode the complexities of human language, including syntax, semantics, and pragmatics, to improve the accuracy and effectiveness of language processing tasks.\n\nIn summary, NLP is a crucial intersection of Artificial Intelligence and linguistics, aimed at programming computers to understand and communicate in human languages."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is the difference between AI, ML and DL?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":493},"id":"AzeZuG1hzvGy","outputId":"a37719fd-ea9b-4dd1-b321-f18733cff3a4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between AI, ML, and DL can be summarized as follows:\n\n### Artificial Intelligence (AI)\n- **Definition**: AI refers to the ability of a computer program or machine to think and learn, mimicking human cognition. It encompasses a broad range of technologies and applications that enable machines to perform tasks that typically require human intelligence.\n- **Scope**: AI is a field of study aimed at creating systems that can interpret external data, learn from it, and adapt to achieve specific goals. It includes various subfields, one of which is machine learning.\n- **Examples**: AI applications can range from simple rule-based systems to complex algorithms that can learn and adapt over time.\n\n### Machine Learning (ML)\n- **Definition**: ML is a subfield of AI that focuses on the study and construction of algorithms that allow computers to learn from and make predictions based on data without being explicitly programmed.\n- **Functionality**: ML algorithms build models from sample inputs and can make predictions or decisions based on new data. It is particularly useful in scenarios where traditional programming is impractical.\n- **Examples**: Applications of ML include spam filtering, network intrusion detection, optical character recognition (OCR), and computer vision.\n\n### Deep Learning (DL)\n- **Definition**: DL is a specialized subset of machine learning that primarily uses neural networks with multiple layers (deep neural networks) to process data.\n- **Characteristics**: In deep learning, the information processed becomes more abstract with each added layer, allowing for complex tasks such as speech and image recognition. It is inspired by the information processing patterns of biological nervous systems.\n- **Examples**: DL is often used in applications that require high levels of abstraction, such as recognizing speech, understanding images, and handwriting recognition.\n\nIn summary, AI is the overarching field that includes both ML and DL, with ML being a method of achieving AI through data-driven learning, and DL being a more advanced technique within ML that utilizes deep neural networks."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is the difference between transformers and vision transformers?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":405},"id":"cWihDiL3zPzY","outputId":"ddb3e888-d504-46b1-c51d-a65c2407b8d8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The difference between transformers and vision transformers primarily lies in their application and input processing methods.\n\n1. **Transformers**: Originally designed for natural language processing (NLP), transformers operate on sequences of tokens (words) and utilize self-attention mechanisms to capture relationships between these tokens. The standard transformer architecture processes input data in a way that allows each token to attend to every other token, which is computationally efficient for text data but can be challenging for high-dimensional data like images.\n\n2. **Vision Transformers (ViT)**: Vision transformers adapt the transformer architecture for image data by treating image patches as sequences of tokens. Instead of processing individual pixels, an image is divided into smaller patches (e.g., 16x16 pixels), which are then flattened and linearly embedded into a lower-dimensional space. These embeddings are treated similarly to tokens in NLP, allowing the model to apply self-attention across the entire image. This approach enables vision transformers to integrate information globally, even in the early layers of the model.\n\nKey distinctions include:\n- **Input Representation**: Transformers work with sequences of words, while vision transformers work with sequences of image patches.\n- **Attention Mechanism**: Vision transformers utilize self-attention to capture relationships across the entire image, which is different from the traditional token-to-token attention in standard transformers.\n- **Inductive Bias**: Traditional CNNs (Convolutional Neural Networks) have built-in inductive biases such as locality and translation equivariance, which are not inherently present in transformers. Vision transformers, however, can achieve competitive performance on image classification tasks when trained on large datasets, demonstrating that large-scale training can overcome the lack of these biases.\n\nIn summary, while both architectures leverage self-attention, vision transformers are specifically tailored for image data by processing image patches as sequences, allowing them to effectively handle visual tasks."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How is self-attention important in transformers?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"mEHwbeaGEbFn","outputId":"d6abe412-4d3a-46a9-b44b-33041b86fdf8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Self-attention is crucial in transformers for several reasons:\n\n1. **Global Information Integration**: Self-attention allows the model to integrate information across the entire input, such as an image, even in the lowest layers. This capability enables the model to attend to relevant regions of the input that are semantically important for tasks like classification. For instance, some attention heads can focus on most of the image early in the processing, demonstrating the model's ability to capture global context.\n\n2. **Attention Mechanism**: The attention function in transformers maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values based on the attention scores derived from the queries and keys. This mechanism allows the model to dynamically focus on different parts of the input based on their relevance to the task at hand.\n\n3. **Layer-wise Attention Dynamics**: The attention distance, which refers to how far the model looks across the input to integrate information, tends to increase with the depth of the network. This means that as the data passes through more layers, the model can consider a broader context, enhancing its understanding and representation of the input.\n\n4. **Comparison with CNNs**: Unlike convolutional neural networks (CNNs), which rely on local receptive fields and translation equivariance, transformers using self-attention do not have these inherent biases. This allows transformers to generalize better, especially when trained on large datasets, as they can learn to attend to relevant features regardless of their spatial arrangement.\n\n5. **Scalability and Efficiency**: The self-attention mechanism is designed to be scalable, allowing transformers to handle larger inputs without the quadratic cost associated with naive pixel-wise attention. This scalability is essential for applying transformers to high-dimensional data like images.\n\nIn summary, self-attention is a foundational component of transformers that enables them to effectively process and understand complex inputs by integrating information globally, dynamically focusing on relevant features, and scaling efficiently to larger datasets."},"metadata":{}}]},{"cell_type":"code","source":["query = \"How does a resnet work?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":680},"id":"PXu21fRhGxgT","outputId":"46c19bef-3ae3-4eca-863c-57dab5a8f6c5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"A ResNet, or Residual Network, operates on the principle of residual learning, which addresses the degradation problem often encountered in deep neural networks. Here’s a detailed explanation of how a ResNet works based on the provided context:\n\n### Key Concepts of ResNet\n\n1. **Residual Learning Framework**:\n   - Instead of directly learning the desired underlying mapping \\( H(x) \\), ResNets learn a residual mapping \\( F(x) = H(x) - x \\). This reformulation allows the network to focus on learning the difference between the desired output and the input, which is often easier than learning the output directly.\n\n2. **Shortcut Connections**:\n   - ResNets incorporate shortcut connections that skip one or more layers. These connections perform identity mapping, meaning they pass the input \\( x \\) directly to the output of the stacked layers. The output of the stacked layers is then added to this input, resulting in the equation \\( F(x) + x \\).\n   - These shortcuts do not introduce additional parameters or computational complexity, allowing the network to maintain efficiency while increasing depth.\n\n3. **Optimization Benefits**:\n   - The architecture of ResNets allows for easier optimization of very deep networks. Traditional deep networks (plain networks) tend to suffer from increased training error as depth increases, but ResNets can achieve lower training errors even with greater depth.\n   - This is evidenced by the performance of ResNets on datasets like ImageNet, where deeper ResNets (e.g., 34-layer, 50-layer, and even 152-layer) show significant accuracy improvements compared to their plain counterparts.\n\n4. **Architecture**:\n   - ResNets are built using blocks of convolutional layers, typically with 3x3 filters, and include batch normalization and ReLU activation functions. The architecture can vary in depth, with configurations such as 18, 34, 50, 101, and 152 layers.\n   - The network structure includes bottleneck layers, which are designed to reduce the number of parameters while maintaining performance.\n\n5. **Training and Performance**:\n   - ResNets are trained using standard techniques such as stochastic gradient descent (SGD) with momentum and weight decay. They have been shown to converge well, even with a large number of layers, and outperform traditional networks in terms of accuracy and generalization.\n\n### Conclusion\nIn summary, ResNets leverage the concept of residual learning through the use of shortcut connections, allowing for the effective training of very deep networks. This architecture not only mitigates the degradation problem but also enhances the model's ability to learn complex mappings, leading to superior performance on various image recognition tasks."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is LangGraph?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"UbaUpVXKz8IK","outputId":"93b846c2-c83c-4d61-a8d9-7f3a60d8450c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I don't know."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is an Agentic AI System?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"FmxNo6B50AdP","outputId":"3edec5f0-5ae7-4928-b86f-1c51134500bd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."},"metadata":{}}]},{"cell_type":"code","source":["query = \"What is LangChain?\"\n","result = qa_rag_chain.invoke(query)\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"Ttz53mEy0J_D","outputId":"bf628efb-07ee-4695-aa4f-2f036694fdf1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I don't know."},"metadata":{}}]}]}