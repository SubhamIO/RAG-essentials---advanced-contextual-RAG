{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"78edb9b2b80c483ab1d65a5e6ac7b00c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0566c23e5037436e97b5178282b12628","IPY_MODEL_a6768074f3ea4cb4b6bf34fd3ec588d2","IPY_MODEL_4d02d332124e489194212dd05a80af68"],"layout":"IPY_MODEL_a73d64e7f0d64d3ab8e7c626193f44f9"}},"0566c23e5037436e97b5178282b12628":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ab47828042947ceb874f360a728d2e9","placeholder":"​","style":"IPY_MODEL_88ec20625df54005a0e6623af57f0b9d","value":"Evaluating: 100%"}},"a6768074f3ea4cb4b6bf34fd3ec588d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a336914f145e42c0a1df04bfb0aa0f61","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d8b6013a8734c3cb2848eee9e7597ff","value":1}},"4d02d332124e489194212dd05a80af68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d7f50e073df423b9e10770933466576","placeholder":"​","style":"IPY_MODEL_b64d491c187c4506942bd5ecb013d313","value":" 1/1 [00:37&lt;00:00, 37.09s/it]"}},"a73d64e7f0d64d3ab8e7c626193f44f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ab47828042947ceb874f360a728d2e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88ec20625df54005a0e6623af57f0b9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a336914f145e42c0a1df04bfb0aa0f61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d8b6013a8734c3cb2848eee9e7597ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d7f50e073df423b9e10770933466576":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b64d491c187c4506942bd5ecb013d313":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9be2346fdf034f3fae44bae5649ea544":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_945dfa6461a14d0a99a207a600be9f99","IPY_MODEL_58a73767e7e34033b80fc8ff6dccd644","IPY_MODEL_3483a09e68ad423d8285c88b0023c0da"],"layout":"IPY_MODEL_9bf97bf583fd4b6f9b8dbe7667d56863"}},"945dfa6461a14d0a99a207a600be9f99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7d70159f003466996088b318a7ed06d","placeholder":"​","style":"IPY_MODEL_f01d72a1d5514dc1a6c35f95cdb04c78","value":"Evaluating: 100%"}},"58a73767e7e34033b80fc8ff6dccd644":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4113a8c3d98d4ac6a55cb7d887fa530c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89bfda1f10344e03a5f132feea2c9bfb","value":1}},"3483a09e68ad423d8285c88b0023c0da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b64ddd749fe4d14bc15b0bf89c698ed","placeholder":"​","style":"IPY_MODEL_af2ae9a883e64c6bbed599f54b6c6915","value":" 1/1 [00:35&lt;00:00, 35.59s/it]"}},"9bf97bf583fd4b6f9b8dbe7667d56863":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7d70159f003466996088b318a7ed06d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f01d72a1d5514dc1a6c35f95cdb04c78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4113a8c3d98d4ac6a55cb7d887fa530c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89bfda1f10344e03a5f132feea2c9bfb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b64ddd749fe4d14bc15b0bf89c698ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af2ae9a883e64c6bbed599f54b6c6915":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"153226563d454cbd92ed327d61cbcf68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59275c721a1849e58c3efc0c339f8954","IPY_MODEL_5ca4e01efe6d44eea9d60e5b9c9080c2","IPY_MODEL_3aee75144e004e8dbcfe75abc6762208"],"layout":"IPY_MODEL_ba7fd441525a45efb8d4bce5a7362a03"}},"59275c721a1849e58c3efc0c339f8954":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c147e908b1b49fbb52d6820495afab7","placeholder":"​","style":"IPY_MODEL_9fddccfcf6a843e69bf3b1e631aca9e1","value":"Evaluating: 100%"}},"5ca4e01efe6d44eea9d60e5b9c9080c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeae6addc36447bd9c82d2b4b4fa99bb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69f10456c157455d8a7ec9fd9cdf28f6","value":1}},"3aee75144e004e8dbcfe75abc6762208":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b53bc9dbfe84a83beec08350f953df4","placeholder":"​","style":"IPY_MODEL_19d664186c594a5a81be22093ea94717","value":" 1/1 [00:34&lt;00:00, 34.01s/it]"}},"ba7fd441525a45efb8d4bce5a7362a03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c147e908b1b49fbb52d6820495afab7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fddccfcf6a843e69bf3b1e631aca9e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eeae6addc36447bd9c82d2b4b4fa99bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69f10456c157455d8a7ec9fd9cdf28f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b53bc9dbfe84a83beec08350f953df4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19d664186c594a5a81be22093ea94717":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"613a3541acae40d38eb8898a78acb445":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b3242c9b3ef4964aa9578af5775a396","IPY_MODEL_5ca5882b64ac4dafb27da9189802d36c","IPY_MODEL_e53c7c33e8d244c3ab1f3a29318e0bcf"],"layout":"IPY_MODEL_072bdd9321e644cc8f25c2749339e083"}},"0b3242c9b3ef4964aa9578af5775a396":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c9ed88787be4036a623d4a7088c40b8","placeholder":"​","style":"IPY_MODEL_2b4046d510a34e38bdd095ca905abc9b","value":"Evaluating: 100%"}},"5ca5882b64ac4dafb27da9189802d36c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88bcb61cb34e4cdaae01127aaa76fb07","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fae02453a4164ba39f26445cc1a01a72","value":1}},"e53c7c33e8d244c3ab1f3a29318e0bcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f3fa03415044d79b0db289024d882de","placeholder":"​","style":"IPY_MODEL_4fcb9e84ee2e4e909f34f4a8fa403ca2","value":" 1/1 [00:32&lt;00:00, 32.50s/it]"}},"072bdd9321e644cc8f25c2749339e083":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c9ed88787be4036a623d4a7088c40b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b4046d510a34e38bdd095ca905abc9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88bcb61cb34e4cdaae01127aaa76fb07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fae02453a4164ba39f26445cc1a01a72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f3fa03415044d79b0db289024d882de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fcb9e84ee2e4e909f34f4a8fa403ca2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff0b25a6b0364081907ec87b853ea6a4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_024d803bdab24516b6770b101969b8e1","IPY_MODEL_1e14a7aa053449f29d94620f654b8fdd","IPY_MODEL_67554dce2ed147daa1e1f6887b706256"],"layout":"IPY_MODEL_9eeb2fb2fa444982acf1e9b95e95703e"}},"024d803bdab24516b6770b101969b8e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9dd4eebc151451085de50702f21a311","placeholder":"​","style":"IPY_MODEL_6caddec7dc934425bf56010398936537","value":"Evaluating: 100%"}},"1e14a7aa053449f29d94620f654b8fdd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_90441504bc7b44898eabe5b78fb6a5ec","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7e2ef49e4b44aec89fa39ed9799cf59","value":1}},"67554dce2ed147daa1e1f6887b706256":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fee5fde75c34cd6999b57d4374dd9c7","placeholder":"​","style":"IPY_MODEL_28fd1350a29149148a0083ff151ee2b1","value":" 1/1 [00:31&lt;00:00, 31.89s/it]"}},"9eeb2fb2fa444982acf1e9b95e95703e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9dd4eebc151451085de50702f21a311":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6caddec7dc934425bf56010398936537":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90441504bc7b44898eabe5b78fb6a5ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7e2ef49e4b44aec89fa39ed9799cf59":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9fee5fde75c34cd6999b57d4374dd9c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28fd1350a29149148a0083ff151ee2b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af5f81ba63744f48969b6230f0e1196d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d76b5cc2bf724998afecba83dd9917c2","IPY_MODEL_bf94afe4446b49b78c325ed123805c29","IPY_MODEL_92df8d04435244659d3a4b4773d619e2"],"layout":"IPY_MODEL_db9c163fe2674abf94bf676d6ca94f0e"}},"d76b5cc2bf724998afecba83dd9917c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_968da5427c57445fa41933ad7aec061a","placeholder":"​","style":"IPY_MODEL_183cf676e13d470c9c0db082b89256ff","value":"Evaluating: 100%"}},"bf94afe4446b49b78c325ed123805c29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db86b4cba7b4402fa8563922ca6e9908","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14f35227c96342dba5aa8b0db9cdabf0","value":1}},"92df8d04435244659d3a4b4773d619e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d46735abc034aa3817f3ef7bd70a0e6","placeholder":"​","style":"IPY_MODEL_fae240f399c44cbd9bb14e2d0ce24013","value":" 1/1 [00:28&lt;00:00, 28.64s/it]"}},"db9c163fe2674abf94bf676d6ca94f0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"968da5427c57445fa41933ad7aec061a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"183cf676e13d470c9c0db082b89256ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db86b4cba7b4402fa8563922ca6e9908":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14f35227c96342dba5aa8b0db9cdabf0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d46735abc034aa3817f3ef7bd70a0e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fae240f399c44cbd9bb14e2d0ce24013":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"915a91064f44458eac9dee659df36362":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a4b4375488a447b9f483798173b5560","IPY_MODEL_654a300e0e624582aad9ce36720cc8d3","IPY_MODEL_d3d5b7ec3dd64df69f6b17dd3fb1188e"],"layout":"IPY_MODEL_c9e569343f6a4cb1b100356cfd56c617"}},"7a4b4375488a447b9f483798173b5560":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48b6fa2757de4c9aa48c429234552b7a","placeholder":"​","style":"IPY_MODEL_89c18cb46f564cb8b4d390cdb75e27a1","value":"Evaluating: 100%"}},"654a300e0e624582aad9ce36720cc8d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba82e24b119d4eb086248ba70f6adcba","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20174965e93c4d9e96a8b2567cda17cf","value":1}},"d3d5b7ec3dd64df69f6b17dd3fb1188e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c7afd75619d43c4b2debae90e9adc2d","placeholder":"​","style":"IPY_MODEL_bf93a40cc97f439b82bc2e51478e56d1","value":" 1/1 [00:28&lt;00:00, 28.27s/it]"}},"c9e569343f6a4cb1b100356cfd56c617":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48b6fa2757de4c9aa48c429234552b7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89c18cb46f564cb8b4d390cdb75e27a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba82e24b119d4eb086248ba70f6adcba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20174965e93c4d9e96a8b2567cda17cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c7afd75619d43c4b2debae90e9adc2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf93a40cc97f439b82bc2e51478e56d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55c66431dcad49088e1c81e2f1a59d68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b789175b58a413d8b4eeb4200854d7f","IPY_MODEL_66872cc0ee164caca99347f90386044b","IPY_MODEL_6d9cd0ea46f34a9faed020f53ada73ab"],"layout":"IPY_MODEL_6a52685481b54fcc9521f61e5d50119f"}},"6b789175b58a413d8b4eeb4200854d7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60422854823d4a1cb19377f25a12ec51","placeholder":"​","style":"IPY_MODEL_a3dca1dc3d3045e79587e5593e0ea74f","value":"Evaluating: 100%"}},"66872cc0ee164caca99347f90386044b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fb30db42d5b49418fdf8de0fad705a9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a35b24a41e5748b8930fa70d6e795504","value":1}},"6d9cd0ea46f34a9faed020f53ada73ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce21520c9bac4044a25157662b091a11","placeholder":"​","style":"IPY_MODEL_66e0a0826a434c8db7f356c1a5aa8575","value":" 1/1 [00:22&lt;00:00, 22.26s/it]"}},"6a52685481b54fcc9521f61e5d50119f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60422854823d4a1cb19377f25a12ec51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3dca1dc3d3045e79587e5593e0ea74f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fb30db42d5b49418fdf8de0fad705a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a35b24a41e5748b8930fa70d6e795504":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce21520c9bac4044a25157662b091a11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66e0a0826a434c8db7f356c1a5aa8575":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02165f6fcfb74fafb5206b582f628a93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3687d4444143421eae1d8c8231bf952d","IPY_MODEL_51a81caf430147789513fc3485416b33","IPY_MODEL_903e7d454cdc48979146cc88183042d7"],"layout":"IPY_MODEL_c2f096e13f164acab1a66d96659249bd"}},"3687d4444143421eae1d8c8231bf952d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3c0b9000c3b4789b7a157459d4bd9c1","placeholder":"​","style":"IPY_MODEL_c8a18774fec54c0cb290edf5ffeb7c1a","value":"Evaluating: 100%"}},"51a81caf430147789513fc3485416b33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_079bf0f9293c43d988315ee189ff7c73","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3772f26e95a3478894125d474a1bd5e3","value":1}},"903e7d454cdc48979146cc88183042d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42a91d50bb494be785a10dc07e64c57d","placeholder":"​","style":"IPY_MODEL_091f150610b0474c8b4227706e6c3878","value":" 1/1 [00:24&lt;00:00, 24.16s/it]"}},"c2f096e13f164acab1a66d96659249bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3c0b9000c3b4789b7a157459d4bd9c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8a18774fec54c0cb290edf5ffeb7c1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"079bf0f9293c43d988315ee189ff7c73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3772f26e95a3478894125d474a1bd5e3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42a91d50bb494be785a10dc07e64c57d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"091f150610b0474c8b4227706e6c3878":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4380c3157b2641949101c333de2860c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00ef1659ddf440ac829bc99c0dcb1baf","IPY_MODEL_d8b78b5823ad4efd9be2be03346d3009","IPY_MODEL_9c6494c628cd4f33a41869f5044e1567"],"layout":"IPY_MODEL_77814ee8e544404fa75ab09334721d43"}},"00ef1659ddf440ac829bc99c0dcb1baf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b3a2dc7dbf94a068ac59a1c71b393f6","placeholder":"​","style":"IPY_MODEL_ae849ff42adb4cfc9a82c45a28da07a8","value":"Evaluating: 100%"}},"d8b78b5823ad4efd9be2be03346d3009":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb9a2dedfed946069909eac89cd5fb0e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9a4a518fd8642598f5f51bb5d3c3091","value":1}},"9c6494c628cd4f33a41869f5044e1567":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4deb2d112aa4427488bc32582289caa0","placeholder":"​","style":"IPY_MODEL_ed63af1291e140b68eb813f6525f4f88","value":" 1/1 [00:17&lt;00:00, 17.35s/it]"}},"77814ee8e544404fa75ab09334721d43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b3a2dc7dbf94a068ac59a1c71b393f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae849ff42adb4cfc9a82c45a28da07a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb9a2dedfed946069909eac89cd5fb0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9a4a518fd8642598f5f51bb5d3c3091":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4deb2d112aa4427488bc32582289caa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed63af1291e140b68eb813f6525f4f88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Build a Simple RAG System"],"metadata":{"id":"jbw4wHV4zlKj"}},{"cell_type":"markdown","source":["## Install OpenAI, and LangChain dependencies"],"metadata":{"id":"4vtFl39Ofu_8"}},{"cell_type":"code","source":["!pip install langchain==0.3.10\n","!pip install langchain-openai==0.2.12\n","!pip install langchain-community==0.3.11\n","!pip install dill"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e27aa4b8-b2b3-453e-9b0e-50c3cfa6373e","id":"LVX6450Lfu_9","executionInfo":{"status":"ok","timestamp":1734095199040,"user_tz":-330,"elapsed":37138,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain==0.3.10 in /usr/local/lib/python3.10/dist-packages (0.3.10)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (3.11.10)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.22 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.22)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.10.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.18.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.10) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.2.2)\n","Collecting langchain-openai==0.2.12\n","  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.12) (0.3.22)\n","Collecting openai<2.0.0,>=1.55.3 (from langchain-openai==0.2.12)\n","  Downloading openai-1.57.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.12)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.1.147)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.10.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.0.0)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.12.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.66.6)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.32.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.2.3)\n","Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.57.3-py3-none-any.whl (390 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.54.5\n","    Uninstalling openai-1.54.5:\n","      Successfully uninstalled openai-1.54.5\n","Successfully installed langchain-openai-0.2.12 openai-1.57.3 tiktoken-0.8.0\n","Collecting langchain-community==0.3.11\n","  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (3.11.10)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.11)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.11)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n","  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community==0.3.11)\n","  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (1.26.4)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.11)\n","  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.18.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.10.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.11) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.27.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.2.2)\n","Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n","Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.22\n","    Uninstalling langchain-core-0.3.22:\n","      Successfully uninstalled langchain-core-0.3.22\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.10\n","    Uninstalling langchain-0.3.10:\n","      Successfully uninstalled langchain-0.3.10\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n","Collecting dill\n","  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n","Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dill\n","Successfully installed dill-0.3.9\n"]}]},{"cell_type":"markdown","source":["## Install Chroma Vector DB and LangChain wrapper"],"metadata":{"id":"bwUBYHjPfu_-"}},{"cell_type":"code","source":["!pip install langchain-chroma==0.1.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"25bb3e59-cc15-406d-bdc3-57f61b581c75","id":"p30SmCgTfu__","executionInfo":{"status":"ok","timestamp":1734095239256,"user_tz":-330,"elapsed":40219,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-chroma==0.1.4\n","  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n","Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma==0.1.4)\n","  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n","Collecting fastapi<1,>=0.95.2 (from langchain-chroma==0.1.4)\n","  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma==0.1.4) (0.3.24)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma==0.1.4) (1.26.4)\n","Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.10.3)\n","Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n","Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n","Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.12.2)\n","Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.28.2)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.28.2)\n","Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.20.3)\n","Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.66.6)\n","Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.4.5)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.68.1)\n","Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.15.1)\n","Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (9.0.0)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.0.2)\n","Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10.12)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (13.9.4)\n","Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma==0.1.4)\n","  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.33)\n","Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (0.1.147)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (24.2)\n","Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (3.0.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.32.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.3)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.0.0)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.25.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.15)\n","Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.5.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.66.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n","Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n","Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.26.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.1)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.10.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.4.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.6.1)\n","Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n","Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n","Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n","Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n","Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n","Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n","Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n","Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n","Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=f8557553cdcd1c43c4d9e1384b4651f0fc6a529013380c0c3f612749cd25dba3\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built pypika\n","Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.25.5\n","    Uninstalling protobuf-4.25.5:\n","      Successfully uninstalled protobuf-4.25.5\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.28.2\n","    Uninstalling opentelemetry-api-1.28.2:\n","      Successfully uninstalled opentelemetry-api-1.28.2\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n","    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.28.2\n","    Uninstalling opentelemetry-sdk-1.28.2:\n","      Successfully uninstalled opentelemetry-sdk-1.28.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 langchain-chroma-0.1.4 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.41.3 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.3 websockets-14.1\n"]}]},{"cell_type":"markdown","source":["## Install RAG Evaluation Libraries"],"metadata":{"id":"lVJuo_Tyu4xv"}},{"cell_type":"code","source":["!pip install ragas==0.2.8\n","!pip install deepeval==1.4.7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CkBlAPKHu4Ih","outputId":"e3dcabc8-7974-4c38-b7d4-4961d4fba51e","executionInfo":{"status":"ok","timestamp":1734095271535,"user_tz":-330,"elapsed":32283,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ragas==0.2.8\n","  Downloading ragas-0.2.8-py3-none-any.whl.metadata (9.1 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (1.26.4)\n","Collecting datasets (from ragas==0.2.8)\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (0.8.0)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (0.3.11)\n","Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (0.3.24)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (0.3.11)\n","Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (0.2.12)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (1.6.0)\n","Collecting appdirs (from ragas==0.2.8)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (2.10.3)\n","Requirement already satisfied: openai>1 in /usr/local/lib/python3.10/dist-packages (from ragas==0.2.8) (1.57.3)\n","Collecting pysbd>=0.3.4 (from ragas==0.2.8)\n","  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas==0.2.8) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas==0.2.8) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas==0.2.8) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas==0.2.8) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas==0.2.8) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas==0.2.8) (4.66.6)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas==0.2.8) (4.12.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ragas==0.2.8) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ragas==0.2.8) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.2.8) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.2.8) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas==0.2.8)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.2.8) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.2.8) (2.32.3)\n","Collecting xxhash (from datasets->ragas==0.2.8)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets->ragas==0.2.8)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas==0.2.8)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.2.8) (3.11.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.2.8) (0.26.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.2.8) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas==0.2.8) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas==0.2.8) (2.0.36)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas==0.2.8) (4.0.3)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas==0.2.8) (0.3.2)\n","Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas==0.2.8) (0.1.147)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas==0.2.8) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas==0.2.8) (1.33)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas==0.2.8) (0.6.7)\n","Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas==0.2.8) (0.4.0)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas==0.2.8) (2.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas==0.2.8) (2024.9.11)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas==0.2.8) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas==0.2.8) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas==0.2.8) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas==0.2.8) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas==0.2.8) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas==0.2.8) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas==0.2.8) (1.18.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas==0.2.8) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas==0.2.8) (1.2.2)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.2.8) (3.23.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.2.8) (0.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.2.8) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.2.8) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas==0.2.8) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas==0.2.8) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->ragas==0.2.8) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->ragas==0.2.8) (1.0.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas==0.2.8) (1.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas==0.2.8) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas==0.2.8) (2.2.3)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas==0.2.8) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas==0.2.8) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas==0.2.8) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas==0.2.8) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas==0.2.8) (1.17.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.2.8) (1.0.0)\n","Downloading ragas-0.2.8-py3-none-any.whl (173 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.9/173.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: appdirs, xxhash, pysbd, fsspec, dill, multiprocess, datasets, ragas\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.9\n","    Uninstalling dill-0.3.9:\n","      Successfully uninstalled dill-0.3.9\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed appdirs-1.4.4 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 pysbd-0.3.4 ragas-0.2.8 xxhash-3.5.0\n","Collecting deepeval==1.4.7\n","  Downloading deepeval-1.4.7-py3-none-any.whl.metadata (977 bytes)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (4.66.6)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (8.3.4)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (0.9.0)\n","Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (0.15.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (13.9.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (5.29.1)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (2.10.3)\n","Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (2.19.2)\n","Collecting pytest-repeat (from deepeval==1.4.7)\n","  Downloading pytest_repeat-0.9.3-py3-none-any.whl.metadata (4.9 kB)\n","Collecting pytest-xdist (from deepeval==1.4.7)\n","  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\n","Collecting portalocker (from deepeval==1.4.7)\n","  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (0.3.11)\n","Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (0.3.24)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (0.2.12)\n","Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (0.2.8)\n","Collecting docx2txt~=0.8 (from deepeval==1.4.7)\n","  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: importlib-metadata>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from deepeval==1.4.7) (8.5.0)\n","Collecting tenacity~=8.4.1 (from deepeval==1.4.7)\n","  Downloading tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n","Collecting opentelemetry-api~=1.24.0 (from deepeval==1.4.7)\n","  Downloading opentelemetry_api-1.24.0-py3-none-any.whl.metadata (1.3 kB)\n","Collecting opentelemetry-sdk~=1.24.0 (from deepeval==1.4.7)\n","  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl.metadata (1.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc~=1.24.0 (from deepeval==1.4.7)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n","Collecting grpcio~=1.63.0 (from deepeval==1.4.7)\n","  Downloading grpcio-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.0.2->deepeval==1.4.7) (3.21.0)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.24.0->deepeval==1.4.7) (1.2.15)\n","Collecting importlib-metadata>=6.0.2 (from deepeval==1.4.7)\n","  Downloading importlib_metadata-7.0.0-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc~=1.24.0->deepeval==1.4.7) (1.66.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc~=1.24.0->deepeval==1.4.7)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl.metadata (1.7 kB)\n","Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc~=1.24.0->deepeval==1.4.7)\n","  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n","Collecting protobuf (from deepeval==1.4.7)\n","  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-sdk~=1.24.0->deepeval==1.4.7)\n","  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk~=1.24.0->deepeval==1.4.7) (4.12.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval==1.4.7) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval==1.4.7) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval==1.4.7) (3.11.10)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval==1.4.7) (4.0.3)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval==1.4.7) (0.3.2)\n","Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval==1.4.7) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain->deepeval==1.4.7) (1.26.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->deepeval==1.4.7) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core->deepeval==1.4.7) (24.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepeval==1.4.7) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepeval==1.4.7) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->deepeval==1.4.7) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->deepeval==1.4.7) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->deepeval==1.4.7) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->deepeval==1.4.7) (2024.8.30)\n","Requirement already satisfied: openai<2.0.0,>=1.55.3 in /usr/local/lib/python3.10/dist-packages (from langchain-openai->deepeval==1.4.7) (1.57.3)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai->deepeval==1.4.7) (0.8.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval==1.4.7) (1.2.2)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval==1.4.7) (2.0.0)\n","Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval==1.4.7) (1.5.0)\n","Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval==1.4.7) (2.2.1)\n","Collecting execnet>=2.1 (from pytest-xdist->deepeval==1.4.7)\n","  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval==1.4.7) (3.2.0)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval==1.4.7) (0.3.11)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval==1.4.7) (1.6.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval==1.4.7) (1.4.4)\n","Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval==1.4.7) (0.3.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->deepeval==1.4.7) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->deepeval==1.4.7) (2.18.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer->deepeval==1.4.7) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->deepeval==1.4.7) (1.5.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval==1.4.7) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval==1.4.7) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval==1.4.7) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval==1.4.7) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval==1.4.7) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval==1.4.7) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->deepeval==1.4.7) (1.18.3)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api~=1.24.0->deepeval==1.4.7) (1.17.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->deepeval==1.4.7) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->deepeval==1.4.7) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->deepeval==1.4.7) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->deepeval==1.4.7) (1.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->deepeval==1.4.7) (0.1.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai->deepeval==1.4.7) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai->deepeval==1.4.7) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai->deepeval==1.4.7) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai->deepeval==1.4.7) (1.3.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->deepeval==1.4.7) (3.1.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai->deepeval==1.4.7) (2024.9.11)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval==1.4.7) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval==1.4.7) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval==1.4.7) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval==1.4.7) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval==1.4.7) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval==1.4.7) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas->deepeval==1.4.7) (2024.9.0)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval==1.4.7) (0.26.5)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->deepeval==1.4.7) (0.6.7)\n","Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->deepeval==1.4.7) (0.4.0)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->deepeval==1.4.7) (2.7.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval==1.4.7) (3.23.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval==1.4.7) (0.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain->deepeval==1.4.7) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain->deepeval==1.4.7) (0.14.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas->deepeval==1.4.7) (1.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval==1.4.7) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval==1.4.7) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval==1.4.7) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas->deepeval==1.4.7) (1.17.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval==1.4.7) (1.0.0)\n","Downloading deepeval-1.4.7-py3-none-any.whl (461 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.1/461.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading grpcio-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n","Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n","Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n","Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tenacity-8.4.2-py3-none-any.whl (28 kB)\n","Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n","Downloading pytest_repeat-0.9.3-py3-none-any.whl (4.2 kB)\n","Downloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading execnet-2.1.1-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docx2txt\n","  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=d804858119aa53bffbbb99857aa4f41a3e83764ce7b3e63da1ee722400c69d5f\n","  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n","Successfully built docx2txt\n","Installing collected packages: docx2txt, tenacity, protobuf, portalocker, opentelemetry-semantic-conventions, importlib-metadata, grpcio, execnet, pytest-xdist, pytest-repeat, opentelemetry-proto, opentelemetry-api, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-common, opentelemetry-exporter-otlp-proto-grpc, deepeval\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.1\n","    Uninstalling protobuf-5.29.1:\n","      Successfully uninstalled protobuf-5.29.1\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.50b0\n","    Uninstalling opentelemetry-semantic-conventions-0.50b0:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.50b0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib_metadata 8.5.0\n","    Uninstalling importlib_metadata-8.5.0:\n","      Successfully uninstalled importlib_metadata-8.5.0\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.68.1\n","    Uninstalling grpcio-1.68.1:\n","      Successfully uninstalled grpcio-1.68.1\n","  Attempting uninstall: opentelemetry-proto\n","    Found existing installation: opentelemetry-proto 1.29.0\n","    Uninstalling opentelemetry-proto-1.29.0:\n","      Successfully uninstalled opentelemetry-proto-1.29.0\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.29.0\n","    Uninstalling opentelemetry-api-1.29.0:\n","      Successfully uninstalled opentelemetry-api-1.29.0\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.29.0\n","    Uninstalling opentelemetry-sdk-1.29.0:\n","      Successfully uninstalled opentelemetry-sdk-1.29.0\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n","    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.29.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-common-1.29.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.29.0\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-grpc\n","    Found existing installation: opentelemetry-exporter-otlp-proto-grpc 1.29.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-grpc-1.29.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-grpc-1.29.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-cloud-pubsub 2.27.1 requires opentelemetry-api>=1.27.0; python_version >= \"3.8\", but you have opentelemetry-api 1.24.0 which is incompatible.\n","google-cloud-pubsub 2.27.1 requires opentelemetry-sdk>=1.27.0; python_version >= \"3.8\", but you have opentelemetry-sdk 1.24.0 which is incompatible.\n","opentelemetry-instrumentation 0.50b0 requires opentelemetry-semantic-conventions==0.50b0, but you have opentelemetry-semantic-conventions 0.45b0 which is incompatible.\n","opentelemetry-instrumentation-asgi 0.50b0 requires opentelemetry-semantic-conventions==0.50b0, but you have opentelemetry-semantic-conventions 0.45b0 which is incompatible.\n","opentelemetry-instrumentation-fastapi 0.50b0 requires opentelemetry-semantic-conventions==0.50b0, but you have opentelemetry-semantic-conventions 0.45b0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed deepeval-1.4.7 docx2txt-0.8 execnet-2.1.1 grpcio-1.63.2 importlib-metadata-7.0.0 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 portalocker-3.0.0 protobuf-4.25.5 pytest-repeat-0.9.3 pytest-xdist-3.6.1 tenacity-8.4.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["importlib_metadata"]},"id":"48d03d6378b64bdead2117a2366940f1"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Enter Open AI API Key"],"metadata":{"id":"EITC17hwfu__"}},{"cell_type":"code","source":["from getpass import getpass\n","\n","OPENAI_KEY = getpass('Enter Open AI API Key: ')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e683fcf2-04c5-47d3-b862-14f68dd2e843","id":"yEh2olNvfvAA","executionInfo":{"status":"ok","timestamp":1734095563906,"user_tz":-330,"elapsed":3135,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Open AI API Key: ··········\n"]}]},{"cell_type":"markdown","source":["## Setup Environment Variables"],"metadata":{"id":"pm_mx0v-fvAA"}},{"cell_type":"code","source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = OPENAI_KEY"],"metadata":{"id":"Jhfb4gMUfvAC","executionInfo":{"status":"ok","timestamp":1734095566435,"user_tz":-330,"elapsed":347,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Open AI Embedding Models\n","\n","LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."],"metadata":{"id":"jiokYxD8fvAC"}},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","\n","# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n","openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"],"metadata":{"id":"-On4AS0HfvAD","executionInfo":{"status":"ok","timestamp":1734095571809,"user_tz":-330,"elapsed":3076,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Loading and Processing the Data"],"metadata":{"id":"afzeN_WkHIz2"}},{"cell_type":"markdown","source":["### Get the dataset"],"metadata":{"id":"RA_-hzHbFeSP"}},{"cell_type":"code","source":["# if you can't download using the following code\n","# go to https://drive.google.com/file/d/1QkSY9W5RyaBnY8c5FLIsmpPVXoHTQ-fb/view?usp=sharing download it\n","# manually upload it on colab\n","!gdown 1QkSY9W5RyaBnY8c5FLIsmpPVXoHTQ-fb"],"metadata":{"id":"RZFMYH-yFhWn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3e648ba-3f02-428e-c724-6c3fae437f62","executionInfo":{"status":"ok","timestamp":1734095579822,"user_tz":-330,"elapsed":4857,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1QkSY9W5RyaBnY8c5FLIsmpPVXoHTQ-fb\n","To: /content/rag_eval_docs.csv\n","\r  0% 0.00/2.66k [00:00<?, ?B/s]\r100% 2.66k/2.66k [00:00<00:00, 8.13MB/s]\n"]}]},{"cell_type":"markdown","source":["### Load and Process JSON Documents"],"metadata":{"id":"wMlxKZ_5jIdE"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('./rag_eval_docs.csv')\n","df"],"metadata":{"id":"RZ5y0NfzHPhg","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"824fd250-0b64-4bb5-a701-278c30e1162e","executionInfo":{"status":"ok","timestamp":1734095580434,"user_tz":-330,"elapsed":613,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id                              title  \\\n","0   1                   Machine Learning   \n","1   2                      Deep Learning   \n","2   3  Natural Language Processing (NLP)   \n","3   4                           Pyramids   \n","4   5                     Photosynthesis   \n","5   6                            Biology   \n","6   7                  Quantum Mechanics   \n","7   8                     Cryptocurrency   \n","8   9                   Renewable Energy   \n","9  10            Artificial Intelligence   \n","\n","                                             context  \n","0  Machine learning is a field of artificial inte...  \n","1  Deep learning is a subset of machine learning ...  \n","2  NLP is a branch of AI that enables computers t...  \n","3  Pyramids are ancient structures, often serving...  \n","4  Photosynthesis is the process plants use to co...  \n","5  Biology is the study of living organisms, cove...  \n","6  Quantum mechanics is a branch of physics that ...  \n","7  Cryptocurrency is a digital currency that uses...  \n","8  Renewable energy sources, such as solar and wi...  \n","9  Artificial intelligence refers to machines mim...  "],"text/html":["\n","  <div id=\"df-eb589a66-e3b6-4ae6-9ea0-b2b2a01ed98a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Machine Learning</td>\n","      <td>Machine learning is a field of artificial inte...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Deep Learning</td>\n","      <td>Deep learning is a subset of machine learning ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Natural Language Processing (NLP)</td>\n","      <td>NLP is a branch of AI that enables computers t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Pyramids</td>\n","      <td>Pyramids are ancient structures, often serving...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Photosynthesis</td>\n","      <td>Photosynthesis is the process plants use to co...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>Biology</td>\n","      <td>Biology is the study of living organisms, cove...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>Quantum Mechanics</td>\n","      <td>Quantum mechanics is a branch of physics that ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>Cryptocurrency</td>\n","      <td>Cryptocurrency is a digital currency that uses...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>Renewable Energy</td>\n","      <td>Renewable energy sources, such as solar and wi...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>Artificial Intelligence</td>\n","      <td>Artificial intelligence refers to machines mim...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb589a66-e3b6-4ae6-9ea0-b2b2a01ed98a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-eb589a66-e3b6-4ae6-9ea0-b2b2a01ed98a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-eb589a66-e3b6-4ae6-9ea0-b2b2a01ed98a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c504bebf-7a86-4880-9df0-9f8e7e670bde\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c504bebf-7a86-4880-9df0-9f8e7e670bde')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c504bebf-7a86-4880-9df0-9f8e7e670bde button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_8b2be79a-e05a-4018-9495-8b1d5053c3a8\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8b2be79a-e05a-4018-9495-8b1d5053c3a8 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Renewable Energy\",\n          \"Deep Learning\",\n          \"Biology\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Renewable energy sources, such as solar and wind, provide sustainable alternatives to fossil fuels. These resources are replenished naturally and reduce greenhouse gas emissions. Governments are investing in renewables to combat climate change.\",\n          \"Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.\",\n          \"Biology is the study of living organisms, covering areas such as genetics, ecology, and physiology. It aims to understand how organisms function and interact with each other and their environment. Modern biology has advanced through the study of cells and DNA.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["docs = df.to_dict(orient='records')\n","docs[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wE4l0DG7tpTI","outputId":"9daba315-9520-4339-847f-33176eff6e88","executionInfo":{"status":"ok","timestamp":1734095580435,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'id': 1,\n","  'title': 'Machine Learning',\n","  'context': 'Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.'},\n"," {'id': 2,\n","  'title': 'Deep Learning',\n","  'context': 'Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.'},\n"," {'id': 3,\n","  'title': 'Natural Language Processing (NLP)',\n","  'context': 'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.'}]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from langchain.docstore.document import Document\n","processed_docs = []\n","\n","for doc in docs:\n","    metadata = {\n","        \"title\": doc['title'],\n","        \"id\": doc['id'],\n","    }\n","    data = doc['context']\n","    processed_docs.append(Document(page_content=data, metadata=metadata))\n","processed_docs[:3]"],"metadata":{"id":"yICyAF85h2DO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b4237c5-1d47-40a9-bfe6-a000a08b1f09","executionInfo":{"status":"ok","timestamp":1734095584182,"user_tz":-330,"elapsed":358,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'title': 'Machine Learning', 'id': 1}, page_content='Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.'),\n"," Document(metadata={'title': 'Deep Learning', 'id': 2}, page_content='Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.'),\n"," Document(metadata={'title': 'Natural Language Processing (NLP)', 'id': 3}, page_content='NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.')]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## Index Document Chunks and Embeddings in Vector DB\n","\n","Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."],"metadata":{"id":"Daqn6Hglw9Nk"}},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","# create vector DB of docs and embeddings - takes < 30s on Colab\n","chroma_db = Chroma.from_documents(documents=processed_docs,\n","                                  collection_name='my_db',\n","                                  embedding=openai_embed_model,\n","                                  # need to set the distance function to cosine else it uses euclidean by default\n","                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n","                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n","                                  persist_directory=\"./my_db\")"],"metadata":{"id":"EYjyZdCyw9Nl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-0qbbpjw9Nl"},"source":["### Load Vector DB from disk\n","\n","This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PI3ITuGZw9Nl"},"outputs":[],"source":["# load from disk\n","chroma_db = Chroma(persist_directory=\"./my_db\",\n","                   collection_name='my_db',\n","                   embedding_function=openai_embed_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Udsb8xyVw9Nl","outputId":"f5f8a656-343d-43b9-9728-78e71a637caa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain_chroma.vectorstores.Chroma at 0x7a5b1dfca560>"]},"metadata":{},"execution_count":10}],"source":["chroma_db"]},{"cell_type":"markdown","source":["### Semantic Similarity based Retrieval\n","\n","We use simple cosine similarity here and retrieve the top 3 similar documents based on the user input query"],"metadata":{"id":"njfZOOVZxj1a"}},{"cell_type":"code","source":["similarity_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n","                                              search_kwargs={\"k\": 3, \"score_threshold\": 0.3})"],"metadata":{"id":"tV1l6HYdxj1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","def display_docs(docs):\n","    for doc in docs:\n","        print('Metadata:', doc.metadata)\n","        print('Content Brief:')\n","        display(Markdown(doc.page_content))\n","        print()"],"metadata":{"id":"nUIJG_bDxj1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"what is AI?\"\n","top_docs = similarity_retriever.invoke(query)\n","display_docs(top_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"PIh4xGv2xj1c","outputId":"a88ace2e-782f-40f8-9bd6-994334b0aba5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata: {'id': 10, 'title': 'Artificial Intelligence'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': 3, 'title': 'Natural Language Processing (NLP)'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Metadata: {'id': 1, 'title': 'Machine Learning'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["query = \"how do plants survive?\"\n","top_docs = similarity_retriever.invoke(query)\n","display_docs(top_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117},"id":"S_PXFMcJxuyO","outputId":"f9548a6a-9203-4440-fb39-eb07ef0ba8b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Metadata: {'id': 5, 'title': 'Photosynthesis'}\n","Content Brief:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Photosynthesis is the process plants use to convert sunlight into energy. This process produces glucose and releases oxygen as a byproduct. It is crucial for sustaining life on Earth by providing food and oxygen."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"markdown","source":["## Build the RAG Pipeline"],"metadata":{"id":"gQFWv7YUyVII"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n","                Answer the following question using only the following pieces of retrieved context.\n","                If the answer is not in the context, do not make up answers, just say that you don't know.\n","                Keep the answer to the point based on the information from the context.\n","\n","                Question:\n","                {question}\n","\n","                Context:\n","                {context}\n","\n","                Answer:\n","            \"\"\"\n","\n","rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"],"metadata":{"id":"PHOrfGXKyVIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.runnables import RunnableLambda\n","from operator import itemgetter\n","\n","\n","chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","src_rag_response_chain = (\n","    {\n","        \"context\": (itemgetter('context')\n","                        |\n","                    RunnableLambda(format_docs)),\n","        \"question\": itemgetter(\"question\")\n","    }\n","        |\n","    rag_prompt_template\n","        |\n","    chatgpt\n","        |\n","    StrOutputParser()\n",")\n","\n","rag_chain_w_sources = (\n","    {\n","        \"context\": similarity_retriever,\n","        \"question\": RunnablePassthrough()\n","    }\n","        |\n","    RunnablePassthrough.assign(response=src_rag_response_chain)\n",")"],"metadata":{"id":"KmWeCB4yyVIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"What is AI?\"\n","result = rag_chain_w_sources.invoke(query)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"419e7f20-aa91-4ea4-c693-b671fa5405fa","id":"xvj_eGIWyVIJ"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': [Document(metadata={'id': 10, 'title': 'Artificial Intelligence'}, page_content=\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"),\n","  Document(metadata={'id': 3, 'title': 'Natural Language Processing (NLP)'}, page_content='NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.'),\n","  Document(metadata={'id': 1, 'title': 'Machine Learning'}, page_content='Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.')],\n"," 'question': 'What is AI?',\n"," 'response': 'AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles.'}"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["query = \"How do plants survive?\"\n","result = rag_chain_w_sources.invoke(query)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXtezDlZzadt","outputId":"6b78d343-83a3-4134-b878-8bbe318be2d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': [Document(metadata={'id': 5, 'title': 'Photosynthesis'}, page_content='Photosynthesis is the process plants use to convert sunlight into energy. This process produces glucose and releases oxygen as a byproduct. It is crucial for sustaining life on Earth by providing food and oxygen.')],\n"," 'question': 'How do plants survive?',\n"," 'response': 'Plants survive by using photosynthesis to convert sunlight into energy, producing glucose and releasing oxygen as a byproduct.'}"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# Create End-to-End RAG Evaluation Workflow\n","\n","![](https://i.imgur.com/GUIkpjy.png)"],"metadata":{"id":"NXxRQxSQiM_E"}},{"cell_type":"markdown","source":["## Create a Synthetic RAG Golden Reference Dataset"],"metadata":{"id":"Q-Gdpk1Fu1aC"}},{"cell_type":"code","source":["doc_contexts = [doc.page_content for doc in processed_docs]\n","doc_contexts[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XrLB2r0CpNnm","outputId":"e40c3d2b-89f9-4182-9872-b27ede79ca12","executionInfo":{"status":"ok","timestamp":1734095590045,"user_tz":-330,"elapsed":409,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.',\n"," 'Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.',\n"," 'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["from deepeval.synthesizer import Synthesizer\n","from deepeval.synthesizer import types"],"metadata":{"id":"Y2NT6Uw2y_Sx","executionInfo":{"status":"ok","timestamp":1734095596780,"user_tz":-330,"elapsed":2545,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1561f551-3125-431b-b4c1-a61d7e22f22a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/deepeval/__init__.py:49: UserWarning: You are using deepeval version 1.4.7, however version 2.0.5 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["synthesizer = Synthesizer(model='gpt-4o',\n","                          embedder=OpenAIEmbeddings())\n","\n","eval_data = synthesizer.generate_goldens(\n","    # Provide a list of context for synthetic data generation\n","    contexts=[[doc] for doc in doc_contexts],\n","    include_expected_output=True,\n","    max_goldens_per_context=1,\n","    num_evolutions=1,\n","    scenario=\"Retrieval Augmented Generation\",\n","    task=\"Question Answering\",\n","    evolutions={\n","        types.Evolution.REASONING: 0.1,     # Evolves the input to require multi-step logical thinking.\n","        types.Evolution.MULTICONTEXT: 0.9,  # Ensures that all relevant information from the context is utilized.\n","        types.Evolution.CONCRETIZING: 0.0,  # Makes abstract ideas more concrete and detailed.\n","        types.Evolution.CONSTRAINED: 0.0,   # Introduces a condition or restriction, testing the model's ability to operate within specific limits.\n","        types.Evolution.COMPARATIVE: 0.0,   # Requires a response that involves a comparison between options or contexts.\n","        types.Evolution.HYPOTHETICAL: 0.0,  # Forces the model to consider and respond to a hypothetical scenario.\n","        types.Evolution.IN_BREADTH: 0.0,    # Broadens the input to touch on related or adjacent topics.\n","    }\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"t0w1TixH4zAH","outputId":"781c5c9b-1eae-4f51-8332-9eaf7a7a7c93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"]},{"output_type":"stream","name":"stderr","text":["✨ Generating up to 10 goldens using DeepEval (using gpt-4o, use case=QA, method=default): 100%|██████████| 10/10 [00:20<00:00,  2.00s/it]\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[38;2;5;245;141m✓\u001b[0m Generation finished 🎉! You can also run \u001b[32m'deepeval login'\u001b[0m to generate and save goldens directly on Confident AI.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Generation finished 🎉! You can also run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to generate and save goldens directly on Confident AI.\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["eval_data[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bqh6c5CT69XM","outputId":"bab3e553-189c-4001-cfab-86ff1db6bc32"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Golden(input='How can machine learning algorithms use data patterns effectively for predictions and applications such as recommendations?', actual_output=None, expected_output='Machine learning algorithms use data patterns effectively by analyzing historical data to identify trends and relationships. This enables them to make accurate predictions and classifications. For applications like recommendation systems, these algorithms assess user behavior to suggest relevant items, while in image recognition, they identify distinct features to classify images accurately.', context=['Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.'], retrieval_context=None, additional_metadata={'evolutions': ['Multi-context'], 'synthetic_input_quality': 0.8, 'context_quality': None}, comments=None, tools_called=None, expected_tools=None, source_file=None)"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["## Save the Synthetic RAG Golden Reference Dataset"],"metadata":{"id":"-BOZXwFDiXKH"}},{"cell_type":"code","source":["import dill"],"metadata":{"id":"rVns4_28KO-z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('golden_ref_data.bin', 'wb') as f:\n","    dill.dump(eval_data, f)"],"metadata":{"id":"YvDzxHxYKCjm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create RAG Evaluation Dataset"],"metadata":{"id":"fl-g5-Q8ibIj"}},{"cell_type":"code","source":["from deepeval.dataset import EvaluationDataset\n","\n","eval_dataset = EvaluationDataset()\n","\n","# load golden dataset\n","with open('golden_ref_data.bin', 'rb') as f:\n","    golden_docs = dill.load(f)\n","\n","eval_dataset.goldens = golden_docs"],"metadata":{"id":"y_oRbB8m1MI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_dataset.goldens[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RUCEaIMWvnf","outputId":"de332695-d2ab-41bc-c4e4-de8c55e5c294"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Golden(input='How can machine learning algorithms use data patterns effectively for predictions and applications such as recommendations?', actual_output=None, expected_output='Machine learning algorithms use data patterns effectively by analyzing historical data to identify trends and relationships. This enables them to make accurate predictions and classifications. For applications like recommendation systems, these algorithms assess user behavior to suggest relevant items, while in image recognition, they identify distinct features to classify images accurately.', context=['Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.'], retrieval_context=None, additional_metadata={'evolutions': ['Multi-context'], 'synthetic_input_quality': 0.8, 'context_quality': None}, comments=None, tools_called=None, expected_tools=None, source_file=None)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["eval_dataset.goldens[0].input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"QEP5l8DMWzwb","outputId":"d6b63cdb-7480-4ab4-cb14-4c2e7b057992"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'How can machine learning algorithms use data patterns effectively for predictions and applications such as recommendations?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["rag_chain_w_sources.invoke(eval_dataset.goldens[0].input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WnHzga5EWsL_","outputId":"206eadd4-5d76-4f61-9958-6fe7351cca29"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': [Document(metadata={'id': 1, 'title': 'Machine Learning'}, page_content='Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.'),\n","  Document(metadata={'id': 2, 'title': 'Deep Learning'}, page_content='Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.'),\n","  Document(metadata={'id': 10, 'title': 'Artificial Intelligence'}, page_content=\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\")],\n"," 'question': 'How can machine learning algorithms use data patterns effectively for predictions and applications such as recommendations?',\n"," 'response': 'Machine learning algorithms use data patterns effectively for predictions and applications such as recommendations by analyzing past data to make predictions or classify information.'}"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["from typing import List\n","from deepeval.test_case import LLMTestCase\n","from deepeval.dataset import Golden\n","from tqdm import tqdm\n","\n","def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase]:\n","    test_cases = []\n","    for golden in tqdm(goldens):\n","        response_obj = rag_chain_w_sources.invoke(golden.input)\n","        test_case = LLMTestCase(\n","            input=golden.input,\n","            actual_output=response_obj['response'],\n","            expected_output=golden.expected_output,\n","            context=golden.context,\n","            retrieval_context=[doc.page_content for doc in response_obj['context']]\n","        )\n","        test_cases.append(test_case)\n","    return test_cases"],"metadata":{"id":"gmQkLxAt3_pe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_dataset.test_cases = convert_goldens_to_test_cases(eval_dataset.goldens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUT7IrIbZgtp","outputId":"746b4d65-6a9e-4e5e-c8da-036c5d7de5a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:13<00:00,  1.31s/it]\n"]}]},{"cell_type":"code","source":["eval_dataset.test_cases[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AGc_Rnpcjj0","outputId":"e02abf0f-cc49-4d03-de71-ea4a5d4e3d66"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LLMTestCase(input='How can machine learning algorithms use data patterns effectively for predictions and applications such as recommendations?', actual_output='Machine learning algorithms use data patterns effectively for predictions and applications such as recommendations by analyzing past data to make predictions or classify information.', expected_output='Machine learning algorithms use data patterns effectively by analyzing historical data to identify trends and relationships. This enables them to make accurate predictions and classifications. For applications like recommendation systems, these algorithms assess user behavior to suggest relevant items, while in image recognition, they identify distinct features to classify images accurately.', context=['Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.'], retrieval_context=['Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.', 'Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.', \"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None)"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["## Run and View RAG Evaluations on the Evaluation Dataset"],"metadata":{"id":"_w8B75MKihxs"}},{"cell_type":"code","source":["from deepeval import evaluate\n","from deepeval.metrics import ContextualPrecisionMetric, ContextualRecallMetric, ContextualRelevancyMetric\n","from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric\n","from deepeval.metrics.ragas import RAGASAnswerRelevancyMetric\n","\n","contextual_precision = ContextualPrecisionMetric(threshold=0.5, include_reason=True, model=\"gpt-4o\")\n","contextual_recall = ContextualRecallMetric(threshold=0.5, include_reason=True, model=\"gpt-4o\")\n","contextual_relevancy = ContextualRelevancyMetric(threshold=0.5, include_reason=True, model=\"gpt-4o\")\n","answer_relevancy = AnswerRelevancyMetric(threshold=0.5, include_reason=True, model=\"gpt-4o\")\n","faithfulness = FaithfulnessMetric(threshold=0.5, include_reason=True, model=\"gpt-4o\")\n","hallucination = HallucinationMetric(threshold=0.5, include_reason=True, model=\"gpt-4o\")\n","ragas_answer_relevancy = RAGASAnswerRelevancyMetric(threshold=0.5, embeddings=OpenAIEmbeddings(), model=\"gpt-4o\")\n","\n","eval_results = evaluate(test_cases=eval_dataset.test_cases,\n","                        metrics=[contextual_precision, contextual_recall, contextual_relevancy,\n","                                 answer_relevancy, ragas_answer_relevancy, faithfulness, hallucination])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["78edb9b2b80c483ab1d65a5e6ac7b00c","0566c23e5037436e97b5178282b12628","a6768074f3ea4cb4b6bf34fd3ec588d2","4d02d332124e489194212dd05a80af68","a73d64e7f0d64d3ab8e7c626193f44f9","4ab47828042947ceb874f360a728d2e9","88ec20625df54005a0e6623af57f0b9d","a336914f145e42c0a1df04bfb0aa0f61","0d8b6013a8734c3cb2848eee9e7597ff","4d7f50e073df423b9e10770933466576","b64d491c187c4506942bd5ecb013d313","9be2346fdf034f3fae44bae5649ea544","945dfa6461a14d0a99a207a600be9f99","58a73767e7e34033b80fc8ff6dccd644","3483a09e68ad423d8285c88b0023c0da","9bf97bf583fd4b6f9b8dbe7667d56863","f7d70159f003466996088b318a7ed06d","f01d72a1d5514dc1a6c35f95cdb04c78","4113a8c3d98d4ac6a55cb7d887fa530c","89bfda1f10344e03a5f132feea2c9bfb","8b64ddd749fe4d14bc15b0bf89c698ed","af2ae9a883e64c6bbed599f54b6c6915","153226563d454cbd92ed327d61cbcf68","59275c721a1849e58c3efc0c339f8954","5ca4e01efe6d44eea9d60e5b9c9080c2","3aee75144e004e8dbcfe75abc6762208","ba7fd441525a45efb8d4bce5a7362a03","3c147e908b1b49fbb52d6820495afab7","9fddccfcf6a843e69bf3b1e631aca9e1","eeae6addc36447bd9c82d2b4b4fa99bb","69f10456c157455d8a7ec9fd9cdf28f6","2b53bc9dbfe84a83beec08350f953df4","19d664186c594a5a81be22093ea94717","613a3541acae40d38eb8898a78acb445","0b3242c9b3ef4964aa9578af5775a396","5ca5882b64ac4dafb27da9189802d36c","e53c7c33e8d244c3ab1f3a29318e0bcf","072bdd9321e644cc8f25c2749339e083","2c9ed88787be4036a623d4a7088c40b8","2b4046d510a34e38bdd095ca905abc9b","88bcb61cb34e4cdaae01127aaa76fb07","fae02453a4164ba39f26445cc1a01a72","7f3fa03415044d79b0db289024d882de","4fcb9e84ee2e4e909f34f4a8fa403ca2","ff0b25a6b0364081907ec87b853ea6a4","024d803bdab24516b6770b101969b8e1","1e14a7aa053449f29d94620f654b8fdd","67554dce2ed147daa1e1f6887b706256","9eeb2fb2fa444982acf1e9b95e95703e","c9dd4eebc151451085de50702f21a311","6caddec7dc934425bf56010398936537","90441504bc7b44898eabe5b78fb6a5ec","e7e2ef49e4b44aec89fa39ed9799cf59","9fee5fde75c34cd6999b57d4374dd9c7","28fd1350a29149148a0083ff151ee2b1","af5f81ba63744f48969b6230f0e1196d","d76b5cc2bf724998afecba83dd9917c2","bf94afe4446b49b78c325ed123805c29","92df8d04435244659d3a4b4773d619e2","db9c163fe2674abf94bf676d6ca94f0e","968da5427c57445fa41933ad7aec061a","183cf676e13d470c9c0db082b89256ff","db86b4cba7b4402fa8563922ca6e9908","14f35227c96342dba5aa8b0db9cdabf0","4d46735abc034aa3817f3ef7bd70a0e6","fae240f399c44cbd9bb14e2d0ce24013","915a91064f44458eac9dee659df36362","7a4b4375488a447b9f483798173b5560","654a300e0e624582aad9ce36720cc8d3","d3d5b7ec3dd64df69f6b17dd3fb1188e","c9e569343f6a4cb1b100356cfd56c617","48b6fa2757de4c9aa48c429234552b7a","89c18cb46f564cb8b4d390cdb75e27a1","ba82e24b119d4eb086248ba70f6adcba","20174965e93c4d9e96a8b2567cda17cf","0c7afd75619d43c4b2debae90e9adc2d","bf93a40cc97f439b82bc2e51478e56d1","55c66431dcad49088e1c81e2f1a59d68","6b789175b58a413d8b4eeb4200854d7f","66872cc0ee164caca99347f90386044b","6d9cd0ea46f34a9faed020f53ada73ab","6a52685481b54fcc9521f61e5d50119f","60422854823d4a1cb19377f25a12ec51","a3dca1dc3d3045e79587e5593e0ea74f","6fb30db42d5b49418fdf8de0fad705a9","a35b24a41e5748b8930fa70d6e795504","ce21520c9bac4044a25157662b091a11","66e0a0826a434c8db7f356c1a5aa8575","02165f6fcfb74fafb5206b582f628a93","3687d4444143421eae1d8c8231bf952d","51a81caf430147789513fc3485416b33","903e7d454cdc48979146cc88183042d7","c2f096e13f164acab1a66d96659249bd","a3c0b9000c3b4789b7a157459d4bd9c1","c8a18774fec54c0cb290edf5ffeb7c1a","079bf0f9293c43d988315ee189ff7c73","3772f26e95a3478894125d474a1bd5e3","42a91d50bb494be785a10dc07e64c57d","091f150610b0474c8b4227706e6c3878","4380c3157b2641949101c333de2860c4","00ef1659ddf440ac829bc99c0dcb1baf","d8b78b5823ad4efd9be2be03346d3009","9c6494c628cd4f33a41869f5044e1567","77814ee8e544404fa75ab09334721d43","1b3a2dc7dbf94a068ac59a1c71b393f6","ae849ff42adb4cfc9a82c45a28da07a8","bb9a2dedfed946069909eac89cd5fb0e","d9a4a518fd8642598f5f51bb5d3c3091","4deb2d112aa4427488bc32582289caa0","ed63af1291e140b68eb813f6525f4f88"]},"id":"7M6isbZqcgny","outputId":"9a8008ce-b84f-4a19-9952-7f2dd049f873"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mragas\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n","\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">ragas</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n","<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating 10 test case(s) in parallel: |          |  0% (0/10) [Time Taken: 00:00, ?test case/s]"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78edb9b2b80c483ab1d65a5e6ac7b00c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9be2346fdf034f3fae44bae5649ea544"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153226563d454cbd92ed327d61cbcf68"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"613a3541acae40d38eb8898a78acb445"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0b25a6b0364081907ec87b853ea6a4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af5f81ba63744f48969b6230f0e1196d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"915a91064f44458eac9dee659df36362"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55c66431dcad49088e1c81e2f1a59d68"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02165f6fcfb74fafb5206b582f628a93"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"display_data","data":{"text/plain":["Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4380c3157b2641949101c333de2860c4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating 10 test case(s) in parallel: |██████████|100% (10/10) [Time Taken: 00:49,  4.95s/test case]"]},{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node is perfectly ranked at the top, providing pertinent information on how 'Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition.' Kudos for getting it right!, error: None)\n","  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because every sentence in the expected output is perfectly supported by the information in the nodes in the retrieval context. Great job capturing all the details accurately!, error: None)\n","  - ✅ Contextual Relevancy (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.50 because while the retrieval context briefly mentions neural networks and their effectiveness in image and speech recognition, most of the context is focused on NLP, which is irrelevant to the input's focus on neural network architectures in image and speech recognition tasks., error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses how neural network architectures enhance deep learning in image and speech recognition tasks. Fantastic job!, error: None)\n","  - ❌ Answer Relevancy (ragas) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions. Everything is perfectly aligned and accurate. Great job!, error: None)\n","  - ❌ Hallucination (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the actual output 'I don't know' does not contain any relevant information or factual alignment with the context, resulting in a complete lack of factual consistency., error: None)\n","\n","For test case:\n","\n","  - input: In what ways do neural network architectures improve the effectiveness of deep learning in image and speech recognition tasks?\n","  - actual output: I don't know.\n","  - expected output: Neural network architectures, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), enhance the effectiveness of deep learning in image and speech recognition by efficiently processing data with multiple layers. CNNs are particularly adept at handling spatial hierarchies in images, allowing for the automatic detection of features like edges and textures. RNNs, on the other hand, excel in processing sequential data, making them suitable for tasks involving speech recognition, where temporal patterns are crucial. Together, these architectures improve accuracy and efficiency in recognizing complex patterns in images and speech.\n","  - context: ['Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.']\n","  - retrieval context: ['Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.', 'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.']\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant nodes in the retrieval context, mentioning 'NLP includes techniques like tokenization, stemming, and sentiment analysis, and that applications range to language translation services' in the first node, are ranked higher than irrelevant nodes that 'discuss artificial intelligence generally and do not relate to the specific techniques' in the second node. Great job ranking the relevant context at the top!, error: None)\n","  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because every sentence in the expected output is fully supported by information in the nodes in retrieval context, showcasing a perfect alignment. Great job!, error: None)\n","  - ✅ Contextual Relevancy (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.50 because while the input focuses on how tokenization, stemming, and sentiment analysis improve language translation applications, the relevant context only generally mentions that these techniques are part of NLP and applicable to language translation, without detailing their specific contributions., error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response was perfectly relevant without any irrelevant statements, directly addressing how tokenization, stemming, and sentiment analysis improve AI-based language translation applications. Great job!, error: None)\n","  - ❌ Answer Relevancy (ragas) (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating perfect alignment between the actual output and the retrieval context. Keep up the great work!, error: None)\n","  - ❌ Hallucination (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there is a complete lack of factual alignment between the actual output and the context, as the output 'I don't know' does not provide any relevant information or agreement with the provided context., error: None)\n","\n","For test case:\n","\n","  - input: How do tokenization, stemming, and sentiment analysis contribute to improving AI-based language translation applications?\n","  - actual output: I don't know.\n","  - expected output: Tokenization breaks text into manageable units, aiding accurate translation. Stemming reduces words to their base form, ensuring consistency across translations. Sentiment analysis helps maintain the intended tone, enhancing translation quality in AI-based applications.\n","  - context: ['NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.']\n","  - retrieval context: ['NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.', \"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"]\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node is perfectly ranked, providing insightful information about the architectural achievements and mysteries of the pyramids of Giza. Fantastic job!, error: None)\n","  - ❌ Contextual Recall (score: 0.3333333333333333, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.33 because only one sentence about the mysteries surrounding the pyramids (sentence 3) can be attributed to the nodes in retrieval context, specifically node 1. However, details about the architectural achievements, such as alignment and materials used (sentences 1 and 2), are not supported by any nodes in the retrieval context., error: None)\n","  - ✅ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the retrieval context perfectly matches the input, highlighting the architectural achievements and mysteries of the pyramids of Giza. Great job!, error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response was spot-on and directly addressed the architectural achievements and mysteries of the pyramids of Giza without any irrelevant information. Great job!, error: None)\n","  - ✅ Answer Relevancy (ragas) (score: 0.9949825375519107, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions in the actual output. Everything aligns perfectly with the information provided!, error: None)\n","  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions between the actual output and the provided context, indicating full alignment and no hallucinations., error: None)\n","\n","For test case:\n","\n","  - input: What are the architectural achievements and mysteries associated with the pyramids of Giza in ancient Egypt?\n","  - actual output: The architectural achievements of the pyramids of Giza include their function as tombs for pharaohs and their status as one of the Seven Wonders of the Ancient World. The mysteries associated with them pertain to the ongoing fascination and speculation regarding their construction.\n","  - expected output: The architectural achievements of the pyramids of Giza include their precise alignment with the cardinal points and their massive scale, with the Great Pyramid originally standing at 146.6 meters tall. The use of large limestone blocks, transported and assembled with remarkable accuracy, is also notable. Mysteries surrounding the pyramids include the exact methods used for construction and the purpose behind their specific design choices, which continue to intrigue researchers and historians.\n","  - context: ['Pyramids are ancient structures, often serving as tombs for pharaohs in Egypt. The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World. Their construction remains a subject of fascination and speculation.']\n","  - retrieval context: ['Pyramids are ancient structures, often serving as tombs for pharaohs in Egypt. The Great Pyramid of Giza is one of the Seven Wonders of the Ancient World. Their construction remains a subject of fascination and speculation.']\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node is perfectly aligned with the input, highlighting the importance of biology in understanding organisms' interactions and functions through cell and DNA studies. Great job!, error: None)\n","  - ✅ Contextual Recall (score: 0.6666666666666666, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.67 because the expected output's references to 'Modern biology' and 'DNA research' are well supported by node 1 in the retrieval context, while the discussion on 'Studying cells' in relation to homeostasis, growth, and environmental response is not specifically covered in the retrieval context., error: None)\n","  - ✅ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the context perfectly aligns with the input, focusing on how modern biology, particularly cell and DNA studies, helps us understand organisms' interactions and functions. Great job!, error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the question about modern biology's role in understanding organisms without any irrelevant information. Great job!, error: None)\n","  - ✅ Answer Relevancy (ragas) (score: 0.9697048968874903, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating perfect alignment between the actual output and the retrieval context. Keep up the great work!, error: None)\n","  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions and the actual output fully aligns with the provided context, indicating no hallucination., error: None)\n","\n","For test case:\n","\n","  - input: How does modern biology, particularly in the areas of cell and DNA studies, help us understand organisms' interactions and functions?\n","  - actual output: Modern biology, particularly in the areas of cell and DNA studies, helps us understand organisms' interactions and functions by providing insights into how organisms function and interact with each other and their environment.\n","  - expected output: Modern biology, through cell and DNA studies, helps us understand organisms' interactions and functions by revealing the molecular mechanisms underlying these processes. Studying cells allows us to see how organisms maintain homeostasis, grow, and respond to environmental changes. DNA research uncovers genetic instructions that guide organism development and behavior, offering insights into heredity, evolution, and the intricate relationships within ecosystems.\n","  - context: ['Biology is the study of living organisms, covering areas such as genetics, ecology, and physiology. It aims to understand how organisms function and interact with each other and their environment. Modern biology has advanced through the study of cells and DNA.']\n","  - retrieval context: ['Biology is the study of living organisms, covering areas such as genetics, ecology, and physiology. It aims to understand how organisms function and interact with each other and their environment. Modern biology has advanced through the study of cells and DNA.']\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the retrieval context perfectly aligns with the input question. It's all about quantum mechanics shaking up classical physics with wave-particle duality, which is spot on!, error: None)\n","  - ✅ Contextual Recall (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.50 because, while the retrieval context successfully aligns with the first sentence regarding wave-particle duality and its challenge to classical physics (found in the 1st node of the retrieval context), it lacks information about the paradigm shift and its impact on theories and technologies like quantum computing and quantum cryptography, which are key aspects of the subsequent sentences in the expected output., error: None)\n","  - ✅ Contextual Relevancy (score: 0.6666666666666666, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.67 because while the context mentioned 'Quantum mechanics is a branch of physics that studies the behavior of particles at atomic and subatomic levels' and 'It challenges classical physics with concepts like wave-particle duality and uncertainty,' it also contained irrelevant information about 'Key figures include Einstein, Schrödinger, and Heisenberg,' which does not directly address the implications of wave-particle duality on classical physics., error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the answer is completely relevant and addresses the implications of wave-particle duality in quantum mechanics on classical physics at subatomic levels without any irrelevant statements. Fantastic job!, error: None)\n","  - ✅ Answer Relevancy (ragas) (score: 1.0000000000000002, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, which means the actual output perfectly aligns with the retrieval context. Great job!, error: None)\n","  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, with no contradictions present., error: None)\n","\n","For test case:\n","\n","  - input: What are the implications of wave-particle duality in quantum mechanics on classical physics at subatomic levels?\n","  - actual output: The implications of wave-particle duality in quantum mechanics on classical physics at subatomic levels include challenging classical physics concepts and introducing ideas like uncertainty.\n","  - expected output: Wave-particle duality implies that particles at subatomic levels exhibit both wave-like and particle-like properties, challenging the classical physics notion that particles and waves are distinct. This duality requires a paradigm shift in understanding phenomena at atomic scales, influencing the development of theories and technologies such as quantum computing and quantum cryptography.\n","  - context: ['Quantum mechanics is a branch of physics that studies the behavior of particles at atomic and subatomic levels. It challenges classical physics with concepts like wave-particle duality and uncertainty. Key figures include Einstein, Schrödinger, and Heisenberg.']\n","  - retrieval context: ['Quantum mechanics is a branch of physics that studies the behavior of particles at atomic and subatomic levels. It challenges classical physics with concepts like wave-particle duality and uncertainty. Key figures include Einstein, Schrödinger, and Heisenberg.']\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node in the retrieval context, which explains that 'renewable energy sources, such as solar and wind, provide sustainable alternatives to fossil fuels' and 'reduce greenhouse gas emissions,' is correctly ranked higher than the irrelevant node about photosynthesis., error: None)\n","  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because all sentences in the expected output are fully supported by the nodes in retrieval context, specifically noting how solar and wind energy reduce emissions and are naturally replenished, aligning perfectly with the context provided. Great job on achieving a perfect match!, error: None)\n","  - ✅ Contextual Relevancy (score: 0.6, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.60 because while the retrieval context contains relevant statements about how 'renewable energy sources, such as solar and wind, provide sustainable alternatives to fossil fuels' and 'reduce greenhouse gas emissions,' it also includes irrelevant information about photosynthesis that does not pertain to the input question., error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addressed the input without any irrelevant information. Great job on providing clear and focused content!, error: None)\n","  - ✅ Answer Relevancy (ragas) (score: 0.9574269757828352, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating the actual output is perfectly aligned with the retrieval context. Great job maintaining accuracy!, error: None)\n","  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context with no contradictions, indicating no hallucination., error: None)\n","\n","For test case:\n","\n","  - input: In what ways do solar and wind energy contribute to reducing emissions and combating climate change by replenishing naturally?\n","  - actual output: Solar and wind energy contribute to reducing emissions and combating climate change by providing sustainable alternatives to fossil fuels and being naturally replenished, which helps reduce greenhouse gas emissions.\n","  - expected output: Solar and wind energy contribute to reducing emissions and combating climate change by providing clean, renewable power that does not produce greenhouse gases. These energy sources are naturally replenished by the sun and wind, ensuring a sustainable supply that lessens reliance on fossil fuels and decreases overall carbon emissions.\n","  - context: ['Renewable energy sources, such as solar and wind, provide sustainable alternatives to fossil fuels. These resources are replenished naturally and reduce greenhouse gas emissions. Governments are investing in renewables to combat climate change.']\n","  - retrieval context: ['Renewable energy sources, such as solar and wind, provide sustainable alternatives to fossil fuels. These resources are replenished naturally and reduce greenhouse gas emissions. Governments are investing in renewables to combat climate change.', 'Photosynthesis is the process plants use to convert sunlight into energy. This process produces glucose and releases oxygen as a byproduct. It is crucial for sustaining life on Earth by providing food and oxygen.']\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the node perfectly captures the essence of photosynthesis, detailing how it converts sunlight into energy, produces glucose, and releases oxygen, all of which are crucial for sustaining life on Earth. Great job!, error: None)\n","  - ✅ Contextual Recall (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.50 because the 1st sentence aligns well with the 1st node in the retrieval context, but the 2nd sentence lacks direct support from any node in the retrieval context regarding glucose's role as an energy source and in respiration., error: None)\n","  - ✅ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the retrieval context perfectly explains how photosynthesis produces glucose and oxygen, highlighting their importance for life on Earth. Great job!, error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the question about photosynthesis and its importance for life on Earth without any irrelevant information. Great job!, error: None)\n","  - ✅ Answer Relevancy (ragas) (score: 0.9394211662002191, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions. Great job on maintaining faithfulness to the retrieval context!, error: None)\n","  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context, with no contradictions present., error: None)\n","\n","For test case:\n","\n","  - input: How is glucose and oxygen produced through photosynthesis, and why are they important for life on Earth?\n","  - actual output: Photosynthesis produces glucose by converting sunlight into energy, and it releases oxygen as a byproduct. Glucose and oxygen are important for life on Earth because glucose provides food, and oxygen is essential for respiration.\n","  - expected output: Photosynthesis converts sunlight into energy, producing glucose and releasing oxygen as a byproduct. Glucose serves as a primary energy source for plants and other organisms, while oxygen is essential for respiration in most living beings, making both crucial for sustaining life on Earth.\n","  - context: ['Photosynthesis is the process plants use to convert sunlight into energy. This process produces glucose and releases oxygen as a byproduct. It is crucial for sustaining life on Earth by providing food and oxygen.']\n","  - retrieval context: ['Photosynthesis is the process plants use to convert sunlight into energy. This process produces glucose and releases oxygen as a byproduct. It is crucial for sustaining life on Earth by providing food and oxygen.']\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the node in retrieval context perfectly captures the essence of the input by explaining that 'cryptocurrency is a digital currency that uses cryptography for secure transactions,' which is precisely what was needed!, error: None)\n","  - ✅ Contextual Recall (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.50 because only part of the expected output is supported by the nodes in the retrieval context. Specifically, while the 1st node in the retrieval context supports the general idea of securing cryptocurrency transactions, there is no mention of 'protecting user identities', 'verifying authenticity through digital signatures', or 'double-spending', which are key components of the expected output., error: None)\n","  - ✅ Contextual Relevancy (score: 0.6666666666666666, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.67 because while the context includes relevant information like 'Cryptocurrency is a digital currency that uses cryptography for secure transactions,' it also contains irrelevant historical details about Bitcoin's launch, which do not pertain to the role of cryptography in security., error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the question about cryptography's role in securing peer-to-peer cryptocurrency transactions without any irrelevant information. Great job!, error: None)\n","  - ✅ Answer Relevancy (ragas) (score: 0.9826834756796661, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions between the actual output and the retrieval context. Keep up the great work!, error: None)\n","  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context without any contradictions, indicating no hallucinations., error: None)\n","\n","For test case:\n","\n","  - input: In what ways does cryptography contribute to the security of peer-to-peer cryptocurrency transactions such as Bitcoin?\n","  - actual output: Cryptography contributes to the security of peer-to-peer cryptocurrency transactions by ensuring secure transactions.\n","  - expected output: Cryptography ensures the security of peer-to-peer cryptocurrency transactions by encrypting transaction data, protecting user identities, and verifying the authenticity of transactions through digital signatures. It also secures the network against double-spending and other fraudulent activities by employing cryptographic algorithms to maintain the integrity of the blockchain.\n","  - context: ['Cryptocurrency is a digital currency that uses cryptography for secure transactions. Bitcoin, the first cryptocurrency, was launched in 2009. Its decentralized nature allows peer-to-peer transfers without traditional banks.']\n","  - retrieval context: ['Cryptocurrency is a digital currency that uses cryptography for secure transactions. Bitcoin, the first cryptocurrency, was launched in 2009. Its decentralized nature allows peer-to-peer transfers without traditional banks.']\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the first node in the retrieval context directly addresses how 'machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data,' perfectly matching the input. The irrelevant nodes discuss 'deep learning specifics' and a 'general overview of artificial intelligence,' which do not specifically relate to the input, but they are ranked lower, ensuring the precision is spot on!, error: None)\n","  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because every sentence in the expected output is thoroughly supported by information in the nodes in retrieval context, showing a perfect alignment. Great job!, error: None)\n","  - ✅ Contextual Relevancy (score: 0.6666666666666666, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.67 because while statements like 'Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data' and 'Algorithms analyze past data to make predictions or classify information' are relevant, they do not comprehensively address how machine learning algorithms specifically use data patterns for the input's focus on predictions and recommendations, as noted in the reasons for irrelevancy., error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant to the question about how machine learning algorithms use data patterns for predictions and applications like recommendations. Keep up the great work!, error: None)\n","  - ✅ Answer Relevancy (ragas) (score: 0.974765686309914, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job!, error: None)\n","  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions., error: None)\n","\n","For test case:\n","\n","  - input: How can machine learning algorithms use data patterns effectively for predictions and applications such as recommendations?\n","  - actual output: Machine learning algorithms use data patterns effectively for predictions and applications such as recommendations by analyzing past data to make predictions or classify information.\n","  - expected output: Machine learning algorithms use data patterns effectively by analyzing historical data to identify trends and relationships. This enables them to make accurate predictions and classifications. For applications like recommendation systems, these algorithms assess user behavior to suggest relevant items, while in image recognition, they identify distinct features to classify images accurately.\n","  - context: ['Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.']\n","  - retrieval context: ['Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.', 'Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.', \"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"]\n","\n","======================================================================\n","\n","Metrics Summary\n","\n","  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because all relevant nodes are ranked higher than irrelevant nodes. Great job!, error: None)\n","  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the expected output is perfectly aligned with the information in the nodes in the retrieval context. Keep up the great work!, error: None)\n","  - ✅ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the retrieval context perfectly aligns with the input, covering AI applications like virtual assistants, robotics, and autonomous vehicles, and highlighting the role of machine learning and deep learning in mimicking human intelligence., error: None)\n","  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the inquiry about AI applications using machine learning and deep learning to mimic human intelligence without any irrelevant information. Great job!, error: None)\n","  - ✅ Answer Relevancy (ragas) (score: 0.9999999999999991, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n","  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating the actual output is perfectly aligned with the retrieval context. Great job!, error: None)\n","  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output is fully aligned with the provided context, with no contradictions present., error: None)\n","\n","For test case:\n","\n","  - input: What are some AI applications that demonstrate the use of machine learning and deep learning to mimic human intelligence?\n","  - actual output: Some AI applications that demonstrate the use of machine learning and deep learning to mimic human intelligence include virtual assistants, robotics, autonomous vehicles, recommendation systems, image recognition, chatbots, and language translation services.\n","  - expected output: AI applications that demonstrate the use of machine learning and deep learning to mimic human intelligence include virtual assistants like Siri and Alexa, robotics for tasks like manufacturing automation, and autonomous vehicles that navigate and make decisions on the road.\n","  - context: [\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"]\n","  - retrieval context: [\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\", 'Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.', 'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.']\n","\n","======================================================================\n","\n","Overall Metric Pass Rates\n","\n","Contextual Precision: 100.00% pass rate\n","Contextual Recall: 90.00% pass rate\n","Contextual Relevancy: 100.00% pass rate\n","Answer Relevancy: 100.00% pass rate\n","Answer Relevancy (ragas): 80.00% pass rate\n","Faithfulness: 100.00% pass rate\n","Hallucination: 80.00% pass rate\n","\n","======================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n","‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n","instead.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n","‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n","instead.\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["eval_results.test_results[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24TsKs6JgbL7","outputId":"1bccb937-a048-42a1-c599-6c4df3acf87f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResult(success=False, metrics_data=[MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason=\"The score is 1.00 because the relevant node is perfectly ranked at the top, providing pertinent information on how 'Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition.' Kudos for getting it right!\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0046825, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context mentions \\'Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition.\\' This is relevant as it addresses the effectiveness of neural network architectures in enhancing deep learning for these tasks.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context discusses \\'NLP as a branch of AI,\\' which is not directly relevant to image and speech recognition tasks mentioned in the input and expected output.\"\\n    }\\n]'), MetricData(name='Contextual Recall', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because every sentence in the expected output is perfectly supported by the information in the nodes in the retrieval context. Great job capturing all the details accurately!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0052125, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The 1st node mentions \\'neural networks with many layers\\' and \\'image and speech recognition\\', aligning with the sentence about CNNs and RNNs enhancing deep learning.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The 1st node refers to \\'convolutional...networks...used\\', supporting the sentence about CNNs handling spatial hierarchies in images.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The 1st node mentions \\'recurrent neural networks\\', relating to the sentence about RNNs processing sequential data in speech recognition.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The 1st node discusses \\'deep learning...excels in...speech recognition\\', which supports the sentence on architectures improving accuracy and efficiency in recognizing patterns.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.5, reason=\"The score is 0.50 because while the retrieval context briefly mentions neural networks and their effectiveness in image and speech recognition, most of the context is focused on NLP, which is irrelevant to the input's focus on neural network architectures in image and speech recognition tasks.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0063725000000000006, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"Deep learning is a subset of machine learning utilizing neural networks with many layers.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"It excels in complex tasks like image and speech recognition.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Convolutional and recurrent neural networks are among the common architectures used.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"NLP is a branch of AI that enables computers to understand, interpret, and generate human language.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement is about NLP and human language, not about neural network architectures or their role in image and speech recognition.\"\\n            },\\n            {\\n                \"statement\": \"Techniques include tokenization, stemming, and sentiment analysis.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The techniques mentioned are related to NLP, not neural network architectures or deep learning in image and speech recognition.\"\\n            },\\n            {\\n                \"statement\": \"Applications range from chatbots to language translation services.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The applications mentioned are related to NLP, not neural network architectures or their role in image and speech recognition tasks.\"\\n            }\\n        ]\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response is perfectly relevant and directly addresses how neural network architectures enhance deep learning in image and speech recognition tasks. Fantastic job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0027375000000000003, verbose_logs='Statements:\\n[\\n    \"I don\\'t know.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Answer Relevancy (ragas)', threshold=0.5, success=False, score=0.0, reason=None, strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions. Everything is perfectly aligned and accurate. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0038725, verbose_logs='Truths (limit=None):\\n[\\n    \"Deep learning is a subset of machine learning.\",\\n    \"Deep learning utilizes neural networks with many layers.\",\\n    \"Deep learning excels in complex tasks like image recognition.\",\\n    \"Deep learning excels in complex tasks like speech recognition.\",\\n    \"Convolutional neural networks are among the common architectures used in deep learning.\",\\n    \"Recurrent neural networks are among the common architectures used in deep learning.\",\\n    \"NLP is a branch of AI.\",\\n    \"NLP enables computers to understand human language.\",\\n    \"NLP enables computers to interpret human language.\",\\n    \"NLP enables computers to generate human language.\",\\n    \"NLP techniques include tokenization.\",\\n    \"NLP techniques include stemming.\",\\n    \"NLP techniques include sentiment analysis.\",\\n    \"NLP applications range from chatbots to language translation services.\"\\n] \\n \\nClaims:\\n[] \\n \\nVerdicts:\\n[]'), MetricData(name='Hallucination', threshold=0.5, success=False, score=1.0, reason=\"The score is 1.00 because the actual output 'I don't know' does not contain any relevant information or factual alignment with the context, resulting in a complete lack of factual consistency.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.002935, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The actual output \\'I don\\'t know\\' does not provide any relevant information to agree or disagree with the context. A corrected response would include information about deep learning being a subset of machine learning utilizing neural networks with many layers, excelling in complex tasks like image and speech recognition.\"\\n    }\\n]')], conversational=False, multimodal=False, input='In what ways do neural network architectures improve the effectiveness of deep learning in image and speech recognition tasks?', actual_output=\"I don't know.\", expected_output='Neural network architectures, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), enhance the effectiveness of deep learning in image and speech recognition by efficiently processing data with multiple layers. CNNs are particularly adept at handling spatial hierarchies in images, allowing for the automatic detection of features like edges and textures. RNNs, on the other hand, excel in processing sequential data, making them suitable for tasks involving speech recognition, where temporal patterns are crucial. Together, these architectures improve accuracy and efficiency in recognizing complex patterns in images and speech.', context=['Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.'], retrieval_context=['Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.', 'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.'])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["eval_metrics = []\n","for result in eval_results.test_results:\n","    eval_dict = {}\n","    eval_dict['Input'] = result.input\n","    eval_dict['Expected Output'] = result.expected_output\n","    eval_dict['Actual Output'] = result.actual_output\n","    eval_dict['Context'] = result.context\n","    eval_dict['Retrieval Context'] = result.retrieval_context\n","    eval_dict['Success'] = result.success\n","    metrics = result.metrics_data\n","    for metric in metrics:\n","        eval_dict[metric.name+'_Score'] = metric.score\n","    for metric in metrics:\n","        eval_dict[metric.name+'_Success'] = metric.success\n","    for metric in metrics:\n","        eval_dict[metric.name+'_Reason'] = metric.reason\n","    eval_metrics.append(eval_dict)"],"metadata":{"id":"wykn6lUXeRoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_metrics[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9J4KlVzIfXCP","outputId":"8d80c436-9951-458e-a54b-637ce96061b6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Input': 'In what ways do neural network architectures improve the effectiveness of deep learning in image and speech recognition tasks?',\n"," 'Expected Output': 'Neural network architectures, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), enhance the effectiveness of deep learning in image and speech recognition by efficiently processing data with multiple layers. CNNs are particularly adept at handling spatial hierarchies in images, allowing for the automatic detection of features like edges and textures. RNNs, on the other hand, excel in processing sequential data, making them suitable for tasks involving speech recognition, where temporal patterns are crucial. Together, these architectures improve accuracy and efficiency in recognizing complex patterns in images and speech.',\n"," 'Actual Output': \"I don't know.\",\n"," 'Context': ['Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.'],\n"," 'Retrieval Context': ['Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition. Convolutional and recurrent neural networks are among the common architectures used.',\n","  'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.'],\n"," 'Success': False,\n"," 'Contextual Precision_Score': 1.0,\n"," 'Contextual Recall_Score': 1.0,\n"," 'Contextual Relevancy_Score': 0.5,\n"," 'Answer Relevancy_Score': 1.0,\n"," 'Answer Relevancy (ragas)_Score': 0.0,\n"," 'Faithfulness_Score': 1.0,\n"," 'Hallucination_Score': 1.0,\n"," 'Contextual Precision_Success': True,\n"," 'Contextual Recall_Success': True,\n"," 'Contextual Relevancy_Success': True,\n"," 'Answer Relevancy_Success': True,\n"," 'Answer Relevancy (ragas)_Success': False,\n"," 'Faithfulness_Success': True,\n"," 'Hallucination_Success': False,\n"," 'Contextual Precision_Reason': \"The score is 1.00 because the relevant node is perfectly ranked at the top, providing pertinent information on how 'Deep learning is a subset of machine learning utilizing neural networks with many layers. It excels in complex tasks like image and speech recognition.' Kudos for getting it right!\",\n"," 'Contextual Recall_Reason': 'The score is 1.00 because every sentence in the expected output is perfectly supported by the information in the nodes in the retrieval context. Great job capturing all the details accurately!',\n"," 'Contextual Relevancy_Reason': \"The score is 0.50 because while the retrieval context briefly mentions neural networks and their effectiveness in image and speech recognition, most of the context is focused on NLP, which is irrelevant to the input's focus on neural network architectures in image and speech recognition tasks.\",\n"," 'Answer Relevancy_Reason': 'The score is 1.00 because the response is perfectly relevant and directly addresses how neural network architectures enhance deep learning in image and speech recognition tasks. Fantastic job!',\n"," 'Answer Relevancy (ragas)_Reason': None,\n"," 'Faithfulness_Reason': 'The score is 1.00 because there are no contradictions. Everything is perfectly aligned and accurate. Great job!',\n"," 'Hallucination_Reason': \"The score is 1.00 because the actual output 'I don't know' does not contain any relevant information or factual alignment with the context, resulting in a complete lack of factual consistency.\"}"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["import pandas as pd\n","\n","eval_results_df = pd.DataFrame(eval_metrics)\n","eval_results_df.T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0chd5WCigMwo","outputId":"ac57379c-49b3-48ce-b3d3-322f9761cb8c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                  0  \\\n","Input                             In what ways do neural network architectures i...   \n","Expected Output                   Neural network architectures, such as convolut...   \n","Actual Output                                                         I don't know.   \n","Context                           [Deep learning is a subset of machine learning...   \n","Retrieval Context                 [Deep learning is a subset of machine learning...   \n","Success                                                                       False   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                         1.0   \n","Contextual Relevancy_Score                                                      0.5   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                                  0.0   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             1.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                      True   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                              False   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                         False   \n","Contextual Precision_Reason       The score is 1.00 because the relevant node is...   \n","Contextual Recall_Reason          The score is 1.00 because every sentence in th...   \n","Contextual Relevancy_Reason       The score is 0.50 because while the retrieval ...   \n","Answer Relevancy_Reason           The score is 1.00 because the response is perf...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 1.00 because the actual output 'I...   \n","\n","                                                                                  1  \\\n","Input                             How do tokenization, stemming, and sentiment a...   \n","Expected Output                   Tokenization breaks text into manageable units...   \n","Actual Output                                                         I don't know.   \n","Context                           [NLP is a branch of AI that enables computers ...   \n","Retrieval Context                 [NLP is a branch of AI that enables computers ...   \n","Success                                                                       False   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                         1.0   \n","Contextual Relevancy_Score                                                      0.5   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                                  0.0   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             1.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                      True   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                              False   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                         False   \n","Contextual Precision_Reason       The score is 1.00 because the relevant nodes i...   \n","Contextual Recall_Reason          The score is 1.00 because every sentence in th...   \n","Contextual Relevancy_Reason       The score is 0.50 because while the input focu...   \n","Answer Relevancy_Reason           The score is 1.00 because the response was per...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 1.00 because there is a complete ...   \n","\n","                                                                                  2  \\\n","Input                             What are the architectural achievements and my...   \n","Expected Output                   The architectural achievements of the pyramids...   \n","Actual Output                     The architectural achievements of the pyramids...   \n","Context                           [Pyramids are ancient structures, often servin...   \n","Retrieval Context                 [Pyramids are ancient structures, often servin...   \n","Success                                                                       False   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                    0.333333   \n","Contextual Relevancy_Score                                                      1.0   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                             0.994983   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             0.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                     False   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                               True   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                          True   \n","Contextual Precision_Reason       The score is 1.00 because the relevant node is...   \n","Contextual Recall_Reason          The score is 0.33 because only one sentence ab...   \n","Contextual Relevancy_Reason       The score is 1.00 because the retrieval contex...   \n","Answer Relevancy_Reason           The score is 1.00 because the response was spo...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 0.00 because there are no contrad...   \n","\n","                                                                                  3  \\\n","Input                             How does modern biology, particularly in the a...   \n","Expected Output                   Modern biology, through cell and DNA studies, ...   \n","Actual Output                     Modern biology, particularly in the areas of c...   \n","Context                           [Biology is the study of living organisms, cov...   \n","Retrieval Context                 [Biology is the study of living organisms, cov...   \n","Success                                                                        True   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                    0.666667   \n","Contextual Relevancy_Score                                                      1.0   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                             0.969705   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             0.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                      True   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                               True   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                          True   \n","Contextual Precision_Reason       The score is 1.00 because the relevant node is...   \n","Contextual Recall_Reason          The score is 0.67 because the expected output'...   \n","Contextual Relevancy_Reason       The score is 1.00 because the context perfectl...   \n","Answer Relevancy_Reason           The score is 1.00 because the response perfect...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 0.00 because there are no contrad...   \n","\n","                                                                                  4  \\\n","Input                             What are the implications of wave-particle dua...   \n","Expected Output                   Wave-particle duality implies that particles a...   \n","Actual Output                     The implications of wave-particle duality in q...   \n","Context                           [Quantum mechanics is a branch of physics that...   \n","Retrieval Context                 [Quantum mechanics is a branch of physics that...   \n","Success                                                                        True   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                         0.5   \n","Contextual Relevancy_Score                                                 0.666667   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                                  1.0   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             0.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                      True   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                               True   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                          True   \n","Contextual Precision_Reason       The score is 1.00 because the retrieval contex...   \n","Contextual Recall_Reason          The score is 0.50 because, while the retrieval...   \n","Contextual Relevancy_Reason       The score is 0.67 because while the context me...   \n","Answer Relevancy_Reason           The score is 1.00 because the answer is comple...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 0.00 because the actual output fu...   \n","\n","                                                                                  5  \\\n","Input                             In what ways do solar and wind energy contribu...   \n","Expected Output                   Solar and wind energy contribute to reducing e...   \n","Actual Output                     Solar and wind energy contribute to reducing e...   \n","Context                           [Renewable energy sources, such as solar and w...   \n","Retrieval Context                 [Renewable energy sources, such as solar and w...   \n","Success                                                                        True   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                         1.0   \n","Contextual Relevancy_Score                                                      0.6   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                             0.957427   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             0.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                      True   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                               True   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                          True   \n","Contextual Precision_Reason       The score is 1.00 because the relevant node in...   \n","Contextual Recall_Reason          The score is 1.00 because all sentences in the...   \n","Contextual Relevancy_Reason       The score is 0.60 because while the retrieval ...   \n","Answer Relevancy_Reason           The score is 1.00 because the output perfectly...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 0.00 because the actual output fu...   \n","\n","                                                                                  6  \\\n","Input                             How is glucose and oxygen produced through pho...   \n","Expected Output                   Photosynthesis converts sunlight into energy, ...   \n","Actual Output                     Photosynthesis produces glucose by converting ...   \n","Context                           [Photosynthesis is the process plants use to c...   \n","Retrieval Context                 [Photosynthesis is the process plants use to c...   \n","Success                                                                        True   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                         0.5   \n","Contextual Relevancy_Score                                                      1.0   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                             0.939421   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             0.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                      True   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                               True   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                          True   \n","Contextual Precision_Reason       The score is 1.00 because the node perfectly c...   \n","Contextual Recall_Reason          The score is 0.50 because the 1st sentence ali...   \n","Contextual Relevancy_Reason       The score is 1.00 because the retrieval contex...   \n","Answer Relevancy_Reason           The score is 1.00 because the output perfectly...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 0.00 because the actual output fu...   \n","\n","                                                                                  7  \\\n","Input                             In what ways does cryptography contribute to t...   \n","Expected Output                   Cryptography ensures the security of peer-to-p...   \n","Actual Output                     Cryptography contributes to the security of pe...   \n","Context                           [Cryptocurrency is a digital currency that use...   \n","Retrieval Context                 [Cryptocurrency is a digital currency that use...   \n","Success                                                                        True   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                         0.5   \n","Contextual Relevancy_Score                                                 0.666667   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                             0.982683   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             0.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                      True   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                               True   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                          True   \n","Contextual Precision_Reason       The score is 1.00 because the node in retrieva...   \n","Contextual Recall_Reason          The score is 0.50 because only part of the exp...   \n","Contextual Relevancy_Reason       The score is 0.67 because while the context in...   \n","Answer Relevancy_Reason           The score is 1.00 because the output perfectly...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 0.00 because the actual output fu...   \n","\n","                                                                                  8  \\\n","Input                             How can machine learning algorithms use data p...   \n","Expected Output                   Machine learning algorithms use data patterns ...   \n","Actual Output                     Machine learning algorithms use data patterns ...   \n","Context                           [Machine learning is a field of artificial int...   \n","Retrieval Context                 [Machine learning is a field of artificial int...   \n","Success                                                                        True   \n","Contextual Precision_Score                                                      1.0   \n","Contextual Recall_Score                                                         1.0   \n","Contextual Relevancy_Score                                                 0.666667   \n","Answer Relevancy_Score                                                          1.0   \n","Answer Relevancy (ragas)_Score                                             0.974766   \n","Faithfulness_Score                                                              1.0   \n","Hallucination_Score                                                             0.0   \n","Contextual Precision_Success                                                   True   \n","Contextual Recall_Success                                                      True   \n","Contextual Relevancy_Success                                                   True   \n","Answer Relevancy_Success                                                       True   \n","Answer Relevancy (ragas)_Success                                               True   \n","Faithfulness_Success                                                           True   \n","Hallucination_Success                                                          True   \n","Contextual Precision_Reason       The score is 1.00 because the first node in th...   \n","Contextual Recall_Reason          The score is 1.00 because every sentence in th...   \n","Contextual Relevancy_Reason       The score is 0.67 because while statements lik...   \n","Answer Relevancy_Reason           The score is 1.00 because the response is perf...   \n","Answer Relevancy (ragas)_Reason                                                None   \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...   \n","Hallucination_Reason              The score is 0.00 because the actual output fu...   \n","\n","                                                                                  9  \n","Input                             What are some AI applications that demonstrate...  \n","Expected Output                   AI applications that demonstrate the use of ma...  \n","Actual Output                     Some AI applications that demonstrate the use ...  \n","Context                           [Artificial intelligence refers to machines mi...  \n","Retrieval Context                 [Artificial intelligence refers to machines mi...  \n","Success                                                                        True  \n","Contextual Precision_Score                                                      1.0  \n","Contextual Recall_Score                                                         1.0  \n","Contextual Relevancy_Score                                                      1.0  \n","Answer Relevancy_Score                                                          1.0  \n","Answer Relevancy (ragas)_Score                                                  1.0  \n","Faithfulness_Score                                                              1.0  \n","Hallucination_Score                                                             0.0  \n","Contextual Precision_Success                                                   True  \n","Contextual Recall_Success                                                      True  \n","Contextual Relevancy_Success                                                   True  \n","Answer Relevancy_Success                                                       True  \n","Answer Relevancy (ragas)_Success                                               True  \n","Faithfulness_Success                                                           True  \n","Hallucination_Success                                                          True  \n","Contextual Precision_Reason       The score is 1.00 because all relevant nodes a...  \n","Contextual Recall_Reason          The score is 1.00 because the expected output ...  \n","Contextual Relevancy_Reason       The score is 1.00 because the retrieval contex...  \n","Answer Relevancy_Reason           The score is 1.00 because the response perfect...  \n","Answer Relevancy (ragas)_Reason                                                None  \n","Faithfulness_Reason               The score is 1.00 because there are no contrad...  \n","Hallucination_Reason              The score is 0.00 because the actual output is...  "],"text/html":["\n","  <div id=\"df-5f5ffaa8-6650-4036-8e9a-990c028902e4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Input</th>\n","      <td>In what ways do neural network architectures i...</td>\n","      <td>How do tokenization, stemming, and sentiment a...</td>\n","      <td>What are the architectural achievements and my...</td>\n","      <td>How does modern biology, particularly in the a...</td>\n","      <td>What are the implications of wave-particle dua...</td>\n","      <td>In what ways do solar and wind energy contribu...</td>\n","      <td>How is glucose and oxygen produced through pho...</td>\n","      <td>In what ways does cryptography contribute to t...</td>\n","      <td>How can machine learning algorithms use data p...</td>\n","      <td>What are some AI applications that demonstrate...</td>\n","    </tr>\n","    <tr>\n","      <th>Expected Output</th>\n","      <td>Neural network architectures, such as convolut...</td>\n","      <td>Tokenization breaks text into manageable units...</td>\n","      <td>The architectural achievements of the pyramids...</td>\n","      <td>Modern biology, through cell and DNA studies, ...</td>\n","      <td>Wave-particle duality implies that particles a...</td>\n","      <td>Solar and wind energy contribute to reducing e...</td>\n","      <td>Photosynthesis converts sunlight into energy, ...</td>\n","      <td>Cryptography ensures the security of peer-to-p...</td>\n","      <td>Machine learning algorithms use data patterns ...</td>\n","      <td>AI applications that demonstrate the use of ma...</td>\n","    </tr>\n","    <tr>\n","      <th>Actual Output</th>\n","      <td>I don't know.</td>\n","      <td>I don't know.</td>\n","      <td>The architectural achievements of the pyramids...</td>\n","      <td>Modern biology, particularly in the areas of c...</td>\n","      <td>The implications of wave-particle duality in q...</td>\n","      <td>Solar and wind energy contribute to reducing e...</td>\n","      <td>Photosynthesis produces glucose by converting ...</td>\n","      <td>Cryptography contributes to the security of pe...</td>\n","      <td>Machine learning algorithms use data patterns ...</td>\n","      <td>Some AI applications that demonstrate the use ...</td>\n","    </tr>\n","    <tr>\n","      <th>Context</th>\n","      <td>[Deep learning is a subset of machine learning...</td>\n","      <td>[NLP is a branch of AI that enables computers ...</td>\n","      <td>[Pyramids are ancient structures, often servin...</td>\n","      <td>[Biology is the study of living organisms, cov...</td>\n","      <td>[Quantum mechanics is a branch of physics that...</td>\n","      <td>[Renewable energy sources, such as solar and w...</td>\n","      <td>[Photosynthesis is the process plants use to c...</td>\n","      <td>[Cryptocurrency is a digital currency that use...</td>\n","      <td>[Machine learning is a field of artificial int...</td>\n","      <td>[Artificial intelligence refers to machines mi...</td>\n","    </tr>\n","    <tr>\n","      <th>Retrieval Context</th>\n","      <td>[Deep learning is a subset of machine learning...</td>\n","      <td>[NLP is a branch of AI that enables computers ...</td>\n","      <td>[Pyramids are ancient structures, often servin...</td>\n","      <td>[Biology is the study of living organisms, cov...</td>\n","      <td>[Quantum mechanics is a branch of physics that...</td>\n","      <td>[Renewable energy sources, such as solar and w...</td>\n","      <td>[Photosynthesis is the process plants use to c...</td>\n","      <td>[Cryptocurrency is a digital currency that use...</td>\n","      <td>[Machine learning is a field of artificial int...</td>\n","      <td>[Artificial intelligence refers to machines mi...</td>\n","    </tr>\n","    <tr>\n","      <th>Success</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Precision_Score</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Recall_Score</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Relevancy_Score</th>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.666667</td>\n","      <td>0.6</td>\n","      <td>1.0</td>\n","      <td>0.666667</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Answer Relevancy_Score</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Answer Relevancy (ragas)_Score</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.994983</td>\n","      <td>0.969705</td>\n","      <td>1.0</td>\n","      <td>0.957427</td>\n","      <td>0.939421</td>\n","      <td>0.982683</td>\n","      <td>0.974766</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Faithfulness_Score</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>Hallucination_Score</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Precision_Success</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Recall_Success</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Relevancy_Success</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Answer Relevancy_Success</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Answer Relevancy (ragas)_Success</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Faithfulness_Success</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Hallucination_Success</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Precision_Reason</th>\n","      <td>The score is 1.00 because the relevant node is...</td>\n","      <td>The score is 1.00 because the relevant nodes i...</td>\n","      <td>The score is 1.00 because the relevant node is...</td>\n","      <td>The score is 1.00 because the relevant node is...</td>\n","      <td>The score is 1.00 because the retrieval contex...</td>\n","      <td>The score is 1.00 because the relevant node in...</td>\n","      <td>The score is 1.00 because the node perfectly c...</td>\n","      <td>The score is 1.00 because the node in retrieva...</td>\n","      <td>The score is 1.00 because the first node in th...</td>\n","      <td>The score is 1.00 because all relevant nodes a...</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Recall_Reason</th>\n","      <td>The score is 1.00 because every sentence in th...</td>\n","      <td>The score is 1.00 because every sentence in th...</td>\n","      <td>The score is 0.33 because only one sentence ab...</td>\n","      <td>The score is 0.67 because the expected output'...</td>\n","      <td>The score is 0.50 because, while the retrieval...</td>\n","      <td>The score is 1.00 because all sentences in the...</td>\n","      <td>The score is 0.50 because the 1st sentence ali...</td>\n","      <td>The score is 0.50 because only part of the exp...</td>\n","      <td>The score is 1.00 because every sentence in th...</td>\n","      <td>The score is 1.00 because the expected output ...</td>\n","    </tr>\n","    <tr>\n","      <th>Contextual Relevancy_Reason</th>\n","      <td>The score is 0.50 because while the retrieval ...</td>\n","      <td>The score is 0.50 because while the input focu...</td>\n","      <td>The score is 1.00 because the retrieval contex...</td>\n","      <td>The score is 1.00 because the context perfectl...</td>\n","      <td>The score is 0.67 because while the context me...</td>\n","      <td>The score is 0.60 because while the retrieval ...</td>\n","      <td>The score is 1.00 because the retrieval contex...</td>\n","      <td>The score is 0.67 because while the context in...</td>\n","      <td>The score is 0.67 because while statements lik...</td>\n","      <td>The score is 1.00 because the retrieval contex...</td>\n","    </tr>\n","    <tr>\n","      <th>Answer Relevancy_Reason</th>\n","      <td>The score is 1.00 because the response is perf...</td>\n","      <td>The score is 1.00 because the response was per...</td>\n","      <td>The score is 1.00 because the response was spo...</td>\n","      <td>The score is 1.00 because the response perfect...</td>\n","      <td>The score is 1.00 because the answer is comple...</td>\n","      <td>The score is 1.00 because the output perfectly...</td>\n","      <td>The score is 1.00 because the output perfectly...</td>\n","      <td>The score is 1.00 because the output perfectly...</td>\n","      <td>The score is 1.00 because the response is perf...</td>\n","      <td>The score is 1.00 because the response perfect...</td>\n","    </tr>\n","    <tr>\n","      <th>Answer Relevancy (ragas)_Reason</th>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>Faithfulness_Reason</th>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","      <td>The score is 1.00 because there are no contrad...</td>\n","    </tr>\n","    <tr>\n","      <th>Hallucination_Reason</th>\n","      <td>The score is 1.00 because the actual output 'I...</td>\n","      <td>The score is 1.00 because there is a complete ...</td>\n","      <td>The score is 0.00 because there are no contrad...</td>\n","      <td>The score is 0.00 because there are no contrad...</td>\n","      <td>The score is 0.00 because the actual output fu...</td>\n","      <td>The score is 0.00 because the actual output fu...</td>\n","      <td>The score is 0.00 because the actual output fu...</td>\n","      <td>The score is 0.00 because the actual output fu...</td>\n","      <td>The score is 0.00 because the actual output fu...</td>\n","      <td>The score is 0.00 because the actual output is...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f5ffaa8-6650-4036-8e9a-990c028902e4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5f5ffaa8-6650-4036-8e9a-990c028902e4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5f5ffaa8-6650-4036-8e9a-990c028902e4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-fd17a226-a27b-42fd-afa8-967fe99f5c03\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fd17a226-a27b-42fd-afa8-967fe99f5c03')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-fd17a226-a27b-42fd-afa8-967fe99f5c03 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"eval_results_df\",\n  \"rows\": 27,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["eval_results_df[['Contextual Precision_Score', 'Contextual Recall_Score', 'Contextual Relevancy_Score',\n","                 'Answer Relevancy_Score', 'Answer Relevancy (ragas)_Score',\n","                 'Faithfulness_Score', 'Hallucination_Score']].describe()"],"metadata":{"id":"pROkSYPUgRse","colab":{"base_uri":"https://localhost:8080/","height":355},"outputId":"d9834373-7009-467e-d8fe-92e801aa4886"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Contextual Precision_Score  Contextual Recall_Score  \\\n","count                        10.0                10.000000   \n","mean                          1.0                 0.750000   \n","std                           0.0                 0.274986   \n","min                           1.0                 0.333333   \n","25%                           1.0                 0.500000   \n","50%                           1.0                 0.833333   \n","75%                           1.0                 1.000000   \n","max                           1.0                 1.000000   \n","\n","       Contextual Relevancy_Score  Answer Relevancy_Score  \\\n","count                   10.000000                    10.0   \n","mean                     0.760000                     1.0   \n","std                      0.215338                     0.0   \n","min                      0.500000                     1.0   \n","25%                      0.616667                     1.0   \n","50%                      0.666667                     1.0   \n","75%                      1.000000                     1.0   \n","max                      1.000000                     1.0   \n","\n","       Answer Relevancy (ragas)_Score  Faithfulness_Score  Hallucination_Score  \n","count                       10.000000                10.0            10.000000  \n","mean                         0.781898                 1.0             0.200000  \n","std                          0.412537                 0.0             0.421637  \n","min                          0.000000                 1.0             0.000000  \n","25%                          0.943923                 1.0             0.000000  \n","50%                          0.972235                 1.0             0.000000  \n","75%                          0.991908                 1.0             0.000000  \n","max                          1.000000                 1.0             1.000000  "],"text/html":["\n","  <div id=\"df-8cbcf6fa-b339-4ea7-a62c-20dbf3724e06\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Contextual Precision_Score</th>\n","      <th>Contextual Recall_Score</th>\n","      <th>Contextual Relevancy_Score</th>\n","      <th>Answer Relevancy_Score</th>\n","      <th>Answer Relevancy (ragas)_Score</th>\n","      <th>Faithfulness_Score</th>\n","      <th>Hallucination_Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>10.0</td>\n","      <td>10.000000</td>\n","      <td>10.000000</td>\n","      <td>10.0</td>\n","      <td>10.000000</td>\n","      <td>10.0</td>\n","      <td>10.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>1.0</td>\n","      <td>0.750000</td>\n","      <td>0.760000</td>\n","      <td>1.0</td>\n","      <td>0.781898</td>\n","      <td>1.0</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.0</td>\n","      <td>0.274986</td>\n","      <td>0.215338</td>\n","      <td>0.0</td>\n","      <td>0.412537</td>\n","      <td>0.0</td>\n","      <td>0.421637</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.0</td>\n","      <td>0.333333</td>\n","      <td>0.500000</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.0</td>\n","      <td>0.500000</td>\n","      <td>0.616667</td>\n","      <td>1.0</td>\n","      <td>0.943923</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.0</td>\n","      <td>0.833333</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>0.972235</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>0.991908</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cbcf6fa-b339-4ea7-a62c-20dbf3724e06')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8cbcf6fa-b339-4ea7-a62c-20dbf3724e06 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8cbcf6fa-b339-4ea7-a62c-20dbf3724e06');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d1db5f95-51eb-43e9-810f-2a68583a167c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d1db5f95-51eb-43e9-810f-2a68583a167c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d1db5f95-51eb-43e9-810f-2a68583a167c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"                 'Faithfulness_Score', 'Hallucination_Score']]\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Contextual Precision_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.251373336211726,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Recall_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3104399942401277,\n        \"min\": 0.27498597046143514,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.75,\n          0.8333333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Relevancy_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.305220888889962,\n        \"min\": 0.2153378052511225,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.76,\n          0.6666666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer Relevancy_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.251373336211726,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer Relevancy (ragas)_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2969893735116127,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.7818984738412035,\n          0.9722352915987021,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Faithfulness_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.251373336211726,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hallucination_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4710017470415684,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2,\n          1.0,\n          0.42163702135578396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":38}]}]}